<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>algebra3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="algebra3_files/libs/clipboard/clipboard.min.js"></script>
<script src="algebra3_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="algebra3_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="algebra3_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="algebra3_files/libs/quarto-html/popper.min.js"></script>
<script src="algebra3_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="algebra3_files/libs/quarto-html/anchor.min.js"></script>
<link href="algebra3_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="algebra3_files/libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="algebra3_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="algebra3_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="algebra3_files/libs/bootstrap/bootstrap-d6a003b94517c951b2d65075d42fb01b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><p>Lecture Notes MA21001<br>
Linear Algebra –&gt;</p></h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date"></p><p>August 2024<br>
©University of Dundee</p><p></p>
    </div>
  </div>
  
    
  </div>
  


</header>


<!--{{< include notation.qmd >}} -->
<section id="vector-spaces" class="level1">
<h1>Vector spaces</h1>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>You have used vectors in previous modules to do elementary geometry. These vectors were introduced as lines connecting two points with a direction: <span class="math display">\[{\vec a} = \overrightarrow{PQ},\]</span> where <span class="math inline">\(P\)</span> is the starting point of the vector and <span class="math inline">\(Q\)</span> is the endpoint. The space of all these vectors is called the Euclidean space. It is usually introduced as a two- or three-dimensional space, but it is possible to generalise this to more dimensions. The Euclidean space was introduced as a simple model for the physical space around us. One can furthermore introduce orthogonal coordinates for the points in this space, e.g. <span class="math display">\[P =  (P_x,P_y,P_z) = (3,2,1).\]</span> and this then allows us to define the components of a vector: <span class="math display">\[{\vec a} = \overrightarrow{PQ} =  \begin{pmatrix}    Q_x- P_x \\ Q_y- P_y \\ Q_z -P_z  \end{pmatrix} \]</span> The identification between points and vectors in an Euclidean space becomes particularly simple if we can choose <span class="math inline">\(P\)</span> to be the origin of the coordinate system, <span class="math inline">\(P=(0,0,0)\)</span>, <span class="math display">\[{\vec a} = \overrightarrow{OQ} =   \begin{pmatrix}    Q_x \\ Q_y \\ Q_z  \end{pmatrix} .\]</span> Note that while the coordinates of points are usually denoted by a row of numbers (tuplet), the components of vectors are written as a column. An alternative way to write a vector is to write it as a sum of multiples of unit vectors, <span class="math display">\[{\vec v} =   v_x {\bf e}_x + v_y {\bf e}_y + v_z {\bf e}_z  =  \begin{pmatrix}    v_x \\ v_y \\ v_z  \end{pmatrix}.\]</span> An addition as well as a scalar multiplication was defined for these vectors. Later, the scalar product between two vectors and the cross-product (vector product) were also defined.</p>
<p>The operations on vectors of this Euclidean space <span class="math inline">\(E\)</span>, the addition and scalar multiplication, had certain properties, for instance</p>
<ul>
<li><ul>
<li><span class="math inline">\(\vec{u} + \vec{v}  \in  E\)</span>, closure condition for addition</li>
<li><span class="math inline">\(\vec{u}+\vec{v}  =  \vec{v}+\vec{u}\)</span>, the addition is commutative</li>
<li><span class="math inline">\(\lambda (\vec{u}+ \vec{v})  =  \lambda \vec{u} + \lambda \vec{v}\)</span>, distributive law</li>
<li></li>
</ul></li>
</ul>
<p>In the following, we will generalise this space and define an abstract notion of a vector space, which has a much wider range of applications. To understand the crucial properties of such a definition, consider two examples.</p>
<div class="example">
<p><em>Example 2.1</em>. The captain of a sailing boat can calculate the velocity of his ship (speed and direction over ground) as the sum of two velocities: the velocity of his boat relative to the water (speed through water) and the velocity of the water (drift due to tides or ocean currents). The sum of the two velocities is found in the same way as above, as the addition of two vectors; however, the coordinate system used is usually a polar one (the angle as determined by a compass and speed in knots) rather than a Cartesian one (x, y-components). The velocities obtained this way are part of a <em>velocity space</em>. The elements of this space, the velocities, follow the same rules of addition and scalar multiplication as the vectors in our Euclidean space. Remark: In this example, the velocity space is a tangent space to a manifold (the surface of the ocean), a concept explained in more detail in Differential Geometry.</p>
</div>
<div class="example">
<p><em>Example 2.2</em>. The set of all polynomials with degree <span class="math inline">\(\leq n\)</span> is denoted by <span class="math inline">\(\mathcal{P}_{n}\)</span>. We can add polynomials in <span class="math inline">\(\mathcal{P}_{n}\)</span> to obtain a polynomial in <span class="math inline">\(\mathcal{P}_{n}\)</span> again. Polynomials <span class="math inline">\(p(x)= a_{0} + a_{1}x + ...+a_{n} x^{n}\)</span> and <span class="math inline">\(q(x)= b_{0} + b_{1}x + ...+b_{n} x^{n}\)</span><span class="math inline">\(\in \mathcal{P}_{n}\)</span> add up to <span class="math display">\[p(x)+q(x)  =  (a_{0}+ b_{0}) + (a_{1} + b_{1}) x + ...+(a_{n}+ b_{n}) x^{n}   \in \mathcal{P}_{n} .\]</span> Similarly, we can multiply polynomials by a scalar <span class="math display">\[\lambda p(x)  =   \lambda a_{0} +  \lambda a_{1}x + ...+ \lambda a_{n} x^{n}  \in \mathcal{P}_{n}\]</span> Note that we can identify a polynomial of degree <span class="math inline">\(\leq n\)</span> with the vector of coefficients <span class="math display">\[p(x) \sim  \scriptstyle \begin{pmatrix}    a_{0} \\ a_{1} \\ \vdots \\ a_{n}   \end{pmatrix} \textstyle.\]</span></p>
</div>
<p>The two examples above illustrate that numerous diverse examples exist where we find a structure similar to the Euclidean space. The elements in these spaces often don’t look like “vectors"; they can be polynomials, velocities, matrices, etc.~, but addition and scalar multiplication work in the same way, so they follow the same rules as vectors. This is the motivation to define an abstract notion of a vector space and define it based on how its elements behave under addition and scalar multiplication, rather than what they represent.</p>
</section>
<section id="definition-of-a-vector-space" class="level2">
<h2 class="anchored" data-anchor-id="definition-of-a-vector-space">Definition of a Vector Space</h2>
</section>
<div id="vksoverr" class="Definition">
<section id="vector-space-over-r" class="level2">
<h2 class="anchored" data-anchor-id="vector-space-over-r">Vector Space over R</h2>
<p>A vector space over <span class="math inline">\(\mathbb{R}\)</span> is a set <span class="math inline">\(V\)</span>, the elements of which are called vectors <span class="math inline">\(\vec{v} \in V\)</span> along with two operations, an addition <span class="math inline">\(\vec{v} + \vec{w}\)</span> and a scalar multiplication <span class="math inline">\(a \vec{v}\)</span>, <span class="math inline">\(a \in \mathbb{R}\)</span>, subject to the following axioms for all <span class="math inline">\(\vec{u},\vec{v}, \vec{w}\in V\)</span> and <span class="math inline">\(a,b\in \mathbb{R}\)</span> : <span id="eq-cond0"><span class="math display">\[\vec{u} + \vec{v} \in  V \tag{1.1}\]</span></span> <span id="eq-cond1"><span class="math display">\[a \vec{u}  \in  V \tag{1.2}\]</span></span> <span id="eq-cond2"><span class="math display">\[\vec{u}+\vec{v} = \vec{v}+\vec{u} \tag{1.3}\]</span></span> <span id="eq-cond3"><span class="math display">\[(\vec{u}+\vec{v})+\vec{w}  = \vec{u}+(\vec{v}+\vec{w}) \tag{1.4}\]</span></span> <span id="eq-cond4"><span class="math display">\[\mbox{there exists a vector } {\vec 0}  \in V \mbox{such that } \vec{u}+\vec{0} = \vec{0}+\vec{u}=\vec{u} \tag{1.5}\]</span></span> <span id="eq-cond5"><span class="math display">\[ \mbox{given } \vec{v} \mbox{ there exists a vector } -\vec{v}  \in V  \mbox{ such that }\vec{v}+ (-\vec{v})  = \vec{0} \tag{1.6}\]</span></span> <span id="eq-cond6"><span class="math display">\[1 \vec{u}  = \vec{u} \tag{1.7}\]</span></span> <span id="eq-cond7"><span class="math display">\[a (\vec{u}+ \vec{v})  = a\vec{u} +a \vec{v} \tag{1.8}\]</span></span> <span id="eq-cond8"><span class="math display">\[(a+b) \vec{u} = a\vec{u} +b \vec{u} \tag{1.9}\]</span></span> <span id="eq-cond9"><span class="math display">\[a (b \vec{u}) =(a b) \vec{u} \tag{1.10}\]</span></span></p>
</section>
</div>
<p><a href="#eq-cond0" class="quarto-xref">Equation&nbsp;1.1</a> and <a href="#eq-cond1" class="quarto-xref">Equation&nbsp;1.2</a> ensure that both operations do not lead to elements not in <span class="math inline">\(V\)</span>, <a href="#eq-cond2" class="quarto-xref">Equation&nbsp;1.3</a> to <a href="#eq-cond5" class="quarto-xref">Equation&nbsp;1.6</a> ensure that the addition is commutative and associative and that there is a neutral element <span class="math inline">\(\vec{0}\)</span> and an inverse element with respect to the addition. Eq.&nbsp;<a href="#eq-cond6" class="quarto-xref">Equation&nbsp;1.7</a> states that the neutral element of the multiplication in <span class="math inline">\(\mathbb{R}\)</span> is also the neutral element for the scalar multiplication, while the remaining conditions are distributive laws.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Remark on Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Alternative notations for vectors encountered in the literature are bold face symbols <span class="math inline">\({\bf v}\)</span> or <span class="math inline">\(\bar v\)</span> (not to be confused with complex conjugation) or <span class="math inline">\(\underline v\)</span>. The zero vector <span class="math inline">\(\vec{0}\)</span> is often written just as <span class="math inline">\(0\)</span>. Note that there are two kinds of additions in this definition, both denoted by the same symbol <span class="math inline">\(+\)</span>. The addition of two real numbers (<span class="math inline">\(a+b\)</span>) and the addition of two vectors <span class="math inline">\(\vec{v}+\vec{w}\)</span>. Also, for the multiplication, we have <span class="math inline">\(ab \in \mathbb{R}\)</span> as well as <span class="math inline">\(a \vec{v} \in V\)</span>. No symbol is used for multiplication, since the dot as well as the cross will be later used to define two different types of multiplication between two vectors.</p>
<p>If we want to avoid writing a vector as a column, as this always takes up a lot of space, we can use the transpose (denoted by a superscript T): <span class="math display">\[ (1,2)^T = \begin{pmatrix}  1 \\ 2 \end{pmatrix} \]</span>.</p>
</div>
</div>
<div class="example">
<p><em>Example 2.3</em>. The set <span class="math inline">\(\mathbb{R}^2 = \{(x_{1},x_{2})^T \vert x_{1} \in \mathbb{R}, x_{2} \in \mathbb{R}\}\)</span> is a vector space, the elements of which are written as columns <span class="math inline">\(\vec{x}=\scriptstyle \begin{pmatrix}   x_{1}\\x_{2}  \end{pmatrix} \textstyle\)</span>, if addition and multiplication are defined as <span class="math display">\[ \begin{pmatrix}   x_1 \\ x_2  \end{pmatrix}
  +
   \begin{pmatrix}   y_1 \\ y_2  \end{pmatrix}
  =
   \begin{pmatrix}   x_1+y_1 \\ x_2+y_2  \end{pmatrix} ;
  \qquad
  a
   \begin{pmatrix}   x_1 \\ x_2  \end{pmatrix}
  =
   \begin{pmatrix}   a x_1 \\  a x_2  \end{pmatrix}  \ .\]</span> We have to check that all the axioms <a href="#eq-cond0" class="quarto-xref">1.1</a> to <a href="#eq-cond9" class="quarto-xref">1.10</a> are satisfied.</p>
<ul>
<li>axioms <a href="#eq-cond0" class="quarto-xref">1.1</a> and <a href="#eq-cond1" class="quarto-xref">1.2</a> are satisfied because the elements are again element in <span class="math inline">\(V\)</span>.</li>
<li>axioms <a href="#eq-cond2" class="quarto-xref">1.3</a> to <a href="#eq-cond3" class="quarto-xref">1.4</a> are satisfied because the addition of real numbers in each entry of the vector is associative and commutative,</li>
<li><a href="#eq-cond4" class="quarto-xref">1.5</a> and <a href="#eq-cond5" class="quarto-xref">1.6</a> are correct if we use the following zero vector and negative element: <span class="math display">\[\vec{0}=\scriptstyle \begin{pmatrix}   0\\0  \end{pmatrix} \textstyle \qquad   -\vec{x} = \scriptstyle \begin{pmatrix}   -x_{1}\\-x_{2}  \end{pmatrix} \textstyle \ .\]</span></li>
<li>axiom <a href="#eq-cond6" class="quarto-xref">1.7</a> is obvious from the definition of the scalar multiplication,</li>
<li>axiom <a href="#eq-cond7" class="quarto-xref">1.8</a> <span class="math display">\[(r+s) \begin{pmatrix}   v_1 \\ v_2  \end{pmatrix}
  =\begin{pmatrix}   (r+s)v_1 \\ (r+s)v_2  \end{pmatrix}
  = \begin{pmatrix}   rv_1+sv_1 \\ rv_2+sv_2  \end{pmatrix}
  =r \begin{pmatrix}   v_1 \\ v_2  \end{pmatrix} +s \begin{pmatrix}   v_1 \\ v_2  \end{pmatrix}; \]</span></li>
<li>axiom <a href="#eq-cond8" class="quarto-xref">1.9</a> <span class="math display">\[r \left( \begin{pmatrix}   v_1 \\ v_2  \end{pmatrix} + \begin{pmatrix}   w_1 \\ w_2  \end{pmatrix} \right)
  =\begin{pmatrix}   r(v_1+w_1) \\ r(v_2+w_2)  \end{pmatrix}
  =\begin{pmatrix}   rv_1+rw_1 \\ rv_2+rw_2  \end{pmatrix}
  =r \begin{pmatrix}   v_1 \\ v_2  \end{pmatrix} +r \begin{pmatrix}   w_1 \\ w_2  \end{pmatrix}; \]</span></li>
<li>axiom <a href="#eq-cond9" class="quarto-xref">1.10</a> <span class="math display">\[(rs) \begin{pmatrix}   v_1 \\ v_2  \end{pmatrix}
  = \begin{pmatrix}   (rs)v_1 \\ (rs)v_2  \end{pmatrix}
  =\begin{pmatrix}   r(sv_1) \\ r(sv_2)  \end{pmatrix}
  =r \left(s \begin{pmatrix}   v_1 \\ v_2  \end{pmatrix} \right).\]</span></li>
</ul>
</div>
<div class="example">
<p><em>Example 2.4</em>. The set <span class="math display">\[V= \left\{ \left.  \begin{pmatrix}   v_{1}\\ v_{2}  \end{pmatrix}  \in \mathbb{R}^2 \right|  v_1^2+v_2^2 \le 1\right\}\]</span> with the addition and scalar multiplication as in the previous example is <strong>not</strong> a vector space. The first two axioms (closure conditions) are not satisfied. For example <span class="math display">\[{\bf u}=  \begin{pmatrix}   1/2\\ 1/2  \end{pmatrix}  \in V, \ \mbox{since} \    \left(\frac{1}{2}\right)^2+\left(\frac{1}{2}\right)^2 = \frac{1}{2} \le 1, \  \mbox{but} \quad  2{\bf u}= 2  \begin{pmatrix}   1/2\\ 1/2  \end{pmatrix}  = \begin{pmatrix}   1\\ 1  \end{pmatrix}   \notin V.\]</span></p>
</div>
<div class="example">
<p><em>Example 2.5</em>. The set <span class="math display">\[V= \left\{ \left.  \begin{pmatrix}   v_{1}\\ v_{2}  \end{pmatrix}  \right| v_1 \in   \mathbb{R}, v_2 \in  \mathbb{R}\right\}\]</span> with the same scalar multiplication as before <span class="math display">\[a   \begin{pmatrix}   x_1 \\ x_2  \end{pmatrix}  =   \begin{pmatrix}   a x_1 \\  a x_2  \end{pmatrix}  \ ,\]</span> but the addition <span class="math display">\[ \begin{pmatrix}   x_1 \\ x_2  \end{pmatrix}
  +
  \begin{pmatrix}   y_1 \\ y_2  \end{pmatrix} \
  =\begin{pmatrix}   x_1+y_1+1 \\ x_2+y_2+1  \end{pmatrix}\]</span> is <strong>not</strong> a vector space. Axioms 8 and 9 are not satisfied.</p>
</div>
<div class="example">
<p><em>Example 2.6</em>. What we have shown for a vector space consisting of two vectors with two real components can be easily extended to n components. The vector space <span class="math display">\[I\!\!R^{n} = \left\{ \left. \scriptscriptstyle \begin{pmatrix}   v_{1}\\ v_{2} \\ \vdots \\ v_{n}  \end{pmatrix} \textstyle \right| v_{i} \in \mathbb{R}, i=1,2, .. ,n \right\}\]</span> together with the operation of addition <span class="math display">\[\scriptscriptstyle \begin{pmatrix}   v_{1}\\ v_{2} \\ \vdots \\ v_{n}  \end{pmatrix} \textstyle +  \scriptscriptstyle \begin{pmatrix}   w_{1}\\ w_{2} \\ \vdots \\ w_{n}  \end{pmatrix} \textstyle =  \scriptscriptstyle \begin{pmatrix}   v_{1}+w_{1}\\ v_{2} +w_{2}\\ \vdots \\ v_{n}+w_{n}  \end{pmatrix} \textstyle\]</span> and the scalar multiplication <span class="math display">\[ \alpha \scriptscriptstyle \begin{pmatrix}   v_{1}\\ v_{2} \\ \vdots \\ v_{n}  \end{pmatrix} \textstyle =  \scriptscriptstyle \begin{pmatrix}    \alpha v_{1}\\  \alpha v_{2} \\ \vdots \\  \alpha v_{n}  \end{pmatrix} \textstyle \quad \alpha \in \mathbb{R}\]</span> is called the vector space <span class="math inline">\(\mathbb{R}^{n}\)</span>.</p>
<p>Although we haven’t defined yet what a basis is, we mention here for future reference that the vectors, <span class="math display">\[\vec{e}_{1} =  \scriptscriptstyle \begin{pmatrix}   1\\ 0 \\ \vdots \\ 0   \end{pmatrix} \textstyle, \quad  \vec{e}_{2} =  \scriptscriptstyle \begin{pmatrix}   0\\ 1 \\ 0 \\ \vdots  \end{pmatrix} \textstyle, ...    ,\vec{e}_{n} =  \scriptscriptstyle \begin{pmatrix}   0\\ 0 \\ \vdots \\ 1   \end{pmatrix} \textstyle\]</span> form a basis of <span class="math inline">\(\mathbb{R}^{n}\)</span>, the so called the standard basis. That is, every vector in <span class="math inline">\(\mathbb{R}^{n}\)</span> can be expressed as a linear combination of the basis vectors, <span class="math display">\[\vec{r} = r_{1} \vec{e}_{1} + r_{2} \vec{e}_{2} + ... + r_{n} \vec{e}_{n} ,\]</span> Or, in other words, the vector space is spanned by the basis vectors.</p>
</div>
<div class="example">
<p><em>Example 2.7</em>. The set of all <span class="math inline">\(m\times n\)</span> matrices where the entries are real numbers forms a vector space. We denote this space by <span class="math inline">\(M^{(m \times n)}\)</span>. Addition and scalar multiplication are the usual addition and scalar multiplication of matrices. A basis of the space consists of the matrices <span class="math inline">\({\rm E}^{(i,j)}\)</span> which have a 1 at entry (i,j) and zeros everywhere else. The space is of dimension <span class="math inline">\(m n\)</span>.</p>
</div>
<div class="example">
<p><em>Example 2.8</em>. Let <span class="math inline">\(\mathcal{P}_{n}\)</span> denote the set of all polynomials with degree <span class="math inline">\(\leq n\)</span>. <span class="math inline">\(\mathcal{P}_{n}\)</span> is a vector space if we declare an addition and scalar multiplication of polynomials <span class="math inline">\(p(x)= a_{0} + a_{1}x + ...+a_{n} x^{n}\)</span> and <span class="math inline">\(q(x)= b_{0} + b_{1}x + ...+b_{n} x^{n}\)</span><span class="math inline">\(\in \mathcal{P}_{n}\)</span> by <span class="math display">\[\begin{aligned}
p(x)+q(x) &amp;= (a_{0}+ b_{0}) + (a_{1} + b_{1}) x + ...+(a_{n}+ b_{n}) x^{n}   \in \mathcal{P}_{n} ,  \\
\lambda p(x) &amp;=  \lambda a_{0} +  \lambda a_{1}x + ...+ \lambda a_{n} x^{n}  \in \mathcal{P}_{n} .
\end{aligned}\]</span> Note that we can identify a polynomial of degree <span class="math inline">\(\leq n\)</span> with the vector of coefficients <span class="math display">\[p(x) \sim  (a_{0}, a_{1},.., a_{n})^{T} .\]</span> Hence, a basis for the space consists of the <em>monomials</em> <span class="math inline">\(x^{r}, r=0,1, ..,n\)</span> and is <span class="math inline">\(n+1\)</span> dimensional.</p>
</div>
<div class="example">
<p><em>Example 2.9</em>. The solutions of a homogeneous linear differential equation of order <span class="math inline">\(n\)</span>, e.g. <span class="math display">\[\frac{\mathrm{d}^{2}y}{\mathrm{d}x^{2}}-3\frac{\mathrm{d}y}{\mathrm{d}x}
+2y=0.\qquad\]</span> form a vector space. Addition and scalar multiplication in this vector space are the usual addition and scalar multiplication of functions. The space is spanned by <span class="math inline">\(n\)</span> linearly independent solutions. This is an example of a so-called <em>function space</em>.</p>
</div>
<div class="example">
<p><em>Example 2.10</em>. The set <span class="math inline">\({\cal F}\)</span> of all infinitely differentiable functions, <span class="math inline">\(\mathbb{R} \rightarrow \mathbb{R}\)</span>, i.e., those&nbsp;that have derivatives of all orders. Note that (among others) the following functions belong to <span class="math inline">\({\cal F}\)</span>: <span class="math display">\[e^{nx},\,\cos \left( 2\pi nx\right)
,\,\,x^{n}\quad \,(\text{for all }n=0,1,2,...).\]</span> <span class="math inline">\({\cal F}\)</span> is an example of an infinite-dimensional space because it contains an unlimited number of linearly independent elements.</p>
</div>
<p>The definition of a vector space over <span class="math inline">\(\mathbb{R}\)</span> can be extended to a vector space over a more general set of numbers, a field K. To understand what a ‘field’ is, we need to take a short detour into number systems.</p>
<div id="vksoverk" class="Definition">
<section id="vector-space-over-k" class="level2">
<h2 class="anchored" data-anchor-id="vector-space-over-k">Vector Space over K</h2>
<p>A vector space over a field <span class="math inline">\(K\)</span> is a set <span class="math inline">\(V\)</span>, the elements of which are called vectors <span class="math inline">\(\vec{v} \in V\)</span> along with two operations, an addition <span class="math inline">\(\vec{v} + \vec{w}\)</span> and a scalar multiplication <span class="math inline">\(a \vec{v}\)</span>, <span class="math inline">\(a \in K\)</span>, subject to the same 10 axioms <a href="#eq-cond0" class="quarto-xref">Equation&nbsp;1.1</a> to <a href="#eq-cond9" class="quarto-xref">Equation&nbsp;1.10</a> as before only that now <span class="math inline">\(a,b\in K\)</span>.</p>
</section>
</div>
<div class="example">
<p><em>Example 2.11</em>. The set <span class="math inline">\(\ref{vksoverk}\)</span> <span class="math inline">\(\mathbb{C}^{n} =\{ (c_{1},...,c_{n})^{T}| c_{1},..,c_{n}\in \mathbb{C}\}\)</span> is a vector space over <span class="math inline">\(\mathbb{C}\)</span>. Addition and scalar multiplication are the usual addition and scalar multiplication of complex numbers. A basis for this space is given by the n vectors <span class="math inline">\(e_{1}=(1,0,...,0)\)</span>,..., <span class="math inline">\(e_{n}=(0,...,0,1)\)</span>. The space has <span class="math inline">\(n\)</span> dimensions. Note that we can identify <span class="math inline">\(\mathbb{C}\)</span> with <span class="math inline">\(\mathbb{R}^{2}\)</span> due to <span class="math inline">\(z= a + b i\)</span> with <span class="math inline">\(a,b \in \mathbb{R}\)</span>. Hence <span class="math inline">\(\mathbb{C}^{n}  \sim \mathbb{R}^{2n}\)</span> is also a vector space over <span class="math inline">\(\mathbb{R}\)</span>, but in this case it is 2n-dimensional with two basis vectors for the real and imaginary part of each complex dimension.</p>
</div>
<section id="linear-independence" class="level2">
<h2 class="anchored" data-anchor-id="linear-independence">Linear independence</h2>
<p>In the same way as a coordinate system is introduced on a plane to allocate a unique pair of coordinates to any point in the plane, we would like to have a coordinate system for a vector space, which uniquely assigns to any vector of this space a coordinate tuple (set). To generate such a coordinate system, we need a set of ‘basis’ vectors which are, in a sense, independent. Consider the following example in the x-y plane.</p>
<p>Given the two unit vectors in <span class="math inline">\(\mathbb{R}^2\)</span>, <span class="math display">\[\vec{e}_1 = \scriptstyle \begin{pmatrix}   1 \\ 0  \end{pmatrix} \textstyle, \ \vec{e}_2= \scriptstyle \begin{pmatrix}   0 \\ 1  \end{pmatrix} \textstyle\]</span> Any point in the plane, that is, any vector in <span class="math inline">\(\mathbb{R}^2\)</span>, can be represented by a linear combination <span class="math display">\[\vec{v } = v_1  \vec{e}_1  + v_2  \vec{e}_2,  v_1,v_2 \in \mathbb{R}\]</span> where <span class="math inline">\((v_1,v_2)\)</span> play the role of coordinates. These coordinates are unique; that is, there are no two different vectors with the same coordinate pair, nor are two different coordinate pairs assigned to the same vector.</p>
<p>The same is true if we replace our basis vectors by (check!) <span class="math display">\[\vec{e}_1 = \scriptstyle \begin{pmatrix}   1 \\ 0  \end{pmatrix} \textstyle, \ \vec{e}_2= \scriptstyle \begin{pmatrix}   1 \\ 1  \end{pmatrix} \textstyle .\]</span></p>
<p>However, if we use a triplet of vectors: <span class="math display">\[\vec{e}_1 = \scriptstyle \begin{pmatrix}   1 \\ 0  \end{pmatrix} \textstyle, \ \vec{e}_2= \scriptstyle \begin{pmatrix}   0 \\ 1  \end{pmatrix} \textstyle,  \ \vec{e}_3= \scriptstyle \begin{pmatrix}   1 \\ 1  \end{pmatrix} \textstyle ,\]</span> <span class="math display">\[\vec{v } = v_1  \vec{e}_1  + v_2  \vec{e}_2 + v_3  \vec{e}_3 ,  v_1,v_2,v_3 \in \mathbb{R}\]</span></p>
<p>Then there are vectors with non-unique coordinates: <span class="math display">\[\vec{v}   = 2  \vec{e}_1 + 3  \vec{e}_2 -1 \vec{e}_3  =   1  \vec{e}_1 + 2  \vec{e}_2 + 0 \vec{e}_3\]</span></p>
<p>More generally, if such a non-unique case occurs, one can write <span class="math display">\[\begin{aligned}
{\bf v} &amp; =   v_1  \vec{e}_1 +  v_2 \vec{e}_2 + v_3 \vec{e}_3 \\
  {\bf v} &amp; =   v_1'  \vec{e}_1 + v_2'  \vec{e}_2 + v_3' \vec{e}_3 \\
  \Rightarrow \vec{0} &amp; =   (v_1-v_1')  \vec{e}_1 + (v_2-v_2')  \vec{e}_2 + (v_3-v_3') \vec{e}_3, \\
   \Rightarrow   \vec{0} &amp; =   a_1  \vec{e}_1 + a_2  \vec{e}_2 + a_3 \vec{e}_3, \quad (a_1,a_2,a_3) \neq (0,0,0)
  \end{aligned}\]</span> where at least one of the three brackets is non-zero since we assumed that <span class="math inline">\((v_1,v_2,v_3) \neq (v_1',v_2',v_3')\)</span>. This is called a non-trivial linear combination.</p>
</section>
<section id="linear-combination" class="level2 Definition">
<h2 class="anchored" data-anchor-id="linear-combination">Linear Combination</h2>
<p>An expression of the form <span class="math inline">\(a_{1}{\vec v}
_{1}+a_{2}{\vec v}_{2}+\cdots +a_{n}{\vec v}_{n}\)</span> is known as a <em>linear combination of the <span class="math inline">\(n\)</span> vectors <span class="math inline">\({\vec v}
_{1},{\vec v}_{2},\cdots ,{\vec v}_{n}\)</span>. The numbers <span class="math inline">\(a_{1}, a_{2},..., a_{n}\)</span> are known as the coefficients of the linear combination.</em></p>
</section>
<section id="linear-independence-1" class="level2 Definition">
<h2 class="anchored" data-anchor-id="linear-independence-1">Linear Independence</h2>
<p>A set of vectors <span class="math inline">\(\vec{v_{1}}, \vec{v_{2}},..., \vec{v_{n}}\)</span> is called <em>linearly independent</em> if none of its elements is a linear combination of the others. That is the equation <span class="math display">\[a_{1}\vec{v_{1}} + a_{2} \vec{v_{2}}+  ... + a_{n}\vec{v_{n}} = 0\]</span> has no solution (<span class="math inline">\(a_{1},a_{2},..,a_{n})\)</span> other than the trivial solution <span class="math inline">\((0,0,...,0)\)</span>. Otherwise, the set is called <em>linearly dependent</em>.</p>
</section>
<p>I.e., a set of vectors is linearly dependent if we can find a non-trivial linear combination that yields the zero vector. Note that a simple relation between just two of the vectors, e.g. <span class="math inline">\(\mathbf{v}_{1}=3\mathbf{v}_{2},\)</span> is enough to make the complete set <span class="math inline">\(\mathit{
\{}\mathbf{v}_{1},\mathbf{v}_{2},...,\mathbf{v}_{n}\mathit{\}}\)</span> linearly dependent.</p>
<div class="example">
<p><em>Example 2.12</em>. Are the vectors <span class="math inline">\((1,-1,0)^{T},(2,1,1)^{T}\)</span> and <span class="math inline">\((-1,2,1)^{T}\)</span> linearly independent?</p>
<p>We solve <span class="math display">\[a_{1} \begin{pmatrix}   
1 \\
-1 \\
0  \end{pmatrix} +a_{2}  \begin{pmatrix}   
2 \\
1 \\
1  \end{pmatrix} +a_{3}  \begin{pmatrix}   
-1 \\
2 \\
1  \end{pmatrix} = \begin{pmatrix}   
0 \\
0 \\
0  \end{pmatrix}.\]</span> for <span class="math inline">\((a_{1}, a_{2}, a_{3})\)</span>. We find that <span class="math inline">\(a_{1}=a_{2}=a_{3}=0\)</span>; so the 3 given vectors are linearly independent. If we replace the first entry in the first vector by 3, the set becomes linearly dependent. A solution is <span class="math inline">\(a_1=1\)</span>, <span class="math inline">\(a_2=-1\)</span>, <span class="math inline">\(a_3=1\)</span>.</p>
</div>
<div class="example">
<p><em>Example 2.13</em>. Are the vectors <span class="math inline">\((1,0,0)^{T}\)</span>, <span class="math inline">\((1,1,0)^{T}\)</span> and <span class="math inline">\((1,0,1)^{T}\)</span> linearly independent?</p>
<p>We solve <span class="math display">\[a_{1} \scriptstyle \begin{pmatrix}   1 \\ 0 \\ 0  \end{pmatrix} \textstyle + a_{2} \scriptstyle \begin{pmatrix}   1 \\ 1 \\ 0  \end{pmatrix} \textstyle + a_{3} \scriptstyle \begin{pmatrix}   1 \\ 0 \\ 1  \end{pmatrix} \textstyle = \scriptstyle \begin{pmatrix}   0\\0\\0  \end{pmatrix} \textstyle,\]</span> for <span class="math inline">\((a_{1},a_{2},a_{3})\)</span>. The third component of this equation implies <span class="math inline">\(a_{3}=0\)</span>. From the second and first component follow <span class="math inline">\(a_{2}=0\)</span> and <span class="math inline">\(a_{1} =0\)</span>. Hence, the vectors are linearly independent.</p>
<p>On the other hand, the vectors <span class="math display">\[\scriptstyle \begin{pmatrix}   1 \\ 0 \\ 0  \end{pmatrix} \textstyle, \quad \scriptstyle \begin{pmatrix}   1 \\ 1 \\ 0  \end{pmatrix} \textstyle,  \quad \scriptstyle \begin{pmatrix}   0 \\ 1 \\ 0  \end{pmatrix} \textstyle,\]</span> are linearly dependent, since the third component reads <span class="math inline">\(a_{3} 0 = 0\)</span> which is satisfied for any <span class="math inline">\(a_{3}=\lambda\)</span>. The second component requires <span class="math inline">\(a_{2} = - a_{3}\)</span> and the first <span class="math inline">\(a_{1} = a_{3}\)</span>. So there are non-trivial solutions <span class="math inline">\((\lambda , -\lambda , \lambda )\)</span>, e.g.&nbsp;<span class="math inline">\((1,-1,1)\)</span>.</p>
</div>
<div class="example">
<p><em>Example 2.14</em>. A set of two vectors is linearly independent if they are not parallel: <span class="math inline">\(a_{1} \vec{v}_{1} \neq a_{2} \vec{v_{2}}\)</span> for <span class="math inline">\(a_{1}, a_{2}\)</span> non-zero. A set of three vectors in <span class="math inline">\(\mathbb{R}^{2}\)</span> is always linearly dependent. A set of three vectors in <span class="math inline">\(\mathbb{R}^{3}\)</span> is linearly independent if they do not all lie in the same plane.</p>
</div>
<div class="example">
<p><em>Example 2.15</em>. Show that the vectors <span class="math inline">\((1,0,2,1)^{T},(0,1,-1,2)^{T},(2,1,3,4)^{T}\)</span> are linearly dependent in <span class="math inline">\(\mathbb{R}^{4}.\)</span></p>
</div>
<section id="span" class="level2 Definition">
<h2 class="anchored" data-anchor-id="span">Span</h2>
<p>The set of vectors <span class="math inline">\(\{{\vec v}_{1},{\vec v}_{2},...,{\vec v}_{n}\}\)</span> in <span class="math inline">\(V\)</span> <strong>span</strong> <span class="math inline">\(V\)</span> if every vector <span class="math inline">\(\vec{v}\in V\)</span> is a linear combination of <span class="math inline">\(\{{\vec v}_{1},{\vec v}_{2},...,{\vec v}_{n}\}\)</span>. The set of all vectors of the form <span class="math inline">\({\vec v=}a_{1}{\vec v}_{1}+a_{2}{\vec v}_{2}+\cdots +a_{n}{\vec v}_{n}\)</span> is called the span of <span class="math inline">\(\{{\vec v}_{1},{\vec v}_{2},...,{\vec v}_{n}\}\)</span> , and denoted by span(<span class="math inline">\({\vec v}_{1},{\vec v}_{2},...,{\vec v}_{n}\)</span>).</p>
</section>
<div class="example">
<p><em>Example 2.16</em>. <span class="math display">\[\begin{aligned}
{\rm span}\left(\scriptscriptstyle \begin{pmatrix}   1\\0  \end{pmatrix} \textstyle,\scriptscriptstyle \begin{pmatrix}   2\\0  \end{pmatrix} \textstyle\right) &amp; = \left\{ \left. {\bf v}  =a_1 \scriptscriptstyle \begin{pmatrix}   1\\0  \end{pmatrix} \textstyle + a_2 \scriptscriptstyle \begin{pmatrix}   2\\0  \end{pmatrix} \textstyle\right| a_1,a_2 \in \mathbb{R} \right\} \\
&amp;=  \left\{ \left.{\bf v} =  \scriptscriptstyle \begin{pmatrix}   a_1+2a_2 \\ 0  \end{pmatrix} \textstyle \right| a_1,a_2 \in \mathbb{R} \right\}  =  \mathbb{R}^{1},
\end{aligned}\]</span> spans the whole of <span class="math inline">\(\mathbb{R}\)</span>. This space contains only the vectors with 0 in the second component.</p>
</div>
<div class="example">
<p><em>Example 2.17</em>. The span of two linearly independent vectors in <span class="math inline">\(\mathbb{R}^{3}\)</span> is a plane. The span of three linearly independent vectors in <span class="math inline">\(\mathbb{R}^{3}\)</span> is the whole of <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p>
</div>
<div class="example">
<p><em>Example 2.18</em>. The set <span class="math inline">\(\{(1,0)^{T},(0,1)^{T}\}\)</span> spans <span class="math inline">\(\mathbb{R}^{2}\)</span>. <span class="math display">\[\begin{aligned}
{\rm span}\left(\scriptscriptstyle \begin{pmatrix}   1\\0  \end{pmatrix} \textstyle,\scriptscriptstyle \begin{pmatrix}   0\\1  \end{pmatrix} \textstyle\right) &amp; = \left\{ \left. \vec{v} = a_1 \scriptscriptstyle \begin{pmatrix}   1\\0  \end{pmatrix} \textstyle + a_2 \scriptscriptstyle \begin{pmatrix}   0\\1  \end{pmatrix} \textstyle\right| a_1,a_2 \in \mathbb{R} \right\} \\
&amp; = \left\{ \left. \vec{v} = \scriptscriptstyle \begin{pmatrix}   a_1\\a_2  \end{pmatrix} \textstyle \right| a_1,a_2 \in \mathbb{R} \right\} =    \mathbb{R}^{2}.
\end{aligned}\]</span> Thus, any vector <span class="math inline">\((a_{1},a_{2})^{T}\in\mathbb{R}^{2}\)</span> can be expressed as a linear combination of <span class="math inline">\((1,0)^{T}\)</span> and <span class="math inline">\((0,1)^{T}\)</span>.</p>
</div>
<p>If we add another vector, e.g. <span class="math display">\[{\rm span}\left(\scriptscriptstyle \begin{pmatrix}   1\\0  \end{pmatrix} \textstyle,\scriptscriptstyle \begin{pmatrix}   0\\1  \end{pmatrix} \textstyle, \scriptscriptstyle \begin{pmatrix}   1\\1  \end{pmatrix} \textstyle \right) =\mathbb{R}^{2},\]</span> then the space may or may not become any bigger. In this case, the space remains the same since the third vector is a linear combination of the first two vectors, that is, the set is linearly dependent. The smallest linearly independent set which spans a given vector space is called a basis.</p>
<section id="basis" class="level2 Definition">
<h2 class="anchored" data-anchor-id="basis">Basis</h2>
<p>A basis of a vector space is a linearly independent set of vectors which span the vector space.</p>
</section>
<div class="example">
<p><em>Example 2.19</em>. The set of vectors <span class="math inline">\(\left\{(1,0,0)^{T}, (0,1,0)^{T}, (0,0,1)^{T}\right\}\)</span> forms a basis of <span class="math inline">\(\mathbb{R}^{3}\)</span> (more generally <span class="math inline">\(K^{3}\)</span> for a field <span class="math inline">\(K\)</span>), the set of vectors <span class="math inline">\(\left\{(1,0,0,0)^{T}, (0,1,0,0)^{T},\right\}\)</span><br>
<span class="math inline">\(\left (0,0,1,0)^{T},(0,0,0,1)^{T}\right\}\)</span> forms a basis of <span class="math inline">\(\mathbb{R}^{4}\)</span> (more generally <span class="math inline">\(K^{4}\)</span>) and so on. This is called the <strong>standard basis</strong> of <span class="math inline">\(\mathbb{R}^{n}\)</span> (<span class="math inline">\(K^{n}\)</span> in more general sense) for <span class="math inline">\(n\in\mathbb{N}\)</span>.</p>
</div>
<div class="example">
<p><em>Example 2.20</em>. The vectors <span class="math display">\[\vec{f}_{1} =  \scriptscriptstyle \begin{pmatrix}   0\\ 1 \\ 1   \end{pmatrix} \textstyle, \quad \vec{f}_{2} =  \scriptscriptstyle \begin{pmatrix}   0\\ 1 \\ 0   \end{pmatrix} \textstyle, \quad \vec{f}_{3} =  \scriptscriptstyle \begin{pmatrix}   0\\ 0 \\ 1  \end{pmatrix} \textstyle,\]</span> do not form a basis for <span class="math inline">\(\mathbb{R}^{3}\)</span> since they do not span the whole <span class="math inline">\(\mathbb{R}^{3}\)</span>. The vector <span class="math inline">\(\vec{e}_{1}\)</span>, for instance, has no representation in this set.</p>
</div>
<div class="example">
<p><em>Example 2.21</em>. The vectors <span class="math display">\[\vec{f}_{1} =  \scriptscriptstyle \begin{pmatrix}   1\\ 0 \\ 0   \end{pmatrix} \textstyle, \quad \vec{f}_{2} =  \scriptscriptstyle \begin{pmatrix}   0\\ 1 \\ 0   \end{pmatrix} \textstyle, \quad \vec{f}_{3} =  \scriptscriptstyle \begin{pmatrix}   0\\ 0 \\ 1  \end{pmatrix} \textstyle, \quad  \vec{f}_{4} =  \scriptscriptstyle \begin{pmatrix}   1\\ 1 \\ 1   \end{pmatrix} \textstyle ,\]</span> are also not a basis. Although they span the whole <span class="math inline">\(\mathbb{R}^{3}\)</span>, they are not linearly independent.</p>
</div>
<div class="example">
<p><em>Example 2.22</em>. Consider the set of all <span class="math inline">\(n\times n\)</span> matrices with real entries <span class="math inline">\(\mathbb{R}^{(n\times n)}\)</span>. Let <span class="math inline">\(E_{ij}\)</span> be the <span class="math inline">\(n\times n\)</span> matrix with a ‘1’ in position <span class="math inline">\((i,j)\)</span> and ‘0’ elsewhere. Then the collection of all such matrices (for all <span class="math inline">\(i,j=1,2,\ldots,n\)</span>) is a basis for <span class="math inline">\(\mathbb{R}^{(n\times n)}\)</span>.</p>
<p>Consider the case <span class="math inline">\(n=2\)</span>, then the basis matrices are <span class="math display">\[\left(\begin{array}{cc}
a &amp; b\\
c &amp; d
\end{array}\right)=aE_{11}+bE_{12}+cE_{21}+dE_{22},\quad\textrm{(spanning)}.\]</span> Also, we can show that the four <span class="math inline">\(E\)</span> matrices are linearly independent. <span class="math display">\[\alpha _{1}E_{11}+\alpha _{2}E_{12}+\alpha _{3}E_{21}+\alpha _{4}E_{22}=\mathbf{0}\]</span> means that <span class="math display">\[\left(\begin{array}{cc}
\alpha _{1} &amp; \alpha _{2}\\
\alpha _{3} &amp; \alpha _{4}
\end{array}\right) = \left(\begin{array}{cc}
0 &amp; 0\\
0 &amp; 0
\end{array}\right),\quad\textrm{i.e.   }\alpha _{1}=\alpha _{2}=\alpha _{3}=\alpha _{4}=0.\]</span> Since the four matrices span the space and are linearly independent, they form a basis for <span class="math inline">\(\mathbb{R}^{(n\times n)}\)</span>.</p>
</div>
<section id="uniqueness-w.r.t.-a-basis" class="level2 Theorem">
<h2 class="anchored" data-anchor-id="uniqueness-w.r.t.-a-basis">Uniqueness w.r.t. a basis</h2>
<p>Let <span class="math inline">\(\left\{ {\vec v}_{1},{\vec v}_{2},\dots ,{\vec v}
_{n}\right\}\)</span> be a basis for a vector space <span class="math inline">\(V.\)</span> Each vector from <span class="math inline">\(V\)</span> can be uniquely expressed as a linear combination of these vectors.</p>
</section>
<div class="Proof">
<p>If there were two different representations of a vector <span class="math inline">\({\vec v}\)</span> with respect to this basis: Eg. <span class="math inline">\({\vec v} =a_{1}{\vec v}_{1} + \dots + a_{n }{\vec v}_{n}\)</span> and <span class="math inline">\({\vec v} =b_{1}  {\vec v}_{1} + \dots + b_{n }{\vec v}_{n}\)</span>, then the difference <span class="math inline">\({\bf 0} =(a_{1} - b_{1}){\vec v}_{1} + \dots + (a_{n }-b_{n}){\vec v}_{n}\)</span> would be a non-trivial linear combination representing the zero vector, which is impossible since the vectors <span class="math inline">\({\vec v}_{1},\dots ,{\vec v}_{n}\)</span> were linearly independent.&nbsp;◻</p>
</div>
<div class="example">
<p><em>Example 2.23</em>. In <span class="math inline">\(\mathbb{R}^{3},\)</span> <span class="math display">\[\begin{pmatrix}   2 \\
-3 \\
4  \end{pmatrix} =2 \begin{pmatrix}   
1 \\
0 \\
0  \end{pmatrix} -3 \begin{pmatrix}   
0 \\
1 \\
0  \end{pmatrix} +4 \begin{pmatrix}   
0 \\
0 \\
1  \end{pmatrix} =2{\vec e}_{1}-3{\vec e}_{2}+4{\vec e}_{3}.\]</span></p>
<p>So, any linear combination of the vectors <span class="math inline">\({\vec e}_{1},
{\vec e}_{2},{\vec e}_{3}\)</span> is a vector in <span class="math inline">\(\mathbb{R}^{3}\)</span> and any vector in <span class="math inline">\(\mathbb{R}^{3}\)</span> can be written as a linear combination of <span class="math inline">\({\vec e}_{1},{\vec e}
_{2},{\vec e}_{3}\)</span> and the expression is unique.</p>
</div>
<div class="example">
<p><em>Example 2.24</em> (Basis). A different basis, other than the standard basis, for the <span class="math inline">\(\mathbb{R}^{3}\)</span> is, for instance <span class="math display">\[\vec{f}_{1} =  \scriptscriptstyle \begin{pmatrix}   1\\ 1 \\ 0   \end{pmatrix} \textstyle, \quad \vec{f}_{2} =  \scriptscriptstyle \begin{pmatrix}   0\\ 1 \\ 0   \end{pmatrix} \textstyle, \quad \vec{f}_{3} =  \scriptscriptstyle \begin{pmatrix}   0\\ 0 \\ 1  \end{pmatrix} \textstyle,\]</span> We can check that we still can express any vector with respect to (w.r.t.) this basis. We express the arbitrary vector <span class="math inline">\(\vec{r}\)</span> first w.r.t.&nbsp;the standard basis and show that we can translate this into a representation w.r.t.&nbsp;the new basis: <span class="math display">\[\begin{aligned}
\vec{r} &amp; = r_{1} \vec{e}_{1} + r_{2} \vec{e}_{2} +r_{3} \vec{e}_{3}  \\
  &amp; = r_{1} (\vec{e}_{1} + \vec{e}_{2})  + (r_{2}-r_{1}) \vec{e}_{2} +r_{3} \vec{e}_{3}  \\
  &amp; = r_{1} \vec{f}_{1} + (r_{2}-r_{1}) \vec{f}_{2} +r_{3} \vec{f}_{3}  .
\end{aligned}\]</span> And this expression is unique. In addition, the definition of a basis requires that the new basis vectors are linearly independent, which they are; otherwise, they could not span the whole three-dimensional space.</p>
</div>
<p>We note that there are different bases, but they all have the same number of elements.</p>
<section id="number-of-elements-of-a-basis" class="level2 Theorem">
<h2 class="anchored" data-anchor-id="number-of-elements-of-a-basis">Number of elements of a basis</h2>
<p>Every basis of a vector space contains the same number of vectors, this number being the largest set of linearly independent vectors in the set.</p>
</section>
<div class="Proof">
<p>Suppose that a vector space <span class="math inline">\(V\)</span> has two bases, one of which is <span class="math inline">\(\left\{ {\vec w}_{1},...,{\vec w}_{p}\right\}\)</span> that contains <span class="math inline">\(p\)</span> vectors and the other <span class="math inline">\(\left\{ {\vec z}_{1},...,{\vec z}_{q}\right\}\)</span> that contains <span class="math inline">\(q\)</span> vectors. We can assume that <span class="math inline">\(p&gt;q.\)</span> We shall show that the vectors <span class="math inline">\(\left\{ {\vec w}_{1},...,{\vec w}_{p}\right\}\)</span> are linearly dependent, contradicting the fact that they form a basis.</p>
<p>To begin, since <span class="math inline">\(V={\rm span}\left( {\vec z}_{1},...,{\vec z}_{q}\right)\)</span> and each of the <span class="math inline">\({\vec w}\)</span>’s lies in <span class="math inline">\(V,\)</span> they must be linear combinations of the <span class="math inline">\({\vec z}\)</span>’s. i.e., <span class="math display">\[\begin{aligned}
{\vec w}_{1} &amp;=c_{11}{\vec z}_{1}+c_{21}{\vec z}_{2}+\cdots +c_{q1}
{\vec z}_{q} \\
&amp;\vdots \\
{\vec w}_{p} &amp;=c_{1p}{\vec z}_{1}+c_{2p}{\vec z}_{2}+\cdots +c_{qp}
{\vec z}_{q}.
\end{aligned}\]</span> Now consider a linear combination of the <span class="math inline">\({\vec w}\)</span>’s: <span class="math display">\[\begin{aligned}
\sum_{j=1}^{p}a_{j}{\vec w}_{j}  = &amp; a_{1}{\vec w}_{1}+\cdots +a_{p}
{\vec w}_{p} \\
= &amp; a_{1}\left( c_{11}{\vec z}_{1}+c_{21}{\vec z}_{2}+\cdots +c_{q1}
{\vec z}_{q}\right) \\
&amp;+ a_{2} \left( c_{12}{\vec z}_{1}+c_{22}{\vec z}_{2}+\cdots +c_{q2}
{\vec z}_{q}\right) \\
&amp; \vdots \\
&amp; + a_{p}\left( c_{1p}{\vec z}_{1}+c_{2p}{\vec z}_{2}+\cdots +c_{qp}
{\vec z}_{q}\right) \\
= &amp;
\left( c_{11}a_{1}+\cdots +c_{1p}a_{p}\right) {\vec z}_{1}+\cdots
+\left( c_{q1}a_{1}+\cdots +c_{qp}a_{p}\right) {\vec z}_{q}.
\end{aligned}\]</span> Can we choose the <span class="math inline">\(a\)</span>’s so that the right-hand side is zero? Since the <span class="math inline">\({\vec z}\)</span>’s are l.i., this will require that each of the coefficients be zero: <span class="math display">\[\begin{aligned}
c_{11}a_{1}+\cdots +c_{1p}a_{p} &amp;=0 \\
&amp;\vdots \\
c_{q1}a_{1}+\cdots +c_{qp}a_{p} &amp;=0
\end{aligned}\]</span> but this is a system of <span class="math inline">\(q\)</span> equations in <span class="math inline">\(p\)</span> unknowns with <span class="math inline">\(p&gt;q.\)</span> Such a system always has non-zero solutions. Hence, there will be a choice if the coefficients <span class="math inline">\(a_{1},\ldots ,a_{p}\)</span> (not all of which are zero) so that <span class="math inline">\(a_{1}
{\vec w}_{1}+\cdots +a_{p}{\vec w}_{p}=0.\)</span> But this means that the <span class="math inline">\({\vec w}\)</span>’s are l.d.. We have a contradiction, since we assumed the <span class="math inline">\({\vec w}\)</span>’s form a basis. A similar argument can be applied if we assume that <span class="math inline">\(q&gt;p.\)</span> Thus, a contradiction can be avoided only when <span class="math inline">\(p=q.\)</span>&nbsp;◻</p>
</div>
<section id="dimension" class="level2 Definition">
<h2 class="anchored" data-anchor-id="dimension">Dimension</h2>
<p>The number of elements of a basis is called the dimension of the space.</p>
</section>
<div class="example">
<p><em>Example 2.25</em>. <span class="math inline">\(\mathbb{R}^{n}\)</span> has dimension <span class="math inline">\(n\)</span>.</p>
</div>
<div class="example">
<p><em>Example 2.26</em>. The space of polynomials of degree <span class="math inline">\(\leq n\)</span> has basis <span class="math inline">\(\{1, x, x^{2},\ldots,x^{n}\}\)</span>, so its dimension is <span class="math inline">\(n+1\)</span> (not <span class="math inline">\(n\)</span>).</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Remark on dimension
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that the dimension of <span class="math inline">\(V\)</span> depends on the field <span class="math inline">\(K\)</span>. Thus the complex numbers <span class="math inline">\(\mathbb{C}\)</span> can be considered as a space of dimension <span class="math inline">\(1\)</span> over <span class="math inline">\(\mathbb{C}\)</span>, or as a space of dimension <span class="math inline">\(2\)</span> over <span class="math inline">\(\mathbb{R}\)</span>, where <span class="math inline">\(\{1, i\}\)</span> is a basis for <span class="math inline">\(\mathbb{C}\)</span> over <span class="math inline">\(\mathbb{R}\)</span>.</p>
</div>
</div>
<section id="smaller-spanning-set" class="level2 Theorem">
<h2 class="anchored" data-anchor-id="smaller-spanning-set">Smaller spanning set</h2>
<p>Any linearly dependent vectors of a spanning set can be omitted without making the span smaller. <span class="math display">\[{\rm span}({\vec v}_{1},{\vec v}_{2},...,{\vec v}_{n}) = {\rm span}( \mbox{largest  l.i. subset of }  {\vec v}_{1},{\vec v}_{2},...,{\vec v}_{n})\]</span></p>
</section>
<div class="Proof">
<p>Without loss of generality (w.l.o.g.) we can assume that <span class="math inline">\(\{{\vec v}_{1},{\vec v}_{2},...,{\vec v}_{r} \}\)</span> is the largest linearly independent subset (<span class="math inline">\(r\le n\)</span>). Then any remaining vectors <span class="math inline">\({\vec v}_{r+1}, ... , {\vec v}_{n}\)</span> can be expressed as a linear combination of <span class="math inline">\(\{{\vec v}_{1},{\vec v}_{2},...,{\vec v}_{r} \}\)</span> hence <span class="math inline">\({\rm span}({\vec v}_{1},{\vec v}_{2},...,{\vec v}_{r}) = {\rm span}({\vec v}_{1},{\vec v}_{2},...,{\vec v}_{n})\)</span>&nbsp;◻</p>
</div>
<div class="Corollary">
<p>Every set of m+1 vectors in <span class="math inline">\(\mathbb{R}^{m}\)</span> is linearly dependent.</p>
</div>
<div class="Proof">
<p>We know that the standard basis of <span class="math inline">\(\mathbb{R}^{m}\)</span> consists of m elements, hence the dimension is m. Suppose there were <span class="math inline">\(m+1\)</span> linearly independent vectors in <span class="math inline">\(\mathbb{R}^{m}\)</span>, then they could be used to form a basis. But then the dimension of <span class="math inline">\(\mathbb{R}^{m}\)</span> would be <span class="math inline">\(m+1\)</span>, in contradiction to the previous result that the dimension is a unique number.&nbsp;◻</p>
</div>
<section id="subspaces" class="level2">
<h2 class="anchored" data-anchor-id="subspaces">Subspaces</h2>
<p>Suppose that <span class="math inline">\(W\)</span> is a nonempty subset of the vector space <span class="math inline">\(V\)</span>, which also satisfies all the vector space axioms itself, then <span class="math inline">\(W\)</span> is called a subspace of <span class="math inline">\(V\)</span>. It is important to realise that for <span class="math inline">\(W\)</span> to be subspace of <span class="math inline">\(V\)</span>, then <span class="math inline">\(W\)</span> must satisfy the following conditions:</p>
</section>
<section id="subspace" class="level2 Definition">
<h2 class="anchored" data-anchor-id="subspace">Subspace</h2>
<p>A subset <span class="math inline">\(W \subseteq V\)</span> of vectors in a vector space <span class="math inline">\(V\)</span> is called a subspace of <span class="math inline">\(V\)</span> if the following conditions hold:</p>
<p>(i) <span class="math inline">\({\vec 0}\in W,\)</span></p>
<p>(ii) if <span class="math inline">\({\vec u} \in W\)</span> and <span class="math inline">\({\vec v} \in W\)</span> then <span class="math inline">\({\vec u}+{\vec
v\in }W\)</span> ;</p>
<p>(iii) if <span class="math inline">\({\vec u\in }W\)</span> then <span class="math inline">\(a{\vec u\in }W\)</span> for all <span class="math inline">\(a\in K\)</span>.</p>
</section>
<p>All other axioms are automatically satisfied because each element in <span class="math inline">\(W\)</span> is already in <span class="math inline">\(V\)</span>.</p>
<div class="example">
<p><em>Example 2.27</em>. For any vector space <span class="math inline">\(V\)</span>, <span class="math inline">\(V\)</span> is always a subspace of itself.</p>
</div>
<div class="example">
<p><em>Example 2.28</em>. We also always have a subspace <span class="math inline">\(\{\vec{0}\}\)</span> consisting of the zero vector alone. This is called the trivial subspace, and its dimension is <span class="math inline">\(0\)</span>, because it has no linearly independent sets of vectors at all.</p>
</div>
<div class="example">
<p><em>Example 2.29</em>. The subspaces of <span class="math inline">\(\mathbb{R}^{2}\)</span> are <span class="math inline">\(\{\vec{0}\}\)</span>, lines through the origin, and <span class="math inline">\(\mathbb{R}^{2}\)</span> itself.</p>
</div>
<div class="example">
<p><em>Example 2.30</em>. The subspaces of <span class="math inline">\(\mathbb{R}^{3}\)</span> are <span class="math inline">\(\{\vec{0}\}\)</span>, lines through the origin, planes through the origin, and <span class="math inline">\(\mathbb{R}^{3}\)</span> itself.</p>
</div>
<div class="example">
<p><em>Example 2.31</em>. Let <span class="math inline">\(W=\{(x,y,z)^{T}:y=2x, z=0,x,y,z\in\mathbb{R}\}\)</span>. This is the set of points lying on the line <span class="math inline">\(y=2x\)</span> in the plane <span class="math inline">\(z=0\)</span>. <span class="math inline">\(W\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p>
</div>
<div class="example">
<p><em>Example 2.32</em>. The set <span class="math inline">\(W=\{(x,y,z,w)^{T}:x+y+z=1\}\)</span> is not a subspace of <span class="math inline">\(\mathbb{R}^{4}\)</span>. The simplest way to see this is that <span class="math inline">\({\vec 0\not\in }W\)</span>. Another way is to choose suitable vectors <span class="math inline">\(\vec{u},\vec{v}\in W\)</span> then show that <span class="math inline">\(\vec{u}+\vec{v}\not\in W\)</span>. A third way is to choose a suitable <span class="math inline">\({\vec u} \in
W\)</span> then show that <span class="math inline">\(a{\vec u\not\in }W\)</span> for any <span class="math inline">\(a\neq 1\)</span>. Any one of the reasons suffices.</p>
</div>
<div class="example">
<p><em>Example 2.33</em>. Consider the subset <span class="math inline">\(S\)</span> of <span class="math inline">\(P_{2}\)</span> defined so as to contain all polynomial <span class="math inline">\({\rm p}\left( x\right) =a+bx+cx^{2}\)</span> for which <span class="math inline">\(a+b-2c=0\)</span>. Show that <span class="math inline">\(S\)</span> is a subspace of <span class="math inline">\(P_{2}\)</span> and obtain a basis for <span class="math inline">\(S\)</span>.</p>
</div>
<section id="intersection-of-subspaces" class="level2 Theorem">
<h2 class="anchored" data-anchor-id="intersection-of-subspaces">Intersection of subspaces</h2>
<p>If <span class="math inline">\(W_{1}\)</span> and <span class="math inline">\(W_{2}\)</span> are subspaces of <span class="math inline">\(V\)</span> then so is <span class="math inline">\(W_{1}\cap W_{2}\)</span>.</p>
</section>
<div class="Proof">
<p>Let <span class="math inline">\(\vec{u},\vec{v}\in W_{1}\cap W_{2}\)</span> and <span class="math inline">\(a\in K\)</span>. Then <span class="math inline">\(\vec{u}+\vec{v}\in W_{1}\)</span> (because <span class="math inline">\(W_{1}\)</span> is a subspace) and <span class="math inline">\(\vec{u}+\vec{v}\in W_{2}\)</span> (because <span class="math inline">\(W_{2}\)</span> is a subspace). Hence <span class="math inline">\(\vec{u}+\vec{v}\in W_{1}\cap W_{2}\)</span>. Similarly, we get <span class="math inline">\(a\vec{v}\in W_{1}\cap W_{2}\)</span> so <span class="math inline">\(W_{1}\cap W_{2}\)</span> is a subspace of <span class="math inline">\(V\)</span>.&nbsp;◻</p>
</div>
<div class="example">
<p><em>Example 2.34</em>. Let <span class="math inline">\(V= \mathbb{R}^{2\times 2}\)</span> and define <span class="math inline">\(W_{1}\)</span> to be the subspace of matrices of the form <span class="math display">\[\left[
\begin{array}{cc}
a &amp; 0 \\
b &amp; c
\end{array}
\right]\]</span> (<span class="math inline">\(a,b,c\)</span> real) while <span class="math inline">\(W_{2}\)</span> is the subspace of matrices of the form <span class="math inline">\(\left[
\begin{array}{cc}
a &amp; a \\
b &amp; b
\end{array}
\right]\)</span>. Then <span class="math inline">\(W_{1}\cap W_{2}\)</span> consists of all matrices of the form <span class="math inline">\(\left[
\begin{array}{cc}
0 &amp; 0 \\
b &amp; b
\end{array}
\right]\)</span> , which is a subspace of <span class="math inline">\(V\)</span>.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>While <span class="math inline">\(W_{1}\cap W_{2}\)</span> is a subpace, <span class="math inline">\(W_{1}\cup W_{2}\)</span> is in general not a subspace. For exampe if <span class="math inline">\(W_1\)</span> and <span class="math inline">\(W_2\)</span> are the x- and y-axis of <span class="math inline">\(\mathbb{R}^2\)</span>, the union is not a subspace.</p>
</div>
</div>
<div class="example">
<p><em>Example 2.35</em>. Let <span class="math inline">\(V=\mathbb{R}^{2}\)</span>, let <span class="math inline">\(W=\{(a,0)^{T}:a\in\mathbb{R}\}\)</span> and <span class="math inline">\(W_{2}=\{(0,b)^{T}:b\in\mathbb{R}\}\)</span>. Then <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span> are subspaces of <span class="math inline">\(V\)</span>, but <span class="math inline">\(W_{1}\cup W_{2}\)</span> is not a subspace, because <span class="math inline">\((1,0)^{T}\in W_{1}\cup W_{2}\)</span> and <span class="math inline">\((0,1)^{T}\in W_{1}\cup W_{2}\)</span>, but <span class="math inline">\((1,0)^{T}+(0,1)^{T}=(1,1)^{T}\notin W_{1}\cup W_{2}\)</span>.</p>
</div>
<div class="example">
<p><em>Example 2.36</em>. The set <span class="math inline">\(W=\{(x,y,z)^{T}:x^{2}+y^{2}=z\}\)</span> is not a subspace of <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p>
<p>The vectors <span class="math inline">\({\vec u=}\left( 0,1,1\right) ^{T}\)</span> and <span class="math inline">\({\vec v}=\left(
1,2,5\right) ^{T}\)</span> are both in <span class="math inline">\(W\)</span> but <span class="math inline">\({\vec u}+{\vec v}\)</span> is not.</p>
</div>
<p>The three conditions in the definition of a subspace state that every linear combination of vectors of the subspace has to be an element of the subspace. This proves the following theorem:</p>
<section id="span-is-a-subspace" class="level2 Corollary">
<h2 class="anchored" data-anchor-id="span-is-a-subspace">Span is a subspace</h2>
<p><span class="math inline">\({\rm span}({\vec v}_{1},{\vec v}_{2},...,{\vec v}_{n})\)</span> is a subspace of <span class="math inline">\(V\)</span> for <span class="math inline">\({\vec v}_{1},{\vec v}_{2},...,{\vec v}_{n} \in V\)</span>.</p>
</section>
<div class="Proof">
<p><span class="math inline">\(W={\rm span}({\vec v}_{1},{\vec v}_{2},...,{\vec v}_{n})\)</span> is by definition a subset of <span class="math inline">\(V\)</span>. The <span class="math inline">\(\vec{0}\)</span> is in <span class="math inline">\(W\)</span>, which is property (i) in the definition of a subspace, and (ii) and (iii) are also satisfied due to the span including all linear combinations.&nbsp;◻</p>
</div>
<div class="example">
<p><em>Example 2.37</em>. The set <span class="math inline">\(W=\{(x,y,z,w)^{T}:x+y+z=w\}\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^{4}\)</span>. We can show this in two ways: (a) by using the definition of a subspace (<span class="math inline">\(\ref{subspace}\)</span>) or (b) by finding a spanning set and using the above theorem. Method (a) requires checking the three conditions for a subspace:</p>
<ol type="1">
<li><p><span class="math inline">\({\bf 0} = (0,0,0,0)^{T} \in W\)</span> for <span class="math inline">\(x=y=z=0\)</span>.</p></li>
<li><p>With <span class="math inline">\({\bf u} =  (x,y,z,x+y+z)^{T} \in W\)</span> and <span class="math inline">\({\bf v} = (x',y',z',x'+y'+z')^{T} \in W\)</span> also <span class="math inline">\({\bf u} + {\bf v}=  (x+x',y+y',z+z',(x+x')+(y+y')+(z+z'))^{T} \in W\)</span></p></li>
<li><p>With <span class="math inline">\({\bf u} =  (x,y,z,x+y+z)^{T} \in W\)</span> and <span class="math inline">\(\lambda {\bf u} =  (\lambda x,\lambda y,\lambda z, \lambda (x+y+z))^{T} \in W\)</span></p></li>
</ol>
<p>For method (b), we have to find vectors which span <span class="math inline">\(W\)</span>. Note that the set <span class="math inline">\(W\)</span> has three free parameters, which can be used to represent an arbitrary element of <span class="math inline">\(W\)</span>: <span class="math display">\[\scriptscriptstyle \begin{pmatrix}   x\\y\\z\\x+y+z  \end{pmatrix} \textstyle = x \scriptscriptstyle \begin{pmatrix}   1\\0\\0\\1  \end{pmatrix} \textstyle +y \scriptscriptstyle \begin{pmatrix}   0\\1\\0\\1  \end{pmatrix} \textstyle + z \scriptscriptstyle \begin{pmatrix}   0\\0\\1\\1  \end{pmatrix} \textstyle\]</span> Hence <span class="math inline">\(W = {\rm span}((1,0,0,1)^{T}, (0,1,0,1)^{T}, (0,0,1,1)^{T})\)</span>. This second approach has the advantage that the spanning set can also be used to find a basis for the subspace. Indeed, the three vectors are also linearly independent (prove!), and we obtain the basis <span class="math inline">\({\vec v}_{1}=(1,0,0,1)^{T},{\vec v}_{2}=(0,1,0,1)^{T},{\vec v}
_{3}=(0,0,1,1)^{T}\)</span>.</p>
</div>
<section id="range-and-nullspace-of-a-matrix" class="level2">
<h2 class="anchored" data-anchor-id="range-and-nullspace-of-a-matrix">Range and Nullspace of a Matrix</h2>
<p>Consider a system of linear equations, <span class="math inline">\(A\mathbf{x} = \mathbf{y}\)</span> where <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times n\)</span> matrix. We can consider the matrix <span class="math inline">\(A\)</span> as a mapping from the space of all <span class="math inline">\(\mathbf{x}\)</span> vectors in <span class="math inline">\(\mathbb{R}^{n}\)</span> onto the space of all possible <span class="math inline">\(\mathbf{y}\)</span> vectors, which is a subset of <span class="math inline">\(\mathbb{R}^{m}\)</span>. <span class="math display">\[A: \mathbf{x} \in \mathbb{R}^{n} \longrightarrow \mathbf{y} = A \mathbf{x} \in \mathbb{R}^{m}\]</span>.</p>
</section>
<section id="range" class="level2 Corollary">
<h2 class="anchored" data-anchor-id="range">Range</h2>
<p>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(m \times n\)</span> matrix. The image of <span class="math inline">\(\mathbb{R}^{n}\)</span> under <span class="math inline">\(A\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^{m}\)</span> called the range (or column space) of <span class="math inline">\(A\)</span>, denoted by <span class="math inline">\({\rm range}(A)\)</span>, and <span class="math display">\[ {\rm range}(A) = {\rm span}({\rm columns \, of } A)\]</span></p>
</section>
<p>With this result, the question to ask whether a system of linear equations <span class="math inline">\(A\mathbf{x} = \mathbf{b}\)</span> has a solution translates to the question whether <span class="math inline">\(\mathbf{b} \in {\rm range}(A)\)</span>.</p>
<div class="example">
<p><em>Example 2.40</em>. Let <span class="math display">\[A = \left[\begin{array}{ccc}
            1 &amp; 0 &amp; 1 \\
            1 &amp; 1 &amp; 3 \\
            0 &amp; 1 &amp; 2
        \end{array}\right]\]</span></p>
<p>Check whether the system <span class="math inline">\(A\mathbf{x} = \mathbf{b}\)</span> consistent when</p>
<ol type="i">
<li><span class="math inline">\(\mathbf{b} = \left[1,1,1\right]^T\)</span>,</li>
<li><span class="math inline">\(\mathbf{b} = \left[1,2,1\right]^T\)</span>.</li>
</ol>
<p>Under what general conditions on <span class="math inline">\(\mathbf{b} = \left[\mathbf{b_1},\mathbf{b_2},\mathbf{b_3}\right]^T\)</span> is the system consistent?</p>
<div class="Solution">
<p>We reduce <span class="math inline">\(A\)</span> to a row echelon form: <span class="math display">\[\left[\begin{array}{ccc} 1 &amp; 0 &amp; 1 \\ 1 &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; 2 \end{array}\right] \xrightarrow{R_2 - R_1} \left[\begin{array}{ccc} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 1 &amp; 2 \end{array}\right] \xrightarrow{R_3 - R_2} \left[\begin{array}{ccc} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 0 &amp; 0 \end{array}\right]\]</span></p>
<p>Hence, for example, the first two columns span <span class="math inline">\({\rm range}(A)\)</span> and hence <span class="math inline">\(\mathbf{b} = \lambda \left[1,1,0\right]^T + \mu \left[0,1,1\right]^T\)</span> <span class="math inline">\(= \left[\mathbf{b_1}, \mathbf{b_1} + \mathbf{b_3}, \mathbf{b_3}\right]^T\)</span> is necessary for the system to be consistent. This is satisfied for <span class="math inline">\(\mathbf{b} = \left[1,2,1\right]^T\)</span>, but not for <span class="math inline">\(\mathbf{b} = \left[1,1,1\right]^T\)</span>. Note that the first and the third columns of <span class="math inline">\(A\)</span> also span <span class="math inline">\({\rm range}(A)\)</span>. Even the last two columns can be used.</p>
</div>
</div>
<section id="nullspace" class="level2 Definition">
<h2 class="anchored" data-anchor-id="nullspace">Nullspace</h2>
<p>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(m \times n\)</span> matrix. The nullspace of <span class="math inline">\(A\)</span>, is the set of vectors <span class="math inline">\({\bf x}\)</span> such that <span class="math inline">\(A\mathbf{x} = \mathbf{0}\)</span>. The nullspace is also called kernel and denoted by <span class="math inline">\({\rm Null}(A)\)</span>.</p>
</section>
<section id="nullspace-is-a-subspace" class="level2 Corollary">
<h2 class="anchored" data-anchor-id="nullspace-is-a-subspace">Nullspace is a subspace</h2>
<p>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(m \times n\)</span> matrix. The nullspace of <span class="math inline">\(A\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^{n}\)</span>.</p>
</section>
<div class="Proof">
<p>We have to check the three condition in the definition for a subspace (<span class="math inline">\(\ref{subspace}\)</span>):</p>
<ol type="i">
<li><p>Since <span class="math inline">\(A\mathbf{0_n} = \mathbf{0_m},  \mathbf{0_n} \in {\rm Null}(A)\)</span>.</p></li>
<li><p>Let <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{v}\)</span> be in <span class="math inline">\({\rm Null}(A)\)</span>. Therefore <span class="math inline">\(A\mathbf{u} = \mathbf{0}\)</span> and <span class="math inline">\(A\mathbf{v} = \mathbf{0}\)</span>. It follows that <span class="math inline">\(A(\mathbf{u} + \mathbf{v}) = A\mathbf{u} + A\mathbf{v}  = \mathbf{0} + \mathbf{0} = \mathbf{0}\)</span> Hence, <span class="math inline">\(\mathbf{u} + \mathbf{v} \in {\rm Null}(A)\)</span>.</p></li>
<li><p>Finally, for any scalar <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(A(\lambda \mathbf{u}) = \lambda (A\mathbf{u}) = \lambda \mathbf{0} = \mathbf{0}\)</span> and therefore <span class="math inline">\(\lambda \mathbf{u} \in {\rm Null}(A)\)</span>. It follows that <span class="math inline">\({\rm Null}(A)\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^{n}\)</span>. &nbsp;◻</p></li>
</ol>
</div>
<section id="row-and-column-space" class="level2 Definition">
<h2 class="anchored" data-anchor-id="row-and-column-space">Row and column space</h2>
<p>The span of the columns of a <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\({\rm A}\)</span> is called the column space. It is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. The span of the rows of <span class="math inline">\({\rm A}\)</span> is called the row space and it is a subspace of <span class="math inline">\(\mathbb{R}^m\)</span>.</p>
</section>
<section id="dimension-of-row-and-column-spaces" class="level2 Theorem">
<h2 class="anchored" data-anchor-id="dimension-of-row-and-column-spaces">Dimension of row and column spaces</h2>
<p>The row and column spaces of a matrix <span class="math inline">\(A\)</span> have the same dimension.</p>
</section>
<div class="Proof">
<p>Let <span class="math inline">\(R\)</span> be a row echelon form of <span class="math inline">\(A\)</span>, <span class="math inline">\(row(A) = row(R)\)</span>, as we used only row operations to convert <span class="math inline">\(A\)</span> to <span class="math inline">\(R\)</span>. <span class="math display">\[\begin{aligned}
{\rm dim}({\rm row}(A)) &amp;=  {\rm dim}({\rm row}(R)) \\
             &amp; = \mbox{number of nonzero rows of} R \\
             &amp; =  \mbox{number of pivots of} R
\end{aligned} \]</span></p>
<p>Let this number be called <span class="math inline">\(\gamma\)</span>.<br>
Now <span class="math inline">\(col(A) \neq col(R)\)</span>, but the columns of <span class="math inline">\(A\)</span> and <span class="math inline">\(R\)</span> have the same dependence relationships. Therefore, <span class="math inline">\(dim(col(A)) = dim(col(R))\)</span>. Since there are <span class="math inline">\(\gamma\)</span> pivots, <span class="math inline">\(R\)</span> has <span class="math inline">\(\gamma\)</span> columns that are linearly independent, and the remaining columns of <span class="math inline">\(R\)</span> are linear combinations of them. Thus, <span class="math inline">\(dim(col(R)) = \gamma\)</span>. It follows that <span class="math inline">\(dim(row(A)) = \gamma = dim(col(A))\)</span>, as we wished to prove.&nbsp;◻</p>
</div>
<section id="rank-and-nullity" class="level2 Definition">
<h2 class="anchored" data-anchor-id="rank-and-nullity">Rank and nullity</h2>
<p>The rank of a matrix <span class="math inline">\(A\)</span> is the dimension of its row and column spaces and is denoted by <span class="math inline">\({\rm rank}(A)\)</span>. The nullity of a matrix <span class="math inline">\(A\)</span> is the dimension of its nullspace and is denoted by <span class="math inline">\({\rm nullity}(A)\)</span>.</p>
</section>
<section id="rank-theorem" class="level2 Theorem">
<h2 class="anchored" data-anchor-id="rank-theorem">Rank Theorem</h2>
<p>If <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times n\)</span> matrix, then <span class="math inline">\({\rm rank}(A) + {\rm nullity}(A) = n\)</span>, where <span class="math inline">\(n\)</span> is the number of columns of <span class="math inline">\(A\)</span>.</p>
</section>
<div class="Proof">
<p>Let <span class="math inline">\(R\)</span> be a row echelon form of <span class="math inline">\(A\)</span>, and suppose that <span class="math inline">\(rank(A) = \gamma\)</span>. Then <span class="math inline">\(R\)</span> has <span class="math inline">\(\gamma\)</span> pivots, so there are <span class="math inline">\(\gamma\)</span> variables corresponding to the leading entries and <span class="math inline">\(n-\gamma\)</span> free variables in the solution to <span class="math inline">\(A\mathbf{x} = \mathbf{0}\)</span>. Since <span class="math inline">\(dim({\rm Null}(A)) = n - \gamma\)</span>, we have <span class="math inline">\({\rm rank}(A) + {\rm nullity}(A) = \gamma + (n- \gamma ) = n\)</span>.</p>
</div>
<div class="example">
<p><em>Example 2.38</em>. Find the nullity of each of the following matrices:</p>
<p><span class="math display">\[M = \left[\begin{array}{cc} 2 &amp; 3 \\ 1 &amp; 5 \\ 4 &amp; 7 \\ 3 &amp; 6 \end{array}\right]; \qquad N = \left[\begin{array}{cccc} 2 &amp; 1 &amp; -2 &amp; -1 \\ 4 &amp; 4 &amp; -3 &amp; 1 \\ 2 &amp; 7 &amp; 1 &amp; 8 \end{array}\right]\]</span></p>
<div class="Solution">
<p>Since the two columns of <span class="math inline">\(M\)</span> are linearly independent, <span class="math inline">\({\rm rank}(M)=2\)</span>. Thus, by the Rank Theorem, <span class="math inline">\({\rm nullity}(M) = 2 - {\rm rank}(M) = 2 - 2 = 0\)</span>. We now apply row operations to reduce <span class="math inline">\(N\)</span> to a row echelon form given by <span class="math display">\[\left[\begin{array}{cccc} 2 &amp; 1 &amp; -2 &amp; -1 \\ 4 &amp; 4 &amp; -3 &amp; 1 \\ 2 &amp; 7 &amp; 1 &amp; 8 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_2 - 2R_1 \\ R_3 - R_1 \end{subarray}} \left[\begin{array}{cccc} 2 &amp; 1 &amp; -2 &amp; -1 \\ 0 &amp; 2 &amp; 1 &amp; 3 \\ 0 &amp; 6 &amp; 3 &amp; 9 \end{array}\right] \xrightarrow{R_3 - 2R_2} \left[\begin{array}{cccc} 2 &amp; 1 &amp; -2 &amp; -1 \\ 0 &amp; 2 &amp; 1 &amp; 3 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]\]</span> We see that there are only two nonzero rows, so <span class="math inline">\({\rm rank}(N)=2\)</span>. Hence, <span class="math inline">\({\rm nullity}(N) = 4 - {\rm rank}(N) = 4 - 2 = 2\)</span>.</p>
</div>
</div>
<div class="example">
<p><em>Example 2.39</em>. Let <span class="math display">\[A = \left[\begin{array}{cccc}
            1 &amp; 0 &amp; 1 &amp; 2 \\
            2 &amp; 1 &amp; 2 &amp; 5 \\
            1 &amp; 1 &amp; 0 &amp; 2 \\
            0 &amp; 1 &amp; 0 &amp; 1
        \end{array}\right]\]</span> Verify the Rank Theorem.</p>
<div class="Solution">
<p>A row echelon form is given by</p>
<p><span class="math display">\[\left[\begin{array}{cccc} 1 &amp; 0 &amp; 1 &amp; 2 \\ 2 &amp; 1 &amp; 2 &amp; 5 \\ 1 &amp; 1 &amp; 0 &amp; 2 \\ 0 &amp; 1 &amp; 0 &amp; 1 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_2 - 2R_1 \\ R_3 - R_1 \end{subarray}} \left[\begin{array}{cccc} 1 &amp; 0 &amp; 1 &amp; 2 \\ 0 &amp; 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 1 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_3 - R_2 \\ R_4 - R_2 \end{subarray}} \left[\begin{array}{cccc} 1 &amp; 0 &amp; 1 &amp; 2 \\ 0 &amp; 1 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; -1 &amp; -1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]\]</span></p>
<p>Notice that columns <span class="math inline">\(1, 2\)</span> and <span class="math inline">\(3\)</span> have pivots, but not column <span class="math inline">\(4\)</span>, so columns <span class="math inline">\(1, 2\)</span> and <span class="math inline">\(3\)</span> are linearly independent and the <span class="math inline">\({\rm rank}(A) = 3\)</span>.<br>
Solving the equation <span class="math inline">\(A\mathbf{x} = \mathbf{0}\)</span>, we see that all solutions are of the form <span class="math inline">\(\mathbf{x} = c \left[1, 1, 1, -1 \right]^{T}\)</span>, so that the dimension of the nullspace is <span class="math inline">\(1\)</span>. Thus, <span class="math inline">\({\rm rank}(A) + {\rm nullity}(A) = 3 + 1 = 4\)</span>, verifying the Rank Theorem.</p>
</div>
</div>
<div class="example">
<p><em>Example 2.41</em>. Find the rank and nullity of</p>
<div class="center">
<p><span class="math inline">\(A = \left[\begin{array}{ccccc}
            1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\
            0 &amp; 1 &amp; 1 &amp; 2 &amp; 0 \\
            1 &amp; 2 &amp; 1 &amp; 3 &amp; 1 \\
            2 &amp; 3 &amp; 1 &amp; 4 &amp; 2
        \end{array}\right]\)</span></p>
</div>
<p>Identify bases for both the range and nullspace of <span class="math inline">\(A\)</span>.</p>
<div class="Solution">
<p>&nbsp;A row echelon form is given by <span class="math display">\[\left[\begin{array}{ccccc} 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; 0 \\ 1 &amp; 2 &amp; 1 &amp; 3 &amp; 1 \\ 2 &amp; 3 &amp; 1 &amp; 4 &amp; 2 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_{3} - R_{1} \\ R_{4} - 2R_{1} \end{subarray}} \left[\begin{array}{ccccc} 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; 0 \\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; 0 \\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; 0 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_{3} - R_{2} \\ R_{4} - R_{2} \end{subarray}} \left[\begin{array}{ccccc} 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]\]</span></p>
<p>Hence <span class="math inline">\({\rm rank}(A) = 2\)</span> and <span class="math inline">\({\rm nullity}(A) = 5 - {\rm rank}(A) = 5 - 2 = 3\)</span>. Columns <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> have pivots, therefore a basis for <span class="math inline">\({\rm range}(A)\)</span> is <span class="math inline">\(\left\{\left[1,0,1,2\right]^T, \left[1,1,2,3\right]^T\right\}\)</span>.</p>
<p>Let <span class="math inline">\(x_3 = r, x_4 = s\)</span> and <span class="math inline">\(x_5 = t\)</span>, then <span class="math inline">\(x_1 = r + s - t, x_2 = -r - 2s\)</span>, so that <span class="math display">\[\mathbf{x} = r \left[\begin{array}{c} 1 \\ -1 \\ 1 \\ 0 \\ 0 \end{array}\right] + s \left[\begin{array}{c} 1 \\ -2 \\ 0 \\ 1 \\ 0 \end{array}\right] + t \left[\begin{array}{c} -1 \\ 0 \\ 0 \\ 0 \\ 1 \end{array}\right]\]</span> So a basis for the nullspace is <span class="math display">\[\left\{\left[1,-1,1,0,0\right]^T, \left[1,-2,0,1,0\right]^T, \left[-1,0,0,0,1\right]^T\right\}\]</span></p>
<p><strong>Summary of the procedure to find a basis for the nullspace of <span class="math inline">\(A\)</span>.</strong></p>
<ol type="1">
<li>Find a row echelon form <span class="math inline">\(R\)</span> of <span class="math inline">\(A\)</span>.<br>
</li>
<li>Solve for the leading variables (<span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> above) of <span class="math inline">\(R\mathbf{x} = \mathbf{0}\)</span> in terms of the free variables (<span class="math inline">\(x_3, x_4\)</span> and <span class="math inline">\(x_5\)</span> in the example).</li>
<li>Set the free variables equal to parameters, substitute back into <span class="math inline">\(\mathbf{x}\)</span>, and write the results as a linear combination of <span class="math inline">\(f\)</span> vectors (where <span class="math inline">\(f\)</span> is the number of free variables). These <span class="math inline">\(f\)</span> vectors form a basis for <span class="math inline">\({\rm Null}(A)\)</span>.</li>
</ol>
</div>
</div>
</section>
<section id="linear-equations" class="level1">
<h1>Linear equations</h1>
<section id="matrices" class="level2">
<h2 class="anchored" data-anchor-id="matrices">Matrices</h2>
</section>
<section id="matrix" class="level2 Definition">
<h2 class="anchored" data-anchor-id="matrix">Matrix</h2>
<p>An <span class="math inline">\(m\times n\)</span> (read “m by n”) matrix is an array of numbers or expressions with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns. The set of all <span class="math inline">\(m\times n\)</span> matrices is denoted by <span class="math inline">\(M_{m\times n}\)</span>.</p>
</section>
<p>Example: A <span class="math inline">\(2\times3\)</span> matrix: <span class="math display">\[{\rm A} =\left(
\begin{array}{rrr}
1 &amp;  2 &amp; 5  \\
  3 &amp; 4  &amp;   -7
  \end{array}
\right) \in M_{2\times3} ,  \quad A_{1,2} =2 .\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Matrices are usually named by upper case roman letters, e.g.&nbsp;<span class="math inline">\({\rm A}\)</span>. Individual entries are referred to by <span class="math inline">\(A_{i,j}\)</span>, i.e.&nbsp;the entry in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> of the array. Entries are also sometimes denoted by the corresponding lower-case letter, e.g.&nbsp;<span class="math inline">\(a_{i,j}\)</span>. A matrix can be symbolically represented by its general entry: <span class="math inline">\({\rm A}=\left[ a_{i,j}\right]\)</span>.</p>
</div>
</div>
<p>Analogous to the rules of addition and scalar multiplication of vectors, we have the following rules for matrices:</p>
<section id="matrix-sum-and-scalar-multiplication" class="level2 Definition">
<h2 class="anchored" data-anchor-id="matrix-sum-and-scalar-multiplication">Matrix sum and scalar multiplication</h2>
<p>Addition of two matrices of the same type (<span class="math inline">\({\rm A} ,{\rm B} \in M_{m \times n}\)</span>) is defined by <span class="math display">\[{\rm A} + {\rm B}=[a_{i,j}] + [b_{i,j}]= [ a_{i,j}+b_{i,j}], \]</span> and the scalar multiplication by <span class="math display">\[c {\rm A} = c [a_{i,j}] = [ c a_{i,j}] .\]</span></p>
</section>
<p>Example: <span class="math display">\[\left(
\begin{array}{rrr}
1 &amp;  2 &amp; 5  \\
  3 &amp; 4  &amp;   -7
  \end{array}
\right) + \left(
\begin{array}{rrr}
2 &amp;  1 &amp; 0  \\
  -1 &amp; 2  &amp;   3
  \end{array}
\right) = \left(
\begin{array}{rrr}
3 &amp;  3 &amp; 5  \\
  2 &amp; 6  &amp;   -4
  \end{array}
\right)\]</span> <span class="math display">\[3 \left(
\begin{array}{rrr}
1 &amp;  2 &amp; 5  \\
  3 &amp; 4  &amp;   -7
  \end{array}  \right)  =  \left(
\begin{array}{rrr}
3 &amp;  6 &amp; 15  \\
  9 &amp; 12  &amp;   -21
  \end{array}
\right)\]</span> The neutral element of the addition is the <em>zero matrix</em>, <span class="math inline">\({\rm O}\)</span>, where all entries are zero. The inverse element of the addition is the negative matrix <span class="math inline">\(-{\rm A} = [-a_{i,j}]\)</span>. The two operations lead to the following properties:</p>
<section id="basic-matrix-operations" class="level2 Corollary">
<h2 class="anchored" data-anchor-id="basic-matrix-operations">Basic Matrix Operations</h2>
<p><span class="math display">\[\begin{aligned}
{\rm A} + {\rm B} &amp; =  {\rm B} + {\rm A} \quad \mbox{(addition is ccommutative)} \\
  ({\rm A} + {\rm B}) + {\rm C} &amp;= {\rm A} + ({\rm B}  + {\rm C}) \quad \mbox{(addition is associative)} \\
  {\rm A} + {\rm O} &amp; =  {\rm A} \quad \mbox{(Existence of a neutral element for add.)} \\
  {\rm A} + {\rm -A} &amp; =  {\rm O} \quad \mbox{(Existence of an inverse element for add.)} \\
  a ({\rm A} + {\rm B})  &amp;= a {\rm A} + a {\rm B} \quad \mbox{(Distributive law 1)} \\
  (a+b)  {\rm A}  &amp;= a {\rm A} + b {\rm A}   \quad \mbox{(Distributive law 2)} \\
  a(b {\rm A} )&amp;= (ab) {\rm A}  \quad \mbox{(scalar mult. is associative)}
\end{aligned}\]</span></p>
</section>
<p>In addition to the rules above, we have a matrix multiplication:</p>
<section id="matrix-product" class="level2 Definition">
<h2 class="anchored" data-anchor-id="matrix-product">Matrix product</h2>
<p>The matrix product of the <span class="math inline">\(m \! \times \! r\)</span> matrix&nbsp;<span class="math inline">\(G\)</span> and the <span class="math inline">\(r \! \times \! n\)</span> matrix&nbsp;<span class="math inline">\(H\)</span> is the <span class="math inline">\(m \! \times \! n\)</span> matrix&nbsp;<span class="math inline">\(P\)</span>, where <span class="math display">\[p_{i,j}  = g_{i,1}h_{1,j}+g_{i,2}h_{2,j}+\dots+g_{i,r}h_{r,j}\]</span> that is, the <span class="math inline">\(i,j\)</span>-th entry of the product is the dot product of the <span class="math inline">\(i\)</span>-th row and the <span class="math inline">\(j\)</span>-th column. <span class="math display">\[GH=
    \begin{pmatrix}
              &amp;\vdots                     \\
      g_{i,1} &amp;g_{i,2} &amp;\ldots  &amp;g_{i,r}  \\
              &amp;\vdots
    \end{pmatrix}
    \begin{pmatrix}
              &amp;h_{1,j}           \\
      \ldots  &amp;h_{2,j} &amp;\ldots   \\
              &amp;\vdots            \\
              &amp;h_{r,j}
    \end{pmatrix}
  =
    \begin{pmatrix}
              &amp;\vdots            \\
      \ldots  &amp;p_{i,j}  &amp;\ldots  \\
              &amp;\vdots
    \end{pmatrix}\]</span></p>
</section>
<p>Example: <span class="math display">\[\begin{pmatrix}
     2  &amp;0  \\
     4  &amp;6  \\
     8  &amp;2
  \end{pmatrix}
  \begin{pmatrix}
    1  &amp;3  \\
    5  &amp;7
  \end{pmatrix}
  =
  \begin{pmatrix}
   2\cdot 1+0\cdot 5  &amp;2\cdot 3+0\cdot 7  \\
   4\cdot 1+6\cdot 5  &amp;4\cdot 3+6\cdot 7  \\
   8\cdot 1+2\cdot 5  &amp;8\cdot 3+2\cdot 7
  \end{pmatrix}
  =
  \begin{pmatrix}
    2  &amp;6  \\
   34  &amp;54 \\
   18  &amp;38
  \end{pmatrix}\]</span></p>
<section id="matrix-product-rules" class="level2 Corollary">
<h2 class="anchored" data-anchor-id="matrix-product-rules">Matrix product rules</h2>
The matrix multiplication is distributive with respect to the addition and scalar multiplication, and associative but not commutative. $$
<span class="math display">\[\begin{aligned}
({\rm A} + {\rm B} ) {\rm C} &amp; =  {\rm A}  {\rm C} + {\rm B}  {\rm C}  , \\
{\rm C}  ({\rm A} + {\rm B} )  &amp; =  {\rm C} {\rm A}  + {\rm C}  {\rm B} , \\
(a {\rm A})(b {\rm B}) &amp;= (ab) {\rm A}  {\rm B} , \\
({\rm A} {\rm B} ){\rm C}  &amp; = {\rm A} ({\rm B} {\rm C}) \\

\mbox{{\bf but} } \quad {\rm A}  {\rm B} &amp;\neq &amp; {\rm B}  {\rm A} .
\end{aligned}\]</span>
<p>$$</p>
<p>That the matrix multiplication is not commutative in general is clear already for dimensional reasons. If <span class="math inline">\({\rm A} \in M_{m\times r}\)</span> and <span class="math inline">\({\rm B} \in M_{r\times n}\)</span> then <span class="math inline">\({\rm AB}\in M_{m\times n}\)</span>, but the product <span class="math inline">\({\rm BA}\)</span> is not even defined, unless <span class="math inline">\(m=n\)</span>. However, even if we have quadratic matrices <span class="math inline">\((m=r=n)\)</span>, the product is not commutative as the following example shows: <span class="math display">\[\begin{pmatrix}
0 &amp; 0\\
1 &amp;0
  \end{pmatrix}
\begin{pmatrix}
2 &amp; 0\\
0 &amp;0
  \end{pmatrix}
= \begin{pmatrix}
0 &amp; 0\\
2 &amp;0
  \end{pmatrix}  \quad \mbox{but} \quad
  \begin{pmatrix}
2 &amp; 0\\
0 &amp;0
  \end{pmatrix}
  \begin{pmatrix}
0 &amp; 0\\
1 &amp;0
  \end{pmatrix}
= \begin{pmatrix}
0 &amp; 0\\
0 &amp;0
  \end{pmatrix}\]</span></p>
</section>
<section id="examples" class="level5">
<h5 class="anchored" data-anchor-id="examples">Examples</h5>
<p>Let <span class="math display">\[A= \begin{pmatrix}
2  &amp; 1  &amp; {-1}  \\
0  &amp; 2  &amp; 1  \\
1  &amp; 0  &amp; 2  \\
\end{pmatrix} ,\quad B= \begin{pmatrix}
1  &amp; 0 &amp; 1  \\
2  &amp; 1 &amp; 2 \\
\end{pmatrix} ,\quad C= \begin{pmatrix}
1  \\
2  \\
1  \\
\end{pmatrix} .\]</span> <em>AB</em>, <em>CA</em>, <em>CB</em> are not defined. <span class="math display">\[BA= \begin{pmatrix}
3  &amp; 1 &amp; 1 \\
6  &amp; 4 &amp; 3 \\
\end{pmatrix} ,\quad BC= \begin{pmatrix}
2  \\
6  \\
\end{pmatrix} ,\quad
AC= \begin{pmatrix}
3  \\
5  \\
3  \\
\end{pmatrix} ,\quad
A^2= \begin{pmatrix}
3  &amp; 4  &amp; {-3}  \\
1  &amp; 4  &amp; 4  \\
4  &amp; 1  &amp; 3  \\
\end{pmatrix}\]</span>.</p>
<p>The matrix <span class="math inline">\(C\)</span> in the above example shows that vectors <span class="math inline">\(\in \mathbb{R}^{n}\)</span> can be considered as <span class="math inline">\(n\times1\)</span> matrices. Similarly, a <span class="math inline">\(1\times n\)</span> matrix is often called a “row-vector”. We can use the operation of transposition to convert a vector to a “row vector” and vice versa. This operation is defined for arbitrary matrices.</p>
<div class="defn">
<p><strong>Definition 3.4</strong> (Transpose of a Matrix). If <span class="math inline">\(A=[a_{i\,j} ]\)</span> is an <span class="math inline">\(m\times n\)</span> matrix, then the transpose of <span class="math inline">\(A\)</span>, denoted by <span class="math inline">\(A^T\)</span>, is defined as follows: <span class="math display">\[A^T=[a_{j,i}]  \in M_{n\times m}  .\]</span></p>
</div>
<p>This means that, for example, the first row of <span class="math inline">\(A^T\)</span> is the first column of <span class="math inline">\(A\)</span> and so on. Note that unless we have a square matrix (<span class="math inline">\(m=n\)</span>), the transpose of a matrix is of a different type than the original matrix. In particular, the transpose of a column matrix (i.e.&nbsp;a <span class="math inline">\(n\times 1\)</span> matrix ) is a row matrix, and vice versa.</p>
</section>
<section id="example" class="level5">
<h5 class="anchored" data-anchor-id="example">Example</h5>
<p><span class="math display">\[\left( \begin{array}{rrr}
2  &amp; 0  &amp; 1  \\
1  &amp; 0  &amp; 3  
\end{array}  \right)^T=\left( \begin{array}{rrr}
2  &amp; 1  \\
0 &amp; 0  \\
1 &amp; 3 \\
\end{array}  \right)    \qquad  (2, 0 , 1)^{T} = \scriptstyle \begin{pmatrix}   2\\ 0 \\ 1  \end{pmatrix} \textstyle\]</span></p>
</section>
<section id="properties-of-the-transpose" class="level5">
<h5 class="anchored" data-anchor-id="properties-of-the-transpose">Properties of the Transpose</h5>
<p><span class="math display">\[\begin{aligned}
&amp;(i)&amp; ({\rm A}^T)^T  =  {\rm A}   \\
&amp;(ii)&amp; ({\rm A}+{\rm B})^T  =  {\rm A}^T+{\rm B}^T \\
&amp;(iii)&amp;({\rm A}{\rm B})^T  =  {\rm B}^T{\rm A}^T
\end{aligned}\]</span></p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span></p>
<ol type="i">
<li>follows directly from the definition. (ii): <span class="math display">\[\left( ({\rm A} + {\rm B})^{T}\right)_{ij} =  ({\rm A} + {\rm B})_{ji} = {\rm A}_{ji} + {\rm B}_{ji}  = ({\rm A}^{T})_{ij} + ({\rm B}^{T})_{ij}\]</span></li>
<li>This requires, of course, that the product is defined, that is <span class="math inline">\({\ rm A}\in M_{m \times r}\)</span> and <span class="math inline">\({\rm B}\in M_{r \times n}\)</span>. <span class="math display">\[\begin{aligned}
({\rm A} {\rm B})_{ij} &amp; = \sum_{s=1}^{r} A_{is}B_{sj} \\
\left( ({\rm A} {\rm B})^{T}\right)_{ij} &amp; = \sum_{s=1}^{r} A_{js}B_{si} \\
&amp; = \sum_{s=1}^{r} ({\rm A}^{T})_{sj}({\rm B}^{T})_{is}  \\
&amp; =\sum_{s=1}^{r} ({\rm B}^{T})_{is} ({\rm A}^{T})_{sj} = {\rm B}^{T} {\rm A}^{T}
\end{aligned}\]</span>&nbsp;◻</li>
</ol>
</div>
</section>
<section id="subsec:symmetric" class="level2">
<h2 class="anchored" data-anchor-id="subsec:symmetric">Symmetric and Skew-symmetric Matrices</h2>
<div class="defn">
<p><strong>Definition 3.5</strong>. If <span class="math inline">\({\rm A}={\rm A}^T\)</span> then <span class="math inline">\({\rm A}\)</span> is said to be symmetric, while if <span class="math inline">\({\rm A}= - {\rm A}^T\)</span> then <span class="math inline">\({\rm A}\)</span> is skew-symmetric. Note that this implies that <span class="math inline">\({\rm A}\)</span> is a square matrix.</p>
</div>
<p>Some matrices are neither symmetric nor skew-symmetric. However, every (square) matrix can be written uniquely as the sum of a symmetric and skew-symmetric matrix because of the identity <span class="math display">\[{\rm A} = \frac{{\rm A}^{T}+{\rm A}}{2} + \frac{{\rm A}- {\rm A}^{T}}{2} ,\]</span> noting that <span class="math inline">\((A^{T}+A)/2\)</span> is symmetric, and <span class="math inline">\(({\rm A}- {\rm A}^{T} )/2\)</span> is skew-symmetric.</p>
<section id="example-1" class="level5">
<h5 class="anchored" data-anchor-id="example-1">Example</h5>
<p><span class="math display">\[\left( \begin{array}{rrr}
2  &amp; 0  &amp; {-4}  \\
{-2}  &amp; 2  &amp; 0  \\
2  &amp; 4  &amp; 6  
\end{array}  \right) =\left(\begin{array}{rrr}
2  &amp; {-1}  &amp; {-1}  \\
{-1}  &amp; 2  &amp; 2  \\
{-1}  &amp; 2  &amp; 6  
\end{array}  \right)+\left( \begin{array}{rrr}
0  &amp; 1  &amp; {-3}  \\
{-1}  &amp; 0  &amp; {-2}  \\
3  &amp; 2  &amp; 0  
\end{array}  \right).\]</span></p>
</section>
</section>
<section id="inverse-matrix" class="level2">
<h2 class="anchored" data-anchor-id="inverse-matrix">Inverse Matrix</h2>
<p>There exists a neutral element <span class="math inline">\({\rm I} \in M_{n\times n}\)</span> called the identity matrix for the multiplication, which consists of 1’s down the main diagonal and 0’s everywhere else: <span class="math display">\[{\rm I} =  \begin{pmatrix}
          1 &amp;0  &amp;\ldots   &amp;0  \\
          0 &amp;1  &amp;   &amp;\vdots   \\
           \vdots  &amp;   &amp; \ddots  &amp;0 \\
          0   &amp;  \ldots &amp;  0&amp;1
        \end{pmatrix}\]</span></p>
<p>The existence of a neutral element with respect to matrix multiplication means: <span class="math display">\[\begin{aligned}
{\rm A} {\rm I} &amp; = {\rm A}  \quad \mbox{and} \quad  {\rm I}  {\rm A} = {\rm A}  \quad \mbox{for all} \ {\rm A} \in M_ {m\times r}.
\end{aligned}\]</span> Note that if <span class="math inline">\({\rm A} \in M_{m\times r}\)</span> then in the first equation <span class="math inline">\({\rm I}\in M_{r\times r}\)</span> while in the second equation <span class="math inline">\({\rm I}\in M_{m\times m}\)</span>.</p>
<div class="defn">
<p><strong>Definition 3.6</strong>. Given two matrices <span class="math inline">\({\rm A}\)</span> and <span class="math inline">\({\rm B}\)</span> which satisfy <span class="math display">\[{\rm A} {\rm B} =  {\rm I}  \ \mbox{\bf and}  \  {\rm B}  {\rm A} = {\rm I}\]</span> then <span class="math inline">\({\rm B} = {\rm A}^{-1}\)</span> is called the inverse of <span class="math inline">\({\rm A}\)</span>. If <span class="math inline">\({\rm A}\)</span> has an inverse, it is said to be non-singular. Any matrix which does not have an inverse is said to be singular.</p>
</div>
<p>The identity matrix in the definition is the same for both equations <span class="math inline">\({\rm A} {\rm B}  =   {\rm I} = {\rm B} {\rm A}\)</span>. Hence, for both products to be defined, the matrix <span class="math inline">\({\rm A}\)</span> has to be square. Non-square matrices can never have an inverse.</p>
<section id="example-2" class="level5">
<h5 class="anchored" data-anchor-id="example-2">Example</h5>
<p><span class="math display">\[A=\left[ \begin{array}{ccc}
2 &amp; 0 &amp; 1 \\
1 &amp; {-2} &amp; 1 \\
3 &amp; 1 &amp; 1
\end{array}  \right], \quad
A^{-1}=\left[ \begin{array}{ccc}
{-3} &amp; 1 &amp; 2 \\
2 &amp; {-1} &amp; {-1} \\
7 &amp; {-2} &amp; {-4} \\
\end{array}  \right]\]</span></p>
<div class="prop">
<p><strong>Proposition 3.1</strong>. <em>If <span class="math inline">\({\rm A}\)</span> and <span class="math inline">\({\rm B}\)</span> are non-singular <span class="math inline">\(n\times n\)</span> matrices then <span class="math display">\[({\rm A}^{-1})^T  =  ({\rm A}^T)^{-1}\]</span> and <span class="math display">\[({\rm A}{\rm B})^{-1}={\rm B}^{-1}{\rm A}^{-1}\]</span></em></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>From <span class="math display">\[\begin{aligned}
{\rm I} &amp; =   {\rm I}^T =  ({\rm A}^{-1}{\rm A})^T  =  {\rm A}^T\left({\rm A}^{-1}\right)^{T} , \\
\mbox{and} \quad {\rm I} &amp; =   {\rm I}^T =  ({\rm A} {\rm A}^{-1})^T  = \left( {\rm A}^{-1}\right)^{T} {\rm A}^T , \\
\Rightarrow \left({\rm A}^{-1}\right)^T  &amp; = \left({\rm A}^T\right)^{-1} .
\end{aligned}\]</span> The second identity is proved by <span class="math display">\[(AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=AI_n A^{-1}=AA^{-1}=I_n .\]</span> and the corresponding reverse sequence: <span class="math display">\[(B^{-1}A^{-1}(AB))=B^{-1}A^{-1}AB=I_n .\]</span>&nbsp;◻</p>
</div>
</section>
</section>
<section id="gaussian-elimination" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-elimination">Gaussian Elimination</h2>
<p>In the previous sections, we considered individual equations of the type <span class="math display">\[a_{1} x_{1} + a_{2} x_{2}  + ... + a_{n} x_{n} = d.\]</span> In the following, we will consider systems of such equations and use a method called <em>Gauss elimination</em> to determine solutions.</p>
We start with a simple example. Consider the system <span class="math display">\[\begin{aligned}
2x + 3y  &amp;= &amp; 1 , \label{firsteq} \\
x+y &amp;= 2 \ .  \label{secondeq}
\end{aligned}\]</span> Usually we would solve the system by solving e.g.&nbsp;the second equation for <span class="math inline">\(x\)</span> and substituting the result in the first equation: $$
<span class="math display">\[\begin{aligned}
2(2-y) + 3y &amp; = 1\nonumber  \\
\Leftrightarrow   (3-2\cdot 1)y &amp; = 1- 2\cdot 2  \label{subseq} \\
\Rightarrow y &amp; =-3 \nonumber
\end{aligned}\]</span>
<span class="math display">\[ The result $y=-3$ is then substituted back in either of
the two equations, and we find $x=5$. When it comes to systems with
large numbers of variables and equations, this approach becomes
increasingly hard to follow, and we need a more systematic approach.
Note that Eq. [\[subseq\]](#subseq){reference-type="ref"
reference="subseq"} can be understood as the result of subtracting twice
Eq. ([\[secondeq\]](#secondeq){reference-type="ref"
reference="secondeq"}) from Eq.
([\[firsteq\]](#firsteq){reference-type="ref" reference="firsteq"}).
That is, we converted the system \]</span>
<span class="math display">\[\begin{aligned}
2x + 3y  &amp;= &amp; 1 ,\\
2x+ 2y &amp;= 4 \ .
\end{aligned}\]</span>
<span class="math display">\[ into the equivalent system \]</span>
<span class="math display">\[\begin{aligned}
2x + 3y  &amp;= &amp; 1 ,\\
      - y &amp;= 3 \ ,
\end{aligned}\]</span>
<p>$$ by subtracting equations. "Equivalent" here means that both systems have the same solutions. This system is now trivial to solve. Operations which lead to equivalent systems are, e.g., multiplying an equation by a (non-zero) number and, as we have seen above, adding multiples of other equations.</p>
<section id="example-1-1" class="level5">
<h5 class="anchored" data-anchor-id="example-1-1">Example 1</h5>
<p>Let us try this method with a more complicated system. We indicate on the right-hand side of the system the operation (multiplication and subtraction) which leads to the new system. <span class="math display">\[\begin{aligned}
x+\phantom{2}y-\phantom{2}z+2t &amp;= 12,\\
2x-\phantom{2}y+\phantom{2}z-\phantom{2}t &amp;=-5,    \qquad Eq. 2- 2 Eq. 1\\
x-2y+3z+4t &amp;= 10, \qquad Eq. 3-  Eq. 1\\
3x+3y+\phantom{2}z+\phantom{2}t &amp;= 12. \qquad Eq. 4- 3 Eq. 1
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
x+\phantom{2}y-\phantom{2}z+2t &amp;= 12,\\
\Leftrightarrow \qquad  0 - 3 y +3 z- 5 t &amp;=-29,    \qquad  \\
0-3y+4z+2t &amp;= -2, \qquad Eq. 3-  Eq. 2\\
0+\phantom{2}0 +4 z -5 t &amp;= -24.
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
x+\phantom{2}y-\phantom{2}z+2t &amp;= 12,\\
\Leftrightarrow \qquad  0 - 3 y +3 z- 5 t &amp;=-29,    \qquad  \\
0 + \phantom{2}0 + \phantom{1} z+7t &amp;= 27,\\
0+\phantom{2}0 +4 z -5 t &amp;= -24.  \qquad  Eq.4- 4 Eq. 3
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
x+\phantom{2}y-\phantom{2}z+2t &amp;= 12,\\
\Leftrightarrow \qquad   0 - 3 y +3 z- 5 t &amp;=-29,    \qquad  \phantom{Eq.4- 4 Eq. 3} \\
0 + \phantom{2}0 + \phantom{1} z+7t &amp;= 27,\\
0+\phantom{2}0 + \phantom{2}0  -33 t &amp;= -132.  
\end{aligned}\]</span> Now the system can be solved recursively: The last equation implies <span class="math inline">\(t=4\)</span>, which can be used in the third equation to find <span class="math inline">\(z=-1\)</span>. This, in turn, leads to <span class="math inline">\(y=2\)</span> in the second equation, and eventually, we get <span class="math inline">\(x=1\)</span> from the first equation.</p>
<p>Note that it was now possible to easily solve the system because the non-zero entries on the left-hand side form an upper triangle, so that we can successively solve for all the variables (back-substitution).</p>
<p>To make the notation more compact, we can suppress the variables and write the system as a matrix, called <em>augmented matrix</em>. For the example from above, we have e.g.&nbsp;</p>
<p><span class="math display">\[\begin{array}{rrrr|rl}
1 &amp;   1&amp; -1 &amp; 2&amp; 12 &amp; \\
2 &amp;  -1 &amp; 1&amp; -1 &amp;-5 &amp; (R_{2}-2R_{1}\rightarrow R_{2})\\
1  &amp;   -2&amp; 3 &amp; 4  &amp; 10 &amp; (R_{3}-R_{1}\rightarrow R_{3})\\
3 &amp; 3 &amp;1 &amp; 1 &amp; 12  &amp;   (R_{4}-3R_{1}\rightarrow R_{4})
\end{array}  \quad \Rightarrow \quad
\begin{array}{rrrr|rl}
1 &amp;   1&amp; -1 &amp; 2&amp; 12  &amp;\\
0&amp;  -3 &amp; 3&amp; -5 &amp;-29 &amp; \\
0  &amp;   -3&amp; 4 &amp; 2  &amp; -2 &amp; (R_{3}-R_{2}\rightarrow R_{3}) \\
0 &amp; 0 &amp;4 &amp; -5 &amp; -24 &amp;    
\end{array}\]</span> <span class="math display">\[\Rightarrow \begin{array}{rrrr|rl}
1 &amp;   1&amp; -1 &amp; 2&amp; 12 &amp; \\
0&amp;  -3 &amp; 3&amp; -5 &amp;-29 &amp;\\
0  &amp;   0&amp; 1 &amp; 7  &amp; 27 &amp;\\
0 &amp; 0 &amp;4 &amp; -5 &amp; -24   &amp; (R_{4}-4R_{3}\rightarrow R_{4})
\end{array}
\quad \Rightarrow \begin{array}{rrrr|r}
1 &amp;   1&amp; -1 &amp; 2&amp; 12  \\
0&amp;  -3 &amp; 3&amp; -5 &amp;-29 \\
0  &amp;   0&amp; 1 &amp; 7  &amp; 27 \\
0 &amp; 0 &amp;0 &amp; -33 &amp; -132    
\end{array}\]</span></p>
<p>This method is called Gauss elimination, named after Carl Friedrich Gauss (see Appendix <a href="#Gauss" data-reference-type="ref" data-reference="Gauss">[Gauss]</a>). To bring the system into upper triangular form, we can use three rules which do not change the solutions of the system of equations:</p>
<ol type="1">
<li><p>multiply an equation by an arbitrary non-zero number,</p></li>
<li><p>adding (or subtracting) multiples of equations to other equations (not itself !),</p></li>
<li><p>change the sequence of equations.</p></li>
</ol>
<p>There are two cases where the system can fail to give a unique solution. The first case is that the system is <em>underdetermined</em> (or has infinitely many solutions), that is, we either have fewer equations than variables or the equations are not linearly independent. In this case, we are left with one (or more) free variables, and the best we can do is express all the other variables in terms of this free variable.</p>
</section>
<section id="example-2-1" class="level5">
<h5 class="anchored" data-anchor-id="example-2-1">Example 2</h5>
<p><span class="math display">\[\begin{aligned}
&amp; &amp;\begin{array}{rrrr|r}
1 &amp;   1&amp; -1 &amp; 2&amp; 12  \\
2 &amp;  -1 &amp; 1&amp; -1 &amp;-5 \\
1  &amp;   -2&amp; 3 &amp; 4  &amp; 10 \\
2 &amp; 2 &amp; -3 &amp; -3 &amp; -3    
\end{array}  \quad \Rightarrow \quad
\begin{array}{rrrr|r}
1 &amp;   1&amp; -1 &amp; 2&amp; 12  \\
0&amp;   -3 &amp; 3&amp; -5 &amp;-29 \\
0&amp;  -3&amp; 4 &amp; 2  &amp; -2 \\
  0&amp;  0 &amp;-1 &amp; -7 &amp; -27    
\end{array} \\
&amp;\Rightarrow&amp; \begin{array}{rrrr|r}
1 &amp;   1&amp; -1 &amp; 2&amp; 12  \\
0&amp;   -3 &amp; 3&amp; -5 &amp;-29 \\
0&amp; 0 &amp; 1 &amp; 7  &amp; 27 \\
0&amp; 0 &amp; -1 &amp; -7 &amp; -27    
\end{array} \Rightarrow \begin{array}{rrrr|r}
1 &amp;   1&amp; -1 &amp; 2&amp; 12  \\
0&amp;   -3 &amp; 3&amp; -5 &amp;-29 \\
0&amp; 0 &amp; 1 &amp; 7  &amp; 27 \\
0 &amp; 0 &amp; 0 &amp;  0 &amp; 0    
\end{array}
\end{aligned}\]</span> Here, the last equation does not determine the variable <span class="math inline">\(t\)</span>. We can, however, express all the other variables in terms of <span class="math inline">\(t\)</span>. The third step yields <span class="math inline">\(z=27-7 t\)</span>, the second step: <span class="math inline">\(y=110/3-26/3t\)</span> and eventually we find <span class="math inline">\(x= 7/3-t/3\)</span>. Note that we can write this as the parametric form of a line <span class="math display">\[\vec{r} = \scriptscriptstyle \begin{pmatrix}   7/3 \\ 110/3 \\ 27 \\ 0  \end{pmatrix} \textstyle + t \scriptscriptstyle \begin{pmatrix}   -1/3 \\ -26/3 \\ -7 \\ 1  \end{pmatrix} \textstyle  .\]</span> Indeed, we would have the same situation if we had ignored the last equation in the first place. The equation is superfluous; unfortunately, it is often not trivial to recognise which equation can be omitted if a system is underdetermined.</p>
<p>The other case where the method can fail to produce a unique solution is when the system is <em>overdetermined</em> or <em>inconsistent</em>.</p>
</section>
<section id="example-3" class="level5">
<h5 class="anchored" data-anchor-id="example-3">Example 3</h5>
<p><span class="math display">\[\begin{array}{rrr|r}
  1 &amp; 0&amp; 1 &amp;1 \\
   0&amp; 1 &amp; 1  &amp; 2 \\
    1 &amp; 2 &amp; 3 &amp; -3    
\end{array}
\Rightarrow
\begin{array}{rrr|r}
  1 &amp; 0&amp; 1 &amp;1 \\
   0&amp; 1 &amp; 1  &amp; 2 \\
    0 &amp; 2 &amp; 2 &amp; -4    
\end{array}
\Rightarrow
\begin{array}{rrr|r}
  1 &amp; 0&amp; 1 &amp;1 \\
   0&amp; 1 &amp; 1  &amp; 2 \\
    0 &amp; 0 &amp; 0 &amp; -8    
\end{array}\]</span> In this case, the last equation states: <span class="math inline">\(0\cdot z=-8\)</span>, which is impossible to satisfy for any <span class="math inline">\(z\)</span>. This case often occurs if there are more equations than variables.</p>
<p>All three cases —the case of a unique solution, the overdetermined case, and the underdetermined case — can be understood in geometric terms. We recall that the solution to each equation is a hyperplane in <span class="math inline">\(\mathbb{R}^{n}\)</span>. Hence, the solution to the whole system is the intersection of all these hyperplanes. Two basic situations can be distinguished.</p>
<ol type="1">
<li><p>The set of normal vectors of the planes is linearly independent. The dimension of the space of solutions is <span class="math inline">\(m=n-k\)</span>, where <span class="math inline">\(n\)</span> is the number of unknowns and <span class="math inline">\(k\)</span> is the number of equations, that is, all variables can be expressed in terms of <span class="math inline">\(m\)</span> free variables or parameters. For the case <span class="math inline">\(k=n\)</span>, there are no free parameters, and we obtain a unique solution.</p></li>
<li><p>The set of normal vectors of the hyperplanes is linearly dependent. This is always the case if <span class="math inline">\(k&gt;n\)</span>.</p>
<ol type="1">
<li><p>One (or more) equations are linear combinations of the others. These equations can be removed from the system, until we are left with a set of linearly independent equations. If the normal vectors are linearly independent, case 1 applies; otherwise, we have case 2b).</p></li>
<li><p>The equations are linearly independent and the system has no solution.</p></li>
</ol></li>
</ol>
<p>In Example 1 we had the situation of case 1 with <span class="math inline">\(k=n\)</span> and hence a unique solution.<br>
In Example 2, we had the situation that one equation is a linear combination of the others: <span class="math inline">\((Eq.~4) = (Eq.~1)+(Eq.~2) - (Eq.~3)\)</span>. This is case 2a) in the above scheme. If we remove this equation, we have situation 1 with <span class="math inline">\(n=4\)</span>, <span class="math inline">\(k=3\)</span> and therefore <span class="math inline">\(m=1\)</span>, which corresponds to a line as a solution.</p>
<p>In Example 3, we had case 2b). The normal vectors of the three planes are <span class="math display">\[\vec{n}_{1} = \scriptscriptstyle \begin{pmatrix}   1 \\ 0 \\ 1  \end{pmatrix} \textstyle, \quad \vec{n}_{2} = \scriptscriptstyle \begin{pmatrix}   0 \\ 1 \\ 1  \end{pmatrix} \textstyle, \quad \vec{n}_{1} = \scriptscriptstyle \begin{pmatrix}   1 \\ 2 \\ 3  \end{pmatrix} \textstyle .\]</span> They are not linearly independent <span class="math inline">\(\vec{n}_{3} = \vec{n}_{1} +2 \vec{n}_{2}\)</span>. But the equations are linearly independent, i.e.&nbsp;<span class="math inline">\(Eq. 3 \neq Eq_{1} + 2 Eq. 2\)</span>.</p>
<figure class="figure">
<p>
<img src="hyperpl-r3.jpg" style="width:6cm" alt="image" class="figure-img"> <img src="hyperpl-r3b.jpg" style="width:6cm" alt="image" class="figure-img">
</p>
<figcaption>
The left figure is the situation in Example 3. The three hyperplanes intersect in three parallel lines. The right figure shows the situation of three linearly independent hyperplanes in <span class="math inline">ℝ<sup>3</sup></span> which have one point in common. This is situation 1.
</figcaption>
</figure>
</section>
<section id="elementary-matrices" class="level3">
<h3 class="anchored" data-anchor-id="elementary-matrices">Elementary Matrices</h3>
<p>Each of the three transformations we performed on the augmented matrix can be achieved by multiplying the matrix on the left by an <em>elementary matrix</em>. The corresponding elementary matrix can be found by applying one of the three elementary row transformations to the identity matrix.</p>
<div class="definition">
<p><em>Definition 3.1</em>. An elementary matrix is an <span class="math inline">\(n\times n\)</span> matrix which can be obtained from the identity matrix <span class="math inline">\(I_{n}\)</span> by performing on <span class="math inline">\(I_{n}\)</span> a single elementary row transformation.</p>
</div>
<div class="example">
<p><em>Example 3.1</em>. <span class="math inline">\(\left(\begin{array}{ccc}1 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 1\end{array}\right)\)</span> is an elementary matrix. It can be obtained by multiplying row <span class="math inline">\(2\)</span> of the identity matrix by <span class="math inline">\(3\)</span>. In other words, we are performing the row operation <span class="math inline">\(3R_{2}\rightarrow R_{2}\)</span>.</p>
</div>
<div class="example">
<p><em>Example 3.2</em>. <span class="math inline">\(\left(\begin{array}{ccc}1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -2 &amp; 0 &amp; 1\end{array}\right)\)</span> is an elementary matrix. It can be obtained by replacing row <span class="math inline">\(3\)</span> of the identity matrix by row <span class="math inline">\(3\)</span> plus <span class="math inline">\(-2\)</span> times row <span class="math inline">\(1\)</span>. In other words, we are performing the row operation <span class="math inline">\(R_{3}-2 R_{1}\rightarrow R_{1}\)</span>.</p>
</div>
<div class="example">
<p><em>Example 3.3</em>. <span class="math inline">\(\left(\begin{array}{ccc}0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1\end{array}\right)\)</span> is an elementary matrix. It can be obtained by switching rows <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> of the identity matrix. In other words, we are performing on the row operation <span class="math inline">\(R_{1}\leftrightarrow R_{2}\)</span>.</p>
</div>
<p>Suppose we want to perform an elementary row operation on a matrix <span class="math inline">\(A\)</span>. In that case, it is equivalent to multiplying the matrix <span class="math inline">\(A\)</span> on the left by the elementary matrix obtained from the identity matrix by the same transformation.</p>
<div class="prop">
<p><strong>Proposition 3.2</strong>.</p>
<dl>
<dt><strong>Interchanging Rows: <span class="math inline">\(R_{i}\leftrightarrow R_{j}\)</span></strong></dt>
<dd>
<p><em>To interchange rows <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> of matrix <span class="math inline">\(A\)</span> (<span class="math inline">\(R_{i}\leftrightarrow R_{j}\)</span>), we multiply <span class="math inline">\(A\)</span> on the left by the elementary matrix obtained from the identity matrix in which rows <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> have been interchanged.</em></p>
</dd>
<dt>Multiplying a Row by a Constant: <span class="math inline">\(aR_{i}\rightarrow R_{i}\)</span></dt>
<dd>
<p><em>To multiply row <span class="math inline">\(i\)</span> of matrix <span class="math inline">\(A\)</span> by a number <span class="math inline">\(a\)</span> (<span class="math inline">\(aR_{i}\rightarrow R_{i}\)</span>), we multiply <span class="math inline">\(A\)</span> on the left by the elementary matrix obtained from the identity matrix in which row <span class="math inline">\(i\)</span> has been multiplied by <span class="math inline">\(a\)</span>.</em></p>
</dd>
<dt>Replacing a Row by Itself Plus a Multiple of Another: <span class="math inline">\(R_{i}+aR_{j}\rightarrow R_{i}\)</span></dt>
<dd>
<p><em>To replace a row <span class="math inline">\(i\)</span> by itself plus a multiple of another row <span class="math inline">\(j\)</span> (<span class="math inline">\(R_{i}+aR_{j}\rightarrow R_{i}\)</span>), we multiply <span class="math inline">\(A\)</span> on the left by the elementary matrix obtained from the identity matrix in which row <span class="math inline">\(i\)</span> has been replaced by itself plus row <span class="math inline">\(j\)</span> multiplied by <span class="math inline">\(a\)</span>.</em></p>
</dd>
</dl>
</div>
<div class="example">
<p><em>Example 3.4</em>. <span class="math inline">\(R_{1}\leftrightarrow R_{3}\)</span>: <span class="math display">\[\left(\begin{array}{ccc}0 &amp; 0 &amp; 1\\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0\end{array}\right)\left(\begin{array}{ccc}1 &amp; 2 &amp; 3\\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9\end{array}\right)
=\left(\begin{array}{ccc}7 &amp; 8 &amp; 9\\ 4 &amp; 5 &amp; 6 \\ 1 &amp; 2 &amp; 3\end{array}\right).\]</span></p>
</div>
<div class="example">
<p><em>Example 3.5</em>. <span class="math inline">\(3R_{1}\rightarrow R_{1}\)</span>: <span class="math display">\[\left(\begin{array}{ccc}3 &amp; 0 &amp; 0\\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1\end{array}\right)\left(\begin{array}{ccc}1 &amp; 2 &amp; 3\\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9\end{array}\right)
=\left(\begin{array}{ccc}3 &amp; 6 &amp; 9\\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9\end{array}\right).\]</span></p>
</div>
<div class="example">
<p><em>Example 3.6</em>. <span class="math inline">\(R_{2}-R_{1}\rightarrow R_{2}\)</span>: <span class="math display">\[\left(\begin{array}{ccc}1 &amp; 0 &amp; 0\\ -1 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1\end{array}\right)\left(\begin{array}{ccc}1 &amp; 2 &amp; 3\\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9\end{array}\right)
=\left(\begin{array}{ccc}1 &amp; 2 &amp; 3\\ 3 &amp; 3 &amp; 3 \\ 7 &amp; 8 &amp; 9\end{array}\right).\]</span></p>
</div>
<div class="thm">
<p><strong>Theorem 3.3</strong>. <em>The elementary matrices are nonsingular. Furthermore, their inverse is also an elementary matrix. That is, we have:</em></p>
<ul>
<li><p><em>The inverse of the elementary matrix which interchanges two rows is itself. For example the inverse of <span class="math inline">\(\left(\begin{array}{ccc}0 &amp; 1 &amp; 0\\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1\end{array}\right)\)</span> is itself <span class="math inline">\(\left(\begin{array}{ccc}0 &amp; 1 &amp; 0\\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1\end{array}\right)\)</span>.</em></p></li>
<li><p><em>The inverse of the elementary matrix which multiply a row <span class="math inline">\(i\)</span> by a constant <span class="math inline">\(a\)</span>, i.e.&nbsp;<span class="math inline">\(aR_{i}\rightarrow R_{i}\)</span> is the elementary matrix which multiply a row <span class="math inline">\(i\)</span> by <span class="math inline">\(\frac{1}{a}\)</span>, i.e. <span class="math inline">\(\frac{1}{a}R_{i}\rightarrow R_{i}\)</span>. For example, the inverse of <span class="math inline">\(\left(\begin{array}{ccc}1 &amp; 0 &amp; 0\\ 0 &amp; a &amp; 0 \\ 0 &amp; 0 &amp; 1\end{array}\right)\)</span> is the matrix <span class="math inline">\(\left(\begin{array}{ccc}1 &amp; 0 &amp; 0\\ 0 &amp; \frac{1}{a} &amp; 0 \\ 0 &amp; 0 &amp; 1\end{array}\right)\)</span>.</em></p></li>
<li><p><em>The inverse of the elementary matrix which replaces a row <span class="math inline">\(i\)</span> by itself plus a multiple of a row <span class="math inline">\(j\)</span>, i.e. <span class="math inline">\(R_{i}+aR_{j}\rightarrow R_{i}\)</span> is the elementary matrix which replaces a row <span class="math inline">\(i\)</span> by itself minus a multiple of a row <span class="math inline">\(j\)</span>. For example, the inverse of the matrix <span class="math inline">\(\left(\begin{array}{ccc}1 &amp; 0 &amp; 0\\ 0 &amp; 1 &amp; 0 \\ a &amp; 0 &amp; 1\end{array}\right)\)</span> is the matrix <span class="math inline">\(\left(\begin{array}{ccc}1 &amp; 0 &amp; 0\\ 0 &amp; 1 &amp; 0 \\ -a &amp; 0 &amp; 1\end{array}\right)\)</span>.</em></p></li>
</ul>
</div>
</section>
</section>
<section id="calculating-the-inverse" class="level2">
<h2 class="anchored" data-anchor-id="calculating-the-inverse">Calculating the inverse</h2>
<p>Now we can prove that a matrix is invertible if we can convert it to the identity matrix with elementary row operations. <span class="math display">\[\begin{aligned}
{\rm I} &amp;= {\rm E}_{k} ... {\rm E}_{2} {\rm E}_{1} {\rm A}  \label{matrixinv1}\\
\Rightarrow   {\rm I} &amp; = {\rm B}  {\rm A}  \quad \mbox{with} \quad  {\rm B} = {\rm E}_{k} ... {\rm E}_{2} {\rm E}_{1} {\rm I}  \nonumber
\end{aligned}\]</span> To show that <span class="math inline">\({\rm B}\)</span> is the inverse, we have to show in addition <span class="math inline">\({\rm A}{\rm B} = {\rm I}\)</span>. From Eq. <a href="#matrixinv1" data-reference-type="ref" data-reference="matrixinv1">[matrixinv1]</a> it follows by multiplying successively with the inverse of the elementary matrices <span class="math inline">\({\rm E}_{j}\)</span> from the left: <span class="math display">\[\begin{aligned}
{\rm E}_{k}^{-1}{\rm I} &amp;=\underbrace{{\rm E}_{k}^{-1}{\rm E}_{k}}_{={\rm I}} {\rm E}_{k-1} ... {\rm E}_{2} {\rm E}_{1} {\rm A} \\
&amp;\vdots &amp;  \\
{\rm E}_{1}^{-1}... {\rm E}_{k}^{-1}{\rm I} &amp;=  {\rm A} \\
\Rightarrow {\rm A}  {\rm B} &amp; ={\rm E}_{1}^{-1}... {\rm E}_{k}^{-1}{\rm I}  {\rm E}_{k} ... {\rm E}_{2} {\rm E}_{1} {\rm I}   \\
&amp; ={\rm E}_{1}^{-1}... \underbrace{{\rm E}_{k}^{-1}{\rm E}_{k}}_{={\rm I}} ... {\rm E}_{2} {\rm E}_{1} \\
&amp;= {\rm I}
\end{aligned}\]</span></p>
<p>Hence, we can obtain the inverse of a matrix <span class="math inline">\({\rm A}\)</span> (if it exists) by applying the same row operations which convert <span class="math inline">\({\rm A}\)</span> into an identity matrix. The method is usually applied to the augmented matrix <span class="math inline">\({\rm A} \vert  {\rm  I}\)</span> where any row operations are executed simultaneously on both sides, to reduce it to <span class="math inline">\({\rm I} \vert  {\rm A}^{-1}\)</span>.</p>
<section id="example-4" class="level5">
<h5 class="anchored" data-anchor-id="example-4">Example</h5>
<p>To calculate <span class="math inline">\({\rm A}^{-1}\)</span> when <span class="math display">\[{\rm A} =\left( \begin{array}{rrr}
1  &amp; 3  &amp; 3  \\
1  &amp; 4  &amp; 3  \\
1  &amp; 3  &amp; 4  
\end{array} \right),\]</span> we reduce the augmented matrix to an identity matrix using elementary row operations. <span class="math display">\[\begin{aligned}
&amp; &amp;\left. \begin{array}{rrr}
1  &amp; 3  &amp; 3  \\
1  &amp; 4  &amp; 3  \\
1  &amp; 3 &amp; 4  
\end{array}  \right| \begin{array}{rrrr}
1  &amp; 0  &amp; 0  &amp; \qquad  \\
0  &amp; 1  &amp; 0  &amp; \qquad R_{2} - R_{1}\rightarrow R_{2}\\
0  &amp; 0  &amp; 1  &amp; \qquad R_{3} - R_{1}\rightarrow R_{3}
\end{array} \\
&amp;\Rightarrow&amp; \quad  \left. \begin{array}{rrr}
1  &amp; 3  &amp; 3  \\
0  &amp; 1  &amp; 0  \\
0 &amp; 0 &amp; 1
\end{array}  \right| \begin{array}{rrrr}
1  &amp; 0  &amp; 0  &amp; \qquad  R_{1} - 3 R_{2}\rightarrow R_{1}, R_{1}- 3R_{3}\rightarrow R_{1}\\
-1  &amp; 1  &amp; 0  &amp; \qquad \\
-1  &amp; 0  &amp; 1  &amp; \qquad
\end{array}\\
&amp;\Rightarrow&amp; \quad  \left. \begin{array}{rrr}
1  &amp; 0  &amp; 0  \\
0  &amp; 1  &amp; 0  \\
0 &amp; 0 &amp; 1
\end{array}  \right| \begin{array}{rrrr}
7  &amp; -3  &amp; -3  &amp; \qquad  \\
-1  &amp; 1  &amp; 0  &amp; \qquad \\
-1  &amp; 0  &amp; 1  &amp; \qquad
\end{array}
\end{aligned}\]</span></p>
</section>
<section id="example-5" class="level5">
<h5 class="anchored" data-anchor-id="example-5">Example</h5>
<p>The solution of the equations <span class="math display">\[\left( \begin{array}{rrr}
2  &amp; 0  &amp; 1  \\
1  &amp; {-2}  &amp; 1  \\
3  &amp; 1  &amp; 1
\end{array} \right)\left(\begin{array}{r}
x  \\
y  \\
z  
\end{array}  \right)=\left( \begin{array}{r}
2  \\
0  \\
2  
\end{array}  \right)\]</span> is given by <span class="math display">\[\left( \begin{array}{r}
x  \\
y  \\
z  
\end{array} \right)=\left( \begin{array}{rrr}
{-3}  &amp; 1  &amp; 2  \\
2  &amp; {-1}  &amp; {-1}  \\
7  &amp; {-2}  &amp; {-4}  
\end{array}  \right) \left( \begin{array}{r}
2  \\
0  \\
2  
\end{array} \right)=\left( \begin{array}{r}
-2 \\
2  \\
6  
\end{array}  \right)\]</span></p>
</section>
</section>
<section id="factorising-matrices-the-lu-factorisation" class="level2">
<h2 class="anchored" data-anchor-id="factorising-matrices-the-lu-factorisation">Factorising Matrices: The LU factorisation</h2>
<div class="definition">
<p><em>Definition 3.2</em>. A square matrix is <em>upper triangular</em> if all its entries below the main diagonal are zero.</p>
<p>A square matrix is <em>lower triangular</em> if all its entries above the main diagonal are zero.</p>
</div>
<p>So far, we know two methods to solve a system of linear equations <span class="math display">\[A\vec{r} =  \vec{q},  \quad \vec{r}, \vec{q}\in \mathbb{R}^{n}, {\rm A}\in M_{n\times n},\]</span> Gaussian elimination and the inversion of the matrix <span class="math inline">\(A\)</span>. The former uses the augmented matrix <span class="math inline">\(A|\vec{q}\)</span> and elementary row operations to convert the system into a form where the left-hand side is an upper triangular matrix. In many applications, e.g., in algorithms to solve partial differential equations, <span class="math inline">\({\rm A}\)</span> is a large matrix (<span class="math inline">\(1000 \ times 1000\)</span> is not unusual), and the system has to be solved repeatedly for various right-hand sides <span class="math inline">\(\ vec {q}\)</span>. Here, the Gaussian elimination is very inefficient, as for every new <span class="math inline">\(\vec{q}\)</span>, the system has to be solved again. Inverting the matrix <span class="math inline">\({\rm A}\)</span> seems to be much more efficient since we invert the matrix only once and then only apply <span class="math inline">\({\rm A}^{-1}\)</span> to every new <span class="math inline">\(\vec{q}\)</span>. <span class="math display">\[{\rm A} \vec{r} = \vec{q} \quad \Rightarrow \vec{r} =  {\rm A}^{-1}  \vec{q}.\]</span> However, the matrix inversion of such large matrices itself is often numerically difficult, “numerically unstable” that is, it tends to produce very large (or very small) numbers, which in turn produce significant numerical errors.</p>
<p>Here, another method, the so-called LU decomposition, has proven to be very efficient. The name is derived from the representation of the matrix <span class="math inline">\(A\)</span> as a product <span class="math display">\[{\rm A} = {\rm L}{\rm U} ;  \quad  {\rm A},  {\rm L}, {\rm U} \in M_{n\times n}\]</span> of two triangular matrices <span class="math inline">\({\rm L}\)</span> and <span class="math inline">\({\rm U}\)</span>, where <span class="math inline">\({\rm L}\)</span> is a lower triangular matrix and <span class="math inline">\({\rm U}\)</span> is an upper triangular matrix.</p>
<p>The advantage of this method is that determining <span class="math inline">\({\rm L}\)</span> and <span class="math inline">\({\rm U}\)</span> is faster (i.e., it uses fewer steps) than inverting <span class="math inline">\({\rm A}\)</span>, and we can still solve the system comparatively easily. The methods consist of three steps:</p>
<div class="description">
<p>First step: Determine <span class="math inline">\({\rm L}\)</span> and <span class="math inline">\({\rm U}\)</span>.</p>
<p>Second step: Solve <span class="math inline">\({\rm L}\underbrace{({\rm U} \vec{r})}_{ \vec{s} }= \vec{q}\)</span> for <span class="math inline">\(\vec{s}\)</span>.</p>
<p>Third step: Solve <span class="math inline">\({\rm U} \vec{r} = \vec{s}\)</span>.</p>
</div>
<p>Step 1: We know already (see Gauss elimination) that we can use elementary row operations to convert a matrix <span class="math inline">\({\rm A}\)</span> to an upper triangular matrix. We have also seen that each of these row operations can be expressed by a matrix <span class="math inline">\({\rm E}_{j}\)</span>, so that <span class="math display">\[{\rm U} =  {\rm E}_{k} \ldots {\rm E}_{2} {\rm E}_{1} {\rm A} ,\]</span> where <span class="math inline">\(k\)</span> is the number of row operations we need to bring <span class="math inline">\({\rm A}\)</span> in an upper triangular form. For the following, we assume that we do not need to exchange rows in this process. (The most general case with the exchange of rows requires a more general representation of <span class="math inline">\({\rm A}\)</span> as <span class="math inline">\({\rm A}= {\rm PLU}\)</span> where <span class="math inline">\({\rm P}\)</span> includes all permutations of rows.) Then each of the elementary matrices <span class="math inline">\({\rm E}_{j}\)</span> can be chosen as a lower triangular matrix and has an inverse <span class="math inline">\({\rm E}_{j}^{-1}\)</span> which is again a lower triangular matrix, hence in <span class="math display">\[\underbrace{{\rm E}_{1}^{-1}  {\rm E}_{2}^{-1}\ldots {\rm E}_{k}^{-1}}_{:={\rm L}} {\rm U} =   {\rm A}\]</span> the product <span class="math inline">\({\rm E}_{1}^{-1}  {\rm E}_{2}^{- 1}\ldots {\rm E}_{k}^{-1}\)</span> is also a lower triangular matrix.</p>
<div class="example">
<em>Example 3.7</em>. $$
<span class="math display">\[\begin{aligned}
A &amp;= \left(\begin{array}{rrr}
a &amp; b &amp; c\\
d &amp; e &amp; f\\
g &amp; h &amp; k
\end{array}\right)
\begin{array}{c}
\\
R2-\lambda R1\rightarrow R2 \\
R3-\mu R1 \rightarrow R3
\end{array} \\
&amp;\rightarrow&amp;
\left(\begin{array}{rrr}
a &amp; b &amp; c \\
0 &amp; e' &amp; f' \\
0 &amp; h' &amp; k'
\end{array}\right)
\begin{array}{c}
\\
\\
R3-\omega R1\rightarrow R3
\end{array} \\
&amp;\rightarrow&amp;
\left(\begin{array}{rrr}
a &amp; b &amp; c \\
0 &amp; e' &amp; f' \\
0 &amp; 0 &amp; k''
\end{array}\right)={\rm U}.
\end{aligned}\]</span>
<span class="math display">\[ where $\lambda =d/a$, $\mu =g/a$, $\omega =h'/e'$. Note that this can be accomplished by multiplying
${\rm A}$ from the left by the elementary matrices \]</span>{}_1 =( {{
<span class="math display">\[\begin{array}{rrr}
1 \hfill &amp; 0 \hfill &amp; 0 \hfill \\
{-\lambda } \hfill &amp; 1 \hfill &amp; 0 \hfill \\
0 \hfill &amp; 0 \hfill &amp; 1 \hfill \\
\end{array}\]</span>
}} ), _2 =( {{
<span class="math display">\[\begin{array}{rrr}
1 \hfill &amp; 0 \hfill &amp; 0 \hfill \\
0  &amp; 1 \hfill &amp; 0 \hfill \\
{-\mu } \hfill &amp; 0 \hfill &amp; 1 \hfill \\
\end{array}\]</span>
}} ), _3 =(
<span class="math display">\[\begin{array}{rrr}
1 \hfill &amp; 0 \hfill &amp; 0 \hfill \\
0  &amp; 1 \hfill &amp; 0 \hfill \\
0  &amp; - \omega \hfill &amp; 1 \hfill \\
\end{array}\]</span>
<p>),$$ which means:<br>
</p>
<p><span class="math inline">\(R2-\lambda R1\rightarrow R2\)</span>: <span class="math display">\[\begin{aligned}
\left(\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
-\lambda &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{array}\right)
\left(\begin{array}{rrr}
a &amp; b &amp; c\\
d &amp; e &amp; f \\
g &amp; h &amp; k
\end{array}\right) &amp;=
\left(\begin{array}{rrr}
a &amp; b &amp; c \\
d-\lambda a &amp; e-\lambda b &amp; f-\lambda c \\
g &amp; h &amp; k
\end{array}\right) =
\left(\begin{array}{rrr}
a &amp; b &amp; c \\
0 &amp; e' &amp; f' \\
g &amp; h &amp; k
\end{array}\right),
\end{aligned}\]</span> <span class="math inline">\(R3-\mu R1\rightarrow R3\)</span>: <span class="math display">\[\begin{aligned}
\left(\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
-\mu &amp; 0 &amp; 1
\end{array}\right)
\left(\begin{array}{rrr}
a &amp; b &amp; c\\
0 &amp; e' &amp; f' \\
g &amp; h &amp; k
\end{array}\right) &amp;=
\left(\begin{array}{rrr}
a &amp; b &amp; c \\
0 &amp; e' &amp; f' \\
g-\mu a &amp; h-\mu b &amp; k-\mu c
\end{array}\right) =
\left(\begin{array}{rrr}
a &amp; b &amp; c \\
0 &amp; e' &amp; f' \\
0 &amp; h' &amp; k'
\end{array}\right),
\end{aligned}\]</span> <span class="math inline">\(R3-\omega R1\rightarrow R3\)</span>: <span class="math display">\[\begin{aligned}
\left(\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; -\omega &amp; 1
\end{array}\right)
\left(\begin{array}{rrr}
a &amp; b &amp; c\\
0 &amp; e' &amp; f' \\
0 &amp; h' &amp; k'
\end{array}\right) &amp;=
\left(\begin{array}{rrr}
a &amp; b &amp; c \\
0 &amp; e' &amp; f' \\
0 &amp; h'-\omega e' &amp; k'-\omega f'
\end{array}\right) =
\left(\begin{array}{rrr}
a &amp; b &amp; c \\
0 &amp; e' &amp; f' \\
0 &amp; 0 &amp; k''
\end{array}\right).
\end{aligned}\]</span></p>
<p>All three elementary matrices are of the lower triangular type and have an inverse <span class="math display">\[{\rm E}_1^{-1} =\left(
{{\begin{array}{rrr}
1 \hfill &amp; 0 \hfill &amp; 0 \hfill \\
{\lambda } \hfill &amp; 1 \hfill &amp; 0 \hfill \\
0 \hfill &amp; 0 \hfill &amp; 1 \hfill \\
\end{array} }} \right), \quad  {\rm E}_2^{-1} =\left(
{{\begin{array}{rrr}
1 \hfill &amp; 0 \hfill &amp; 0 \hfill \\
0  &amp; 1 \hfill &amp; 0 \hfill \\
{\mu } \hfill &amp; 0 \hfill &amp; 1 \hfill \\
\end{array} }} \right), \quad   {\rm E}_3^{-1} =\left(
\begin{array}{rrr}
1 \hfill &amp; 0 \hfill &amp; 0 \hfill \\
0  &amp; 1 \hfill &amp; 0 \hfill \\
0  &amp;  \omega \hfill &amp; 1 \hfill \\
\end{array} \right)  .\]</span></p>
<p>When we perform the matrix multiplication to obtain <span class="math inline">\({\rm L}\)</span>: <span class="math display">\[{\rm L} ={\rm E}_{1}^{-1}  {\rm E}_{2}^{- 1} {\rm E}_{3}^{-1}  =\left(
\begin{array}{rrr}
1 \hfill &amp; 0 \hfill &amp; 0 \hfill \\
\lambda &amp; 1 \hfill &amp; 0 \hfill \\
\mu &amp;  \omega \hfill &amp; 1 \hfill \\
\end{array} \right),\]</span> We notice that this matrix multiplication can be performed by just putting the nonzero off-diagonal entries of the inverses of the elementary matrices into the appropriate positions in the matrix <span class="math inline">\({\ rm L}\)</span>. This means that the entries of <span class="math inline">\({\rm L}\)</span>, which are the multiplying factors in the Gaussian elimination process, can be easily stored during the process of Gaussian elimination.</p>
<p>Thus, the matrix <span class="math inline">\(A\)</span> was factorised into the product of the lower triangular matrix <span class="math inline">\({\rm L}\)</span> and the upper triangular matrix <span class="math inline">\({\rm U}\)</span> as follows <span class="math display">\[\left(\begin{array}{rrr}
a &amp; b &amp; c\\
d &amp; e &amp; f\\
g &amp; h &amp; k
\end{array}\right) =
\left(\begin{array}{rrr}
1 &amp; 0  &amp; 0 \ \\
\lambda &amp; 1 &amp; 0  \\
\mu &amp;  \omega &amp; 1  \\
\end{array} \right)
\left(\begin{array}{rrr}
a &amp; b &amp; c \\
0 &amp; e' &amp; f' \\
0 &amp; 0 &amp; k''
\end{array}\right)\]</span></p>
</div>
<p>We illustrate the method by a simple example.</p>
<div class="example">
<p><em>Example 3.8</em>. Let <span class="math display">\[{\rm A} =\left(\begin{array}{rrr}
1  &amp; 2  &amp; 1  \\
2  &amp; {-1}  &amp; 3  \\
{-1}  &amp; 2  &amp; 2  
\end{array} \right).\]</span> We reduce <span class="math inline">\(A\)</span> to upper triangular form in the usual way, but record the multiplying factors in a lower triangular matrix <span class="math inline">\(L\)</span>: <span class="math display">\[\begin{aligned}
&amp; &amp;\left(\begin{array}{rrr}
1 &amp; 2 &amp; 1 \\
2 &amp; -1 &amp; 3 \\
-1 &amp; 2 &amp; 2
\end{array}\right)
\begin{array}{r}
\\
R2-2R1\rightarrow R2\\
R3+R1\rightarrow R3
\end{array} \\
&amp;\rightarrow&amp;
\left(\begin{array}{rrr}
1 &amp; 2 &amp; 1 \\
0 &amp; -5 &amp; 1 \\
0 &amp; 4 &amp; 3
\end{array}\right)
\begin{array}{c}
\\
\\
R3+\frac{4}{5}R2\rightarrow R3
\end{array} \\
&amp;\rightarrow&amp;
\left(\begin{array}{rrr}
1 &amp; 2 &amp; 1\\
0 &amp; -5 &amp; 1\\
0 &amp; 0 &amp; 19/5
\end{array}\right)
\end{aligned}\]</span></p>
<p>so <span class="math inline">\(L\)</span> becomes <span class="math display">\[{\rm L} = \left(\begin{array}{rrr}
1  &amp; 0  &amp; 0  \\
2  &amp; 1  &amp; 0  \\
{-1}  &amp; {-4/5}  &amp; 1  
\end{array} \right).\]</span> Now check that <span class="math display">\[\left(\begin{array}{rrr}
1  &amp; 0  &amp; 0  \\
2  &amp; 1  &amp; 0  \\
{-1}  &amp; {-4/5}  &amp; 1  \\
\end{array} \right)\left(\begin{array}{rrr}
1  &amp; 2  &amp; 1  \\
0  &amp; {-5}  &amp; 1  \\
0  &amp; 0  &amp; {19/5}  
\end{array} \right)=\left(\begin{array}{rrr}
1  &amp; 2  &amp; 1  \\
2  &amp; {-1}  &amp; 3  \\
{-1}  &amp; 2  &amp; 2  
\end{array} \right).\]</span></p>
</div>
<p>Once we have this factorisation, we can make use of it to solve <span class="math inline">\({\rm A}\vec{r}=\vec{q}\)</span> as follows: <span class="math display">\[{\rm A}\vec{r}={\rm L}{\rm U}\vec{r}=\vec{q}\]</span> is equivalent to solving <span class="math display">\[{\rm L}\vec{s}=\vec{q}\quad\textrm{for}\quad\vec{s}={\rm U}\vec{r},\]</span> using forward substitution, followed by <span class="math display">\[{\rm U}\vec{r}=\vec{s}\]</span> using backwards substitution.</p>
<div class="example">
<em>Example 3.9</em>. To solve <span class="math inline">\({\rm A}\vec{r}= \scriptscriptstyle \begin{pmatrix}    2\\ 9 \\ 0  \end{pmatrix} \textstyle\)</span>, first we solve <span class="math inline">\({\rm L}\vec{s}=\vec{q}\)</span> for <span class="math inline">\(\vec{s}=(s_{x},s_{y},s_{z})^{T}\)</span>: $$
<span class="math display">\[\begin{aligned}
&amp;&amp;
\left( \begin{array}{rrr}
1  &amp; 0  &amp; 0  \\
2  &amp; 1  &amp; 0  \\
-1 &amp; -4/5 &amp; 1
\end{array} \right) \scriptstyle \begin{pmatrix}   s_{x}\\ s_{y} \\ s_{z}  \end{pmatrix} \textstyle = \scriptstyle \begin{pmatrix}    2\\ 9 \\ 0  \end{pmatrix} \textstyle \\
&amp;\Rightarrow &amp;
\begin{array}{l}
s_{x} =2, \\
s_{y} = 9-2s_{x} = 5,\\
s_{z} = s_{x}+\frac{4}{5}s_{y}=6,
\end{array} \\
&amp;\Rightarrow&amp;
\vec{s}= \scriptstyle \begin{pmatrix}    2\\ 5 \\ 6  \end{pmatrix} \textstyle.
\end{aligned}\]</span>
<span class="math display">\[ Then we solve ${\rm U}\vec{r}=\vec{s}$ for
$\vec{r}=(x,y,z)^{T}$: \]</span>
<span class="math display">\[\begin{aligned}
&amp; &amp;\left( \begin{array}{rrr}
1  &amp; 2  &amp; 1  \\
0  &amp; -5 &amp; 1  \\
0  &amp; 0  &amp; 19/5
\end{array} \right) \scriptstyle \begin{pmatrix}   x\\ y \\ z  \end{pmatrix} \textstyle = \scriptstyle \begin{pmatrix}    2\\ 5 \\ 6  \end{pmatrix} \textstyle  \\
&amp;\Rightarrow&amp;
\begin{array}{l}
z = 30/19,\\
y=-\frac{1}{5}(5-z)=-13/19,\\
x=2-2y-z=34/19,
\end{array} \\
&amp;\Rightarrow&amp;
\vec{r}=\scriptstyle \begin{pmatrix}    34/19\\ -13/19 \\ 30/19  \end{pmatrix} \textstyle.
\end{aligned}\]</span>
<p>$$</p>
</div>
</section>
</section>
<section id="determinants" class="level1">
<h1>Determinants</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>See Poole: Linear Algebra, <em>A Modern Introduction</em>, pages 256–285.</p>
<p>Determinants occur naturally as (<span class="math inline">\(\pm\)</span>) the volume of the parallelepiped spanned by a set of <span class="math inline">\(n\)</span> vectors. In two dimensions (x-y-plane) the area (2-dimensional volume) of a parallelepiped spanned by the two vectors <span class="math inline">\({\bf b}=(b_{1},b_{2})^{T}\)</span> and <span class="math inline">\({\bf c}=(c_{1},c_{2})^{t}\)</span> is up to a sign given by <span class="math display">\[area = \pm(b_{1}c_{2}- b_{2} c_{1} )\]</span> We call this the determinant of the matrix <span class="math inline">\({\rm A} = [{\bf b},{\bf c}]\)</span> and write <span class="math display">\[{\rm det(A)}= |{\rm A}|=\left|
\begin{array}{cc}
b_{1} &amp; c_{1} \\
b_{2} &amp; c_{2}
\end{array}
\right| = b_{1}c_{2}- b_{2} c_{1} .\]</span> The determinant of a square matrix <span class="math inline">\(A\)</span>, denoted by <span class="math inline">\(\det\)</span>(<span class="math inline">\(A\)</span>), or <span class="math inline">\(\left|
A\right|\)</span>, is a number (scalar). The now-standard notation was first introduced by Cayley in 1841.</p>
<p>In three dimensions, the volume of the cell spanned by the vectors <span class="math inline">\({\bf a}\)</span>, <span class="math inline">\({\bf b}\)</span>, and <span class="math inline">\({\bf c}\)</span> is given by <span class="math display">\[{\bf a}\cdot ( {\bf b} \times {\bf c} )= a_{1}(b_{2}c_{3}- b_{3}c_{2}) + a_{2} (b_{3}c_{1}-b_{1}c_{3}) + a_{3} (b_{1}c_{2}-b_{2}c_{1}).\]</span> Note that we can write this in terms of the determinants of 2x2 matrices: <span class="math display">\[{\bf a}\cdot ({\bf b} \times {\bf c}) = \left|
\begin{array}{ccc}
a_{1} &amp; b_{1} &amp; c_{1} \\
a_{2} &amp; b_{2} &amp; c_{2} \\
a_{3} &amp; b_{3} &amp; c_{3}
\end{array}
\right|  = a_{1} \left|
\begin{array}{cc}
b_{2} &amp; c_{2} \\
b_{3} &amp; c_{3}
\end{array}
\right|  - a_{2} \left|
\begin{array}{cc}
b_{1} &amp; c_{1} \\
b_{3} &amp; c_{3}
\end{array}
\right| + a_{3} \left|
\begin{array}{cc}
b_{1} &amp; c_{1} \\
b_{2} &amp; c_{2}
\end{array}
\right| ,\]</span> where each of the <span class="math inline">\(2 \times 2\)</span> matrices is obtained by deleting the first column and the corresponding row of the coefficient <span class="math inline">\(a_{j}\)</span> from the matrix <span class="math inline">\({\rm A} = [{\bf a},{\bf b}, {\bf c}]\)</span>. This suggests a recursive scheme where determinants of arbitrary <span class="math inline">\(n \times n\)</span> matrices are defined in terms of determinants of <span class="math inline">\((n-1)\times(n-1)\)</span> matrices, which again are expanded in determinants of <span class="math inline">\((n-2)\times(n-2)\)</span> and so on.</p>
<div class="example">
<p><em>Example 4.1</em>. Evaluate <span class="math inline">\(\left|
\begin{array}{ccc}
1 &amp; 2 &amp; 3 \\
2 &amp; 3 &amp; 2 \\
3 &amp; 3 &amp; 4
\end{array}
\right| .\)</span></p>
</div>
<div class="defn">
<p><strong>Definition 4.1</strong> (Minor and Cofactor). Corresponding to each entry <span class="math inline">\(a_{i,j}\)</span> in an <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(A,\)</span> we define a number <span class="math inline">\(M_{ij},\)</span> called the <em>minor</em> of <span class="math inline">\(a_{ij},\)</span> which is the <span class="math inline">\(\left( n-1\right)\)</span>th order determinant obtained by deleting the <span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column from <span class="math inline">\(A.\)</span></p>
<p>The <em>cofactor</em> <span class="math inline">\(C_{ij}\)</span> corresponding to an entry <span class="math inline">\(a_{ij}\)</span> in an <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(A\)</span> is the product of its minor and the sign <span class="math inline">\(\left( -1\right) ^{i+j}:\)</span> <span class="math inline">\(C_{ij}=\left( -1\right) ^{i+j}M_{ij}.\)</span> The <span class="math inline">\(n^{2}\)</span> cofactors form a matrix of cofactors.</p>
</div>
<div id="detdefn" class="defn">
<p><strong>Definition 4.2</strong> (Determinants). A determinant of an <span class="math inline">\(n \times n\)</span> matrix can be expanded in terms of <span class="math inline">\((n-1)\times(n-1)\)</span> determinants using either a column or a row. Expansion along the i-th column: <span class="math display">\[\left| A\right| =a_{1,i}C_{1,i}+a_{2,i}C_{2,i}+a_{3,i}C_{3,i}+\cdots
+a_{n,i}C_{n,i}.\]</span> Expansion along the i-th row: <span class="math display">\[\left| A\right| =a_{i,1}C_{i,1}+a_{i,2}C_{i,2}+a_{i,3}C_{i,3}+\cdots
+a_{i,n}C_{i,n}.\]</span> The <span class="math inline">\((n-1)\times(n-1)\)</span> determinants can then be recursively further reduced down to <span class="math inline">\(2\times2\)</span> matrices.</p>
</div>
<div class="example">
<p><em>Example 4.2</em>. Find the expansion</p>
<p>For the matrix <span class="math inline">\(A\)</span> of the previous Example. Expansion along the first row: <span class="math display">\[\left|
\begin{array}{ccc}
1 &amp; 2 &amp; 3 \\
2 &amp; 3 &amp; 2 \\
3 &amp; 3 &amp; 4
\end{array}
\right| =  1  \left| \begin{array}{cc}
3 &amp; 2 \\
3 &amp; 4
\end{array} \right| - 2  \left| \begin{array}{cc}
2 &amp; 2 \\
3 &amp; 4
\end{array}  \right| +3 \left| \begin{array}{cc}
2 &amp; 3 \\
3 &amp; 3
\end{array}  \right| =  6-4 -9 =-7\]</span> Expansion along the first column: <span class="math display">\[\left|
\begin{array}{ccc}
1 &amp; 2 &amp; 3 \\
2 &amp; 3 &amp; 2 \\
3 &amp; 3 &amp; 4
\end{array}
\right| =  1  \left| \begin{array}{cc}
3 &amp; 2 \\
3 &amp; 4
\end{array} \right| - 2  \left| \begin{array}{cc}
2 &amp; 3 \\
3 &amp; 4
\end{array}  \right| + 3\left| \begin{array}{cc}
2 &amp; 3 \\
3 &amp; 2
\end{array}  \right| =6+2-15=-7\]</span> Expansion along the second column: <span class="math display">\[\left|
\begin{array}{ccc}
1 &amp; 2 &amp; 3 \\
2 &amp; 3 &amp; 2 \\
3 &amp; 3 &amp; 4
\end{array}
\right| =  - 2  \left| \begin{array}{cc}
2 &amp; 2 \\
3 &amp; 4
\end{array}  \right| +3 \left| \begin{array}{cc}
1 &amp; 3 \\
3 &amp; 4
\end{array}  \right| -3 \left| \begin{array}{cc}
1 &amp; 3 \\
2 &amp; 2
\end{array}  \right| =  -4 -15 +12=-7\]</span></p>
</div>
<div class="example">
<p><em>Example 4.3</em>. Evaluate <span class="math display">\[\begin{aligned}
\left|
\begin{array}{cccc}
1 &amp; 2 &amp; 1 &amp; 3 \\
1 &amp; 1 &amp; 2 &amp; 1 \\
3 &amp; 2 &amp; 0 &amp; 2 \\
2 &amp; 2 &amp; 1 &amp; 0
\end{array}
\right| &amp; = 1\left|
\begin{array}{ccc}
1 &amp; 2 &amp; 1 \\
2 &amp; 0 &amp; 2 \\
2 &amp; 1 &amp; 0
\end{array}
\right| - 1 \left|
\begin{array}{ccc}
2 &amp; 1 &amp; 3 \\
2 &amp; 0 &amp; 2 \\
2 &amp; 1 &amp; 0
\end{array}
\right|  + 3 \left|
\begin{array}{ccc}
2 &amp; 1 &amp; 3 \\
1 &amp; 2 &amp; 1 \\
2 &amp; 1 &amp; 0
\end{array}
\right| - 2 \left|
\begin{array}{ccc}
2 &amp; 1 &amp; 3 \\
1 &amp; 2 &amp; 1 \\
2 &amp; 0 &amp; 2 \end{array}
\right| \\
&amp; = 1\left|
\begin{array}{cc}
0 &amp; 2 \\
1 &amp; 0
\end{array}
\right| -2  \left|\begin{array}{cc}
2&amp; 2 \\
2 &amp; 0
\end{array}
\right|  - ... \mbox{10 more terms}
\end{aligned}\]</span></p>
</div>
<p>The previous example should illustrate the large amount of computation needed by using a “brute force” approach to evaluating determinants. We will now look at smarter ways, using two fundamental theorems to develop a simple numerical procedure closely related to Gaussian elimination. The following example shows that it is much more efficient to calculate determinants with many zeros in columns or rows:</p>
<div class="example">
<p><em>Example 4.4</em>. Evaluate <span class="math inline">\(\left|
\begin{array}{cccc}
1 &amp; 2 &amp; 1 &amp; 3 \\
0 &amp; 1 &amp; 1 &amp; -2 \\
0 &amp; 0 &amp; 7 &amp; -1 \\
0 &amp; 0 &amp; 3 &amp; 2
\end{array}
\right| =\left|
\begin{array}{ccc}
1 &amp; 1 &amp; -2 \\
0 &amp; 7 &amp; -1 \\
0 &amp; 3 &amp; 2
\end{array}
\right| =\left|
\begin{array}{cc}
7 &amp; -1 \\
3 &amp; 2
\end{array}
\right| =\allowbreak 17.\)</span></p>
</div>
</section>
<section id="simplification-of-determinants" class="level2">
<h2 class="anchored" data-anchor-id="simplification-of-determinants">Simplification of Determinants</h2>
<p>The following rules help to simplify determinants efficiently:</p>
<div id="calcdet" class="thm">
<p><strong>Theorem 4.1</strong> (Calculation of Determinants). **</p>
<ol type="1">
<li><p><em>If two rows (columns) of <span class="math inline">\(A\)</span> are interchanged to give a matrix <span class="math inline">\(B\)</span> then <span class="math inline">\(\left| B\right| =-\left| A\right|\)</span>.</em></p></li>
<li><p><em>If two rows (columns) of <span class="math inline">\(A\)</span> are equal then <span class="math inline">\(\left| A\right| =0\)</span></em></p></li>
<li><p><em>If all the entries in any row (column) of <span class="math inline">\(A\)</span> are multiplied by a scalar <span class="math inline">\(k,\)</span> the determinant is also multiplied by <span class="math inline">\(k\)</span></em></p></li>
<li><p><em>If one row (column) of <span class="math inline">\(A\)</span> is a multiple of another row (column), then <span class="math inline">\(\left|
A\right| =0\)</span></em></p></li>
<li><p><em>If the matrix <span class="math inline">\(B\)</span> is obtained from <span class="math inline">\(A\)</span> by taking multiple of one row (column) and adding it to another row (column), then <span class="math inline">\(\left| B\right|
=\left| A\right| .\)</span></em></p></li>
<li><p><em><span class="math inline">\(\left| A\right| =\left| A^{T}\right| .\)</span></em></p></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof of (1) and (6) is omitted.<br>
(2) is proved by using (1): If we interchange the two equal columns of <span class="math inline">\({\rm A}\)</span>, then we obtain <span class="math inline">\({\rm A}\)</span> again, hence <span class="math inline">\(|{\rm A}|\)</span> remains the same, but according to (1) it should change its sign. This is only possible if <span class="math inline">\(|{\ rmA}|=0.\)</span></p>
<p>(3) If <span class="math inline">\(B\)</span> is obtained by multiplying the <span class="math inline">\(i\)</span>th column of <span class="math inline">\(A\)</span> by <span class="math inline">\(k:\)</span> <span class="math display">\[\left| A\right| =\left|
\begin{array}{cccc}
a_{1} &amp; b_{1} &amp; c_{1} &amp; \cdots \\
a_{2} &amp; b_{2} &amp; c_{2} &amp;  \\
\vdots &amp;  &amp;  &amp;  \\
a_{i} &amp; b_{i} &amp; c_{i} &amp;  \\
\vdots &amp;  &amp;  &amp;
\end{array}
\right| ,\quad \left| B\right| =\left|
\begin{array}{cccc}
a_{1} &amp; b_{1} &amp; c_{1} &amp; \cdots \\
a_{2} &amp; b_{2} &amp; c_{2} &amp;  \\
\vdots &amp;  &amp;  &amp;  \\
ka_{i} &amp; kb_{i} &amp; kc_{i} &amp;  \\
\vdots &amp;  &amp;  &amp;
\end{array}
\right|\]</span> and expand both determinants about their <span class="math inline">\(i\)</span>th rows: <span class="math display">\[\begin{aligned}
\left| A\right| &amp;=a_{i}C_{i,1}+b_{i}C_{i,2}+\cdots \\
\left| B\right| &amp;=\left( ka_{i}\right) C_{i,1}+\left( kb_{i}\right)
C_{i,2}+\cdots \\
&amp;=k\left( a_{i}C_{i,1}+b_{i}C_{i,2}+\cdots \right) =k\left| A\right| .
\end{aligned}\]</span> (4) We use (3) and (2).</p>
<p>(5) Suppose we add <span class="math inline">\(k\times\)</span> row <span class="math inline">\(j\)</span> to row <span class="math inline">\(i\)</span> of <span class="math inline">\(\left|
A\right|\)</span> to give <span class="math inline">\(\left| B\right| :\)</span> <span class="math display">\[\left| A\right| =\left|
\begin{array}{cccc}
a_{1} &amp; b_{1} &amp; c_{1} &amp; \cdots \\
a_{2} &amp; b_{2} &amp; c_{2} &amp;  \\
\vdots &amp;  &amp;  &amp;  \\
a_{i} &amp; b_{i} &amp; c_{i} &amp;  \\
\vdots &amp;  &amp;  &amp;
\end{array}
\right| ,\quad \left| B\right| =\left|
\begin{array}{cccc}
a_{1} &amp; b_{1} &amp; c_{1} &amp; \cdots \\
a_{2} &amp; b_{2} &amp; c_{2} &amp;  \\
\vdots &amp;  &amp;  &amp;  \\
a_{i}+ka_{j} &amp; b_{i}+kb_{j} &amp; c_{i}+kc_{j} &amp;  \\
\vdots &amp;  &amp;  &amp;
\end{array}
\right|\]</span> and we now expand <span class="math inline">\(\left| B\right|\)</span> by its <span class="math inline">\(i\)</span>th row <span class="math display">\[\begin{aligned}
\left| A\right| &amp;=a_{i}C_{i,1}+b_{i}C_{i,2}+\cdots \\
\left| B\right| &amp;=\left( a_{i}+ka_{j}\right) C_{i,1}+\left(
b_{i}+kb_{j}\right) C_{i,2}+\cdots \\
&amp;=\left( a_{i}C_{i,1}+b_{i}C_{i,2}+\cdots \right) +k\left(
a_{j}C_{i,1}+b_{i}C_{j,2}+\cdots \right)
\end{aligned}\]</span> <span class="math display">\[=\left|
\begin{array}{cccc}
a_{1} &amp; b_{1} &amp; c_{1} &amp; \cdots \\
a_{2} &amp; b_{2} &amp; c_{2} &amp;  \\
\vdots &amp;  &amp;  &amp;  \\
a_{i} &amp; b_{i} &amp; c_{i} &amp;  \\
\vdots &amp;  &amp;  &amp;
\end{array}
\right| +k\left|
\begin{array}{cccc}
a_{1} &amp; b_{1} &amp; c_{1} &amp; \cdots \\
a_{2} &amp; b_{2} &amp; c_{2} &amp;  \\
\vdots &amp;  &amp;  &amp;  \\
a_{j} &amp; b_{j} &amp; c_{j} &amp;  \\
\vdots &amp;  &amp;  &amp;
\end{array}
\right| =\left| A\right|\]</span> since rows <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in the determinant <span class="math inline">\(\left|
\begin{array}{cccc}
a_{1} &amp; b_{1} &amp; c_{1} &amp; \cdots \\
a_{2} &amp; b_{2} &amp; c_{2} &amp;  \\
\vdots &amp;  &amp;  &amp;  \\
a_{j} &amp; b_{j} &amp; c_{j} &amp;  \\
\vdots &amp;  &amp;  &amp;
\end{array}
\right|\)</span> are identical, so it is zero (Cor. 4).&nbsp;◻</p>
</div>
<p><strong>Remember</strong>: The rules are similar to, <strong>but not the same as,</strong> Gaussian elimination; adding <span class="math inline">\(k\)</span> times row 2 to row 1 is OK if the result goes in row 1, and row 2 is left unchanged, but if you replace row 2 by <span class="math inline">\(k\)</span> times row 2 plus row 1, you will change the value of the determinant by a factor of <span class="math inline">\(k\)</span> (Corollary 2 tells us this). A further difference is that a multiple of a column can be added to another column (not allowed in GE).</p>
<div class="example">
<p><em>Example 4.5</em>. Evaluate the determinant <span class="math inline">\(\left|
\begin{array}{cccc}
1 &amp; 2 &amp; -1 &amp; 3 \\
2 &amp; 0 &amp; 1 &amp; 1 \\
1 &amp; 3 &amp; 2 &amp; 1 \\
2 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right| .\)</span></p>
</div>
<p>First expand by the 4th row: <span class="math display">\[= -2\times \left|
\begin{array}{ccc}
2 &amp; -1 &amp; 3 \\
0 &amp; 1 &amp; 1 \\
3 &amp; 2 &amp; 1
\end{array}
\right| +1\times \left|
\begin{array}{ccc}
1 &amp; 2 &amp; -1 \\
2 &amp; 0 &amp; 1 \\
1 &amp; 3 &amp; 2
\end{array}
\right| ,\]</span></p>
<p>now subtract <span class="math inline">\(c_{3}\)</span> from <span class="math inline">\(c_{2}\)</span> in the 1st , subtract <span class="math inline">\(2\times
c_{3}\)</span> from <span class="math inline">\(c_{1}\)</span> in the 2nd and expand both by their 2nd rows. <span class="math display">\[\begin{aligned}
&amp;=-2\times \left|
\begin{array}{ccc}
2 &amp; -4 &amp; 3 \\
0 &amp; 0 &amp; 1 \\
3 &amp; 1 &amp; 1
\end{array}
\right| +1\times \left|
\begin{array}{ccc}
3 &amp; 2 &amp; -1 \\
0 &amp; 0 &amp; 1 \\
-3 &amp; 3 &amp; 2
\end{array}
\right| \\
&amp;=-2\times \left( -1\right) \left|
\begin{array}{cc}
2 &amp; -4 \\
3 &amp; 1
\end{array}
\right| +1\times \left( -1\right) \left|
\begin{array}{cc}
3 &amp; 2 \\
-3 &amp; 3
\end{array}
\right| \\
&amp;=-2\times \left( -1\right) \times 14+1\times \left( -1\right) \times 15=13.
\end{aligned}\]</span></p>
<div class="example">
<p><em>Example 4.6</em>. Evaluate the determinant <span class="math inline">\(\left|
\begin{array}{rrrr}
4 &amp; 1 &amp; 3 &amp; -1 \\
2 &amp; 0 &amp; 1 &amp; 2 \\
1 &amp; -1 &amp; 2 &amp; 5 \\
2 &amp; 1 &amp; 3 &amp; 1
\end{array}
\right|\)</span>.</p>
</div>
<p>Subtract <span class="math inline">\(r_{4}\)</span> from <span class="math inline">\(r_{1}\)</span>; then add <span class="math inline">\(c_{1}\)</span> to <span class="math inline">\(c_{4},\)</span> then expand about the 1st row <span class="math inline">\(:\)</span> <span class="math display">\[\begin{tabular}[t]{ll}
$r_{1}-r_{4}\rightarrow r_{1}:$ &amp; $\left|
\begin{array}{rrrr}
2 &amp; 0 &amp; 0 &amp; -2 \\
2 &amp; 0 &amp; 1 &amp; 2 \\
1 &amp; -1 &amp; 2 &amp; 5 \\
2 &amp; 1 &amp; 3 &amp; 1
\end{array}
\right| ,$ \\
$c_{4}+c_{1}\rightarrow c_{4}$ &amp; $\left|
\begin{array}{rrrr}
2 &amp; 0 &amp; 0 &amp; 0 \\
2 &amp; 0 &amp; 1 &amp; 4 \\
1 &amp; -1 &amp; 2 &amp; 6 \\
2 &amp; 1 &amp; 3 &amp; 3
\end{array}
\right| =2\times \left|
\begin{array}{rrr}
0 &amp; 1 &amp; 4 \\
-1 &amp; 2 &amp; 6 \\
1 &amp; 3 &amp; 3
\end{array}
\right| $
\end{tabular}\]</span></p>
<p>add <span class="math inline">\(r_{3}\)</span> to <span class="math inline">\(r_{2}:\)</span> <span class="math display">\[\begin{tabular}[t]{lll}
$r_{2}+r_{3}\rightarrow r_{2}$ &amp; $2\times \left|
\begin{array}{rrr}
0 &amp; 1 &amp; 4 \\
0 &amp; 5 &amp; 9 \\
1 &amp; 3 &amp; 3
\end{array}
\right| $ &amp;  \\
&amp; $=2\times 1\times \left|
\begin{array}{rr}
1 &amp; 4 \\
5 &amp; 9
\end{array}
\right| $ &amp; $=2\times 1\times \left( -11\right) =-22.$
\end{tabular}\]</span></p>
<div class="example">
<p><em>Example 4.7</em>. Show that <span class="math inline">\(D=\left|
\begin{array}{cccc}
1 &amp; a &amp; a^{2} &amp; b+c+d \\
1 &amp; b &amp; b^{2} &amp; c+d+a \\
1 &amp; c &amp; c^{2} &amp; d+a+b \\
1 &amp; d &amp; d^{2} &amp; a+b+c
\end{array}
\right| =0\)</span> for all values of <span class="math inline">\(a,b,c,d.\)</span></p>
</div>
<div class="example">
<p><em>Example 4.8</em>. Solve <span class="math inline">\(\left|
\begin{array}{ccc}
1 &amp; 2 &amp; x \\
x &amp; 0 &amp; 3 \\
1 &amp; x &amp; 2
\end{array}
\right| =0\)</span> for <span class="math inline">\(x.\)</span></p>
</div>
<div class="cor">
<p><strong>Corollary 4.2</strong>. <em>If <span class="math inline">\(A\)</span> is either lower or upper triangular, then <span class="math inline">\(\left|
A\right| =a_{1,1}a_{2,2}\cdots a_{n,n}\)</span>, the product of its diagonal entries.</em></p>
</div>
<div class="example">
<p><em>Example 4.9</em>. <span class="math inline">\(\left|\begin{array}{rrr}
1 &amp; 7 &amp; -2 \\
0 &amp; 14 &amp; 32 \\
0 &amp; 0 &amp; -2
\end{array}\right| = 1\times 14\times (-2)=-28\)</span>.</p>
<p>We can see this by expanding down the first column:</p>
<p><span class="math display">\[1\left|\begin{array}{rr} 14 &amp; 32 \\
0 &amp; -2
\end{array}\right|=1(14\times (-2))=-28.\]</span></p>
</div>
<div class="thm">
<p><strong>Theorem 4.3</strong> (Product Rule). <em>For any two matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B,\)</span> <span class="math inline">\(\det (AB)\)</span> = <span class="math inline">\(\det (A) \det (B)\)</span>.</em></p>
</div>
<p>The general proof of this result uses elementary matrices. Although it is a key result, a proof will not be given (See Poole, p.&nbsp;268).</p>
<p>Note that <span class="math inline">\(\textrm{det}(A+B)\neq \textrm{det}(A)+\textrm{det}(B)\)</span>.</p>
<div class="cor">
<p><strong>Corollary 4.4</strong>. <em>If <span class="math inline">\(A\)</span> is <span class="math inline">\(n\times n\)</span>, we have<span class="math inline">\(,\)</span> by Theorem <a href="#calcdet" data-reference-type="ref" data-reference="calcdet">4.1</a>,</em></p>
<p><em><span class="math display">\[\begin{aligned}
\det \left( kA\right) &amp;= \det \left( kI \  A\right) \\
&amp;=
\det \left( kI\right)  \det \left( A\right) \\
&amp;= k^{n}
\det \left( A\right) .
\end{aligned}\]</span></em></p>
</div>
<div class="cor">
<p><strong>Corollary 4.5</strong>. <em>When <span class="math inline">\(A\)</span> is non-singular (i.e., invertible) <span class="math inline">\(\det \left(A^{-1}\right) =\dfrac{1}{\det \left( A\right) }.\)</span></em></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\(1=\textrm{det}{I}=\textrm{det}(AA^{-1})=\textrm{det}(A)\textrm{det}(A^{-1})\)</span>.&nbsp;◻</p>
</div>
<div class="example">
<p><em>Example 4.10</em>. Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n\times n\)</span> matrix with <span class="math inline">\(\textrm{det}(A)=5\)</span>. Then <span class="math inline">\(\det \left( A^{4}\right)
=5^{4},\det \left( A^{-1}\right) =\frac{1}{5},\)</span> and <span class="math inline">\(\det \left( 3A^{2}\right)
=3^{n}\times 25\)</span> .</p>
</div>
<div class="cor">
<p><strong>Corollary 4.6</strong>. <em>The matrix <span class="math inline">\(A\)</span> has an inverse if, and only if, <span class="math inline">\(\left|
A\right| \neq 0.\)</span></em></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>First, we show that if the matrix <span class="math inline">\(A\)</span> has an inverse, then <span class="math inline">\(|A|\neq 0\)</span>. If <span class="math inline">\({\rm A}\)</span> has an inverse then <span class="math inline">\(|{\rm A}{\rm A^{-1}}| = |{\rm A}||{\rm A^{-1}}| =| I | = 1\)</span> and hence <span class="math inline">\(|{\rm A}| \neq 0\)</span>.</p>
<p>Next, we show the reverse. Assume that <span class="math inline">\(|A|\neq 0\)</span>. We want to show that <span class="math inline">\(A\)</span> has an inverse.</p>
<p>By using elementary row operations, that is by either (i) adding a multiple of one row to another or (ii) interchanging rows we can reduce <span class="math inline">\(A\)</span> to upper triangular form—call this <span class="math inline">\(U.\)</span> Operations of type i) do not affect the value of <span class="math inline">\(\left| A\right|\)</span> while operations of type (ii) may cause a change of sign. Hence, <span class="math display">\[\left| A\right| =\pm \left| U\right|,\]</span> and so, in particular <span class="math inline">\(|A|\neq0\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
U&amp;=\left[
\begin{array}{cccc}
u_{11} &amp; u_{12} &amp; \cdots &amp; u_{1n} \\
0 &amp; u_{22} &amp;  &amp; u_{2n} \\
0 &amp; 0 &amp; \ddots &amp;  \\
0 &amp; 0 &amp;  &amp; u_{nn}
\end{array}
\right],\\
\left| U\right| &amp;= u_{11}u_{22}\cdots u_{nn}\neq 0,
\end{aligned}\]</span> so <span class="math inline">\(u_{ij}\neq 0\)</span> for all <span class="math inline">\(i,j=1,2,\dots,n\)</span>. Hence <span class="math inline">\(A\)</span> has <span class="math inline">\(n\)</span> pivots and is invertible.&nbsp;◻</p>
</div>
<div class="cor">
<p><strong>Corollary 4.7</strong>. <em>If <span class="math inline">\(A\)</span> is a square matrix, the system <span class="math inline">\(A\mathbf{x=0}\)</span> has non-trivial solutions if, and only if, <span class="math inline">\(\left| A\right| =0\)</span> (so <span class="math inline">\(A\)</span> is a singular matrix).</em></p>
</div>
<p>Note: Various authors define non-singular matrices to be</p>
<ul>
<li><p>invertible matrices</p></li>
<li><p>matrices <span class="math inline">\(A\)</span> for which <span class="math inline">\(\det(A)\neq 0\)</span>.</p></li>
<li><p>matrices for which <span class="math inline">\(A\mathbf{x}=\mathbf{0}\)</span> has only the trivial (zero) solution.</p></li>
</ul>
<p>The above theorem shows that they are all equivalent definitions.</p>
</section>
<section id="the-adjoint-matrix" class="level2">
<h2 class="anchored" data-anchor-id="the-adjoint-matrix">The Adjoint Matrix</h2>
<div class="defn">
<p><strong>Definition 4.3</strong>. The transpose of the matrix of cofactors of <span class="math inline">\(A\)</span> is called the <em>adjugate</em> or <em>adjoint</em> of <span class="math inline">\({\rm A}\)</span> and denoted by <span class="math inline">\(\rm{adj}({\rm A}) =(\mbox{Cofactors})^{T}\)</span>.</p>
</div>
<div class="thm">
<p><strong>Theorem 4.8</strong>. <em>If <span class="math inline">\(\det({\rm A}) \neq 0\)</span> then <span class="math display">\[{\rm A}^{-1} =\frac{1}{\det({\rm A})} \rm{adj}({\rm A}).\]</span></em></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\({\rm C}\)</span> be the matrix of cofactors of <span class="math inline">\({\rm A}\)</span>. <span class="math display">\[\begin{aligned}
({\rm A} \  \rm{adj}({\rm A}))_{ij} &amp;= \sum_{k=1}^{n} a_{ik} ({\rm C}^{T})_{kj} =  \sum_{k=1}^{n} a_{ik} {\rm C}_{jk} \\
&amp;= \left\{
\begin{array}{cc}
0 &amp; i \neq j \\
\det(\rm{A}) &amp; i=j
\end{array}
\right. \\
\Rightarrow {\rm A} \  \rm{adj}({\rm A}) &amp;= \det(\rm{A}) {\rm I}
\end{aligned}\]</span> Here, the result for <span class="math inline">\(i\neq j\)</span> is obtained from the fact that <span class="math inline">\(\sum_{k=1}^{n} a_{ik} {\rm C}_{jk}\)</span> is the expansion of the determinant of a matrix with two identical rows. For the case <span class="math inline">\(i=j\)</span>, however, this is exactly the definition of the expansion of a determinant along row <span class="math inline">\(i\)</span> of <span class="math inline">\({\rm A}\)</span>. So <span class="math display">\[\begin{aligned}
&amp;&amp; A\textrm{adj}(A)=\det(A)I \\
&amp;\Leftrightarrow&amp; \frac{A \textrm{adj}(A)}{\det(A)}=I.
\end{aligned}\]</span> Multiplying <span class="math inline">\(A^{-1}\)</span> from the left gives <span class="math display">\[\frac{1}{\det(A)} \textrm{adj}(A)=A^{-1}.\]</span>&nbsp;◻</p>
</div>
<div class="example">
<p><em>Example 4.11</em>. Example: find the adjoint of the matrix</p>
<p><span class="math display">\[{\rm A}=\left[
\begin{array}{ccc}
1 &amp; -1 &amp; 2\\
3 &amp; 2 &amp; -1\\
2 &amp; -1 &amp; 2\\
\end{array}
\right]\]</span></p>
<p>(i) Matrix of minors is <span class="math display">\[\left[
\begin{array}{rrr}
3 &amp; 8 &amp; -7\\
0 &amp; -2 &amp; 1\\
-3 &amp; -7 &amp; 5
\end{array}
\right].\]</span> (ii) The matrix of cofactors is <span class="math display">\[\left[
\begin{array}{rrr}
3 &amp; -8 &amp; -7\\
0 &amp; -2 &amp; -1\\
-3 &amp; 7 &amp; 5
\end{array}
\right].\]</span> (iii) Adjoint is <span class="math display">\[{\rm adj}({\rm A}) = \left[
\begin{array}{rrr}
3 &amp; 0 &amp; -3\\
-8 &amp; -2 &amp; 7\\
-7 &amp; -1 &amp; 5
\end{array}
\right].\]</span></p>
<p>Notice that</p>
<p><span class="math display">\[{\rm A} \,  {\rm adj}({\rm A}) = \left[
\begin{array}{rrr}
1 &amp; -1 &amp; 2\\
3 &amp; 2 &amp; -1\\
2 &amp; -1 &amp; 2\\
\end{array}
\right]
\left[
\begin{array}{rrr}
3 &amp; 0 &amp; -3\\
-8 &amp; -2 &amp; 7\\
-7 &amp; -1 &amp; 5
\end{array}
\right]\]</span> $$= = -3 I = det(A) I.$$</p>
</div>
<div class="example">
<p><em>Example 4.12</em>. Use the adjoint matrix to find the inverse of the matrix <span class="math inline">\(A=\left(\begin{array}{rrr}1 &amp; -1 &amp; 2 \\ 3 &amp; 2 &amp; -1 \\ 2 &amp; -1 &amp; 2\end{array}\right)\)</span> in the previous example.</p>
<p>The determinant of the matrix <span class="math inline">\(A\)</span> is <span class="math display">\[\begin{aligned}
&amp;&amp;1\left|\begin{array}{rr}2 &amp; -1 \\ -1 &amp; 2\end{array}\right|+
1\left|\begin{array}{rr}3 &amp; -1 \\ 2 &amp; 2\end{array}\right|
-2\left|\begin{array}{rr}3 &amp; 2 \\ 2 &amp; -1\end{array}\right| \\
&amp;= (4-1)+(6+2)+2(-3-4)\\
&amp;= 3+8 -14\\
&amp;=-3.
\end{aligned}\]</span> Thus, the inverse is <span class="math display">\[A^{-1}=\frac{-1}{3}\left(\begin{array}{rrr}
3 &amp; 0 &amp; -3 \\
-8 &amp; -2 &amp; 7 \\
-7 &amp; -1 &amp; 5
\end{array}\right).\]</span></p>
</div>
<div class="example">
<p><em>Example 4.13</em>. Find the inverse of <span class="math display">\[A=\left(\begin{array}{rrr}
2 &amp; 3 &amp; 4 \\
5 &amp; 6 &amp; 7 \\
8 &amp; 9 &amp; 1
\end{array}\right).\]</span></p>
<p><span class="math display">\[\begin{aligned}
\det(A) &amp;= 2\left|\begin{array}{rr}
6 &amp; 7 \\
9 &amp; 1
\end{array}\right|
-3\left|\begin{array}{rr}
5 &amp; 7 \\
8 &amp; 1
\end{array}\right|
+4\left|\begin{array}{rr}
5 &amp; 6 \\
8 &amp; 9
\end{array}\right| \\
&amp;= 2(6-63)-3(5-56)+4(45-48) \\
&amp;= 2(-57)-3(-51)+4(-3) \\
&amp;= -114+153-12 \\
&amp;= 27.
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
A^{-1} &amp;= \frac{1}{27}
\left(\begin{array}{rrr}
-57 &amp; 33 &amp; -3 \\
51 &amp; -30 &amp; 6 \\
-3 &amp; 6 &amp; -3
\end{array}\right) \\
&amp;= \frac{1}{9}\left(\begin{array}{rrr}
-19 &amp; 11 &amp; -1 \\
17 &amp; -10 &amp; 2 \\
-1 &amp; 2 &amp; -1
\end{array}\right).
\end{aligned}\]</span></p>
</div>
<div class="example">
<p><em>Example 4.14</em>. In the<strong>&nbsp;</strong><span class="math inline">\(2\times 2\)</span> case, if <span class="math display">\[\begin{aligned}
A&amp;=\left[
\begin{array}{cc}
a &amp; b \\
c &amp; d
\end{array}
\right] \ \Rightarrow  \  {\rm adj}({\rm A}) = \left[\begin{array}{rr}
d &amp; -b \\
-c &amp; a
\end{array}
\right]  \\
\Rightarrow A^{-1} &amp;= \frac{1}{ad-bc}\left[
\begin{array}{rr}
d &amp; -b \\
-c &amp; a
\end{array}
\right].  
\end{aligned}\]</span></p>
</div>
</section>
</section>
<section id="eigenvalues-and-eigenvectors" class="level1">
<h1>Eigenvalues and Eigenvectors</h1>
<section id="introduction-to-eigenvalues-and-eigenvectors" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-eigenvalues-and-eigenvectors">Introduction to Eigenvalues and Eigenvectors</h2>
<p>Consider an elastic body, e.g., a rubber ball. If we compress the ball in one direction, it will be elongated in the plane perpendicular to that direction. The figure below shows, in a cross-section, how individual points on the surface move under the compression.</p>
<figure id="eigenv1" class="figure">
<p>
<img src="eigenv1.png" style="width:4cm" alt="image" class="figure-img"> <img src="eigenv2.png" style="width:4cm" alt="image" class="figure-img">
</p>
<figcaption>
Effect of a deformation on the points (vectors) in an elastic sphere
</figcaption>
</figure>
<p>There are some vectors in these plots that don’t change direction, but only their length under the deformation. We can find those by superimposing the two plots:</p>
<figure id="eigenv1" class="figure">
<p>
<img src="eigenv3.png" style="width:4cm" alt="image" class="figure-img"> <img src="eigenv4.png" style="width:4cm" alt="image" class="figure-img">
</p>
<figcaption>
There are directions in which the vectors only change their length
</figcaption>
</figure>
<p>These directions are characteristic of the deformation, as is the factor of elongation/compression along these directions. These directions can be found mathematically as the eigenvectors of a matrix, the deformation matrix, which maps the original vectors to the vectors of the deformed ball. The eigenvalues are the factors of compression/elongation along these directions. The word "eigen" is German for "own" and was likely introduced by David Hilbert (also known for Hilbert spaces).</p>
<p>Eigenvalues and eigenvectors play an essential role in mathematics, physics and engineering. The stability of an equilibrium of a dynamical system is determined by the eigenvalues of a matrix that describes the linearised system at the equilibrium point. The values of a measurement in quantum mechanics are the eigenvalues of an operator. The principal axes of a rigid body are the eigenvectors of the moment of inertia tensor.</p>
<div class="defn">
<p><strong>Definition 5.1</strong>. Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> matrix. A scalar <span class="math inline">\(\lambda\)</span> is called an <strong><em>eigenvalue</em></strong> of <span class="math inline">\(A\)</span> if there is a nonzero vector <span class="math inline">\(\mathbf{x}\)</span> such that <span class="math inline">\(A\mathbf{x} = \lambda \mathbf{x}\)</span>. Such a vector <span class="math inline">\(\mathbf{x}\)</span> is called an <strong><em>eigenvector</em></strong> of <span class="math inline">\(A\)</span> corresponding to <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div class="example">
<p><em>Example 5.1</em>. Suppose <span class="math inline">\(A\left[\begin{array}{c} x \\ y \end{array}\right] = \left[\begin{array}{c} 2x \\ 0 \end{array}\right]\)</span>. Then</p>
<div class="center">
<p><span class="math inline">\(A\left[\begin{array}{c} 1 \\ 0 \end{array}\right] = \left[\begin{array}{c} 2 \\ 0 \end{array}\right] = 2\left[\begin{array}{c} 1 \\ 0 \end{array}\right]\)</span></p>
</div>
<p>so <span class="math inline">\(2\)</span> is an eigenvalue and <span class="math inline">\(\left[\begin{array}{c} 1 \\ 0 \end{array}\right]\)</span> a corresponding eigenvector. Also,</p>
<div class="center">
<p><span class="math inline">\(A\left[\begin{array}{c} 0 \\ 1 \end{array}\right] = \left[\begin{array}{c} 0 \\ 0 \end{array}\right] = 0\left[\begin{array}{c} 0 \\ 1 \end{array}\right]\)</span>,</p>
</div>
<p>so <span class="math inline">\(0\)</span> is an eigenvalue and <span class="math inline">\(\left[\begin{array}{c} 0 \\ 1 \end{array}\right]\)</span> a corresponding eigenvector.</p>
</div>
<p>Notice that <span class="math inline">\(\left[\begin{array}{c} \alpha \\ 0 \end{array}\right]\)</span> and <span class="math inline">\(\left[\begin{array}{c} 0 \\ \alpha \end{array}\right]\)</span> are eigenvectors for any <span class="math inline">\(\alpha \neq 0\)</span>.<br>
In general, if <span class="math inline">\(\mathbf{v}\)</span> is an eigenvector of <span class="math inline">\(A\)</span>, then so is <span class="math inline">\(\alpha \mathbf{v}\)</span> for any nonzero scalar <span class="math inline">\(\alpha\)</span>.</p>
<div class="example">
<p><em>Example 5.2</em>. Show that <span class="math inline">\(5\)</span> is an eigenvalue of <span class="math inline">\(A = \left[\begin{array}{cc} 1 &amp; 2 \\ 4 &amp; 3 \end{array}\right]\)</span> and determine all eigenvectors corresponding to this eigenvalue.</p>
<p><strong><em>Solution</em></strong><br>
We must show that there is a nonzero vector <span class="math inline">\(\mathbf{x}\)</span> such that <span class="math inline">\(A\mathbf{x} = 5\mathbf{x}\)</span>, which is equivalent to the equation <span class="math inline">\((A - 5I)\mathbf{x} = \mathbf{0}\)</span>. We compute the nullspace by:</p>
<div class="center">
<p><span class="math inline">\(\left[A - 5I | \mathbf{0}\right] = \left[\begin{array}{cc|c} \textcircled{\raisebox{-0.9pt}{-4}} &amp; 2 &amp; 0 \\ 4 &amp; -2 &amp; 0 \end{array}\right] \xrightarrow{R_{2} + R_{1}} \left[\begin{array}{cc|c} -4 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right]\)</span></p>
</div>
<p>Thus <span class="math inline">\(\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \end{array}\right] \in {\rm Null}(A-5I)\)</span> satisfies <span class="math inline">\(-4x_{1} + 2x_{2} = 0\)</span>, or <span class="math inline">\(x_{2} = 2x_{1}\)</span>.<br>
Thus, <span class="math inline">\(A\mathbf{x} = 5\mathbf{x}\)</span> has a nontrivial solution of the form <span class="math inline">\(\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \end{array}\right] = \left[\begin{array}{c} x_{1} \\ 2x_{1} \end{array}\right] = x_{1}\left[\begin{array}{c} 1 \\ 2 \end{array}\right]\)</span>, so <span class="math inline">\(5\)</span> is an eigenvalue of <span class="math inline">\(A\)</span> and the corresponding eigenvectors are the nonzero multiples of <span class="math inline">\(\left[\begin{array}{c} 1 \\ 2 \end{array}\right]\)</span>.</p>
</div>
<div class="defn">
<p><strong>Definition 5.2</strong>. Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> matrix and let <span class="math inline">\(\lambda\)</span> be an eigenvalue of <span class="math inline">\(A\)</span>. The collection of all eigenvectors corresponding to <span class="math inline">\(\lambda\)</span>, together with the zero vector, is called the <strong><em>eigenspace</em></strong> of <span class="math inline">\(\lambda\)</span> and is denoted by <span class="math inline">\(E_{\lambda }\)</span>.</p>
</div>
<p>Therefore, in the above Example, <span class="math inline">\(E_{5} = \left\{t\left[\begin{array}{c} 1 \\ 2 \end{array}\right]\right\}\)</span>, where <span class="math inline">\(t \in \mathbb{R}\)</span>.</p>
<div class="thm">
<p><strong>Theorem 5.1</strong>. <em>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> matrix. Then <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(A\)</span> if and only if <span class="math inline">\(|A-\lambda I_{n}| = 0\)</span> (or <span class="math inline">\(det(A - \lambda I_{n}) = 0\)</span>).</em></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose that <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(A\)</span>. Then <span class="math inline">\(A\mathbf{v} = \lambda \mathbf{v}\)</span> for some nonzero <span class="math inline">\(\mathbf{v} \in \mathbb{R}^{n}\)</span>. This is equivalent to <span class="math inline">\(A\mathbf{v} = \lambda I_{n} \mathbf{v}\)</span> or <span class="math inline">\((A - \lambda I_{n})\mathbf{v} = \mathbf{0}\)</span>. But this means that <span class="math inline">\(\mathbf{v}\)</span> is a nonzero solution to the homogeneous system of equations defined by the matrix <span class="math inline">\(A - \lambda I_{n}\)</span>. This means <span class="math inline">\(A - \lambda I_{n}\)</span> is singular, and so <span class="math inline">\(|A - \lambda I_{n}| = 0\)</span>.<br>
Conversely, if <span class="math inline">\(|A - \lambda I_{n}| = 0\)</span> then <span class="math inline">\(A - \lambda I_{n}\)</span> is singular, and so the system of equations defined by <span class="math inline">\(A - \lambda I_{n}\)</span> has nonzero solutions. Hence there exists a nonzero <span class="math inline">\(\mathbf{v} \in \mathbb{R}^{n}\)</span> with <span class="math inline">\((A - \lambda I_{n})\mathbf{v} = \mathbf{0}\)</span>, which is equivalent to <span class="math inline">\(A \mathbf{v} = \lambda \mathbf{v}\)</span>, and so <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(A\)</span>.&nbsp;◻</p>
</div>
<div class="defn">
<p><strong>Definition 5.3</strong>. For an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span>, the equation <span class="math inline">\(|A - \lambda I_{n}| = 0\)</span> is called the <strong><em>characteristic equation</em></strong> of <span class="math inline">\(A\)</span>, and <span class="math inline">\(|A - \lambda I_{n}|\)</span> is called the <strong><em>characteristic polynomial</em></strong> of <span class="math inline">\(A\)</span>.</p>
</div>
<div class="example">
<p><em>Example 5.3</em>. Find the eigenvalues and the corresponding eigenvectors of <span class="math inline">\(A = \left[\begin{array}{cc}
        1 &amp; 2 \\
        4 &amp; -1
    \end{array}\right]\)</span>.</p>
<p><strong><em>Solution</em></strong><br>
The characteristic polynomial is</p>
<div class="center">
<p><span class="math inline">\(\begin{alignedat}{2}
        |A - \lambda I_{2}| &amp;= \left|\begin{array}{cc}
            1 - \lambda &amp; 2 \\
            4 &amp; -1-\lambda
        \end{array}\right| \\
        {} &amp;= (1-\lambda )(-1-\lambda ) - 8 \\
        {} &amp;= \lambda ^{2} - 9 \\
        {} &amp;= (\lambda + 3)(\lambda - 3). \\
    \end{alignedat}\)</span></p>
</div>
<p>Hence the eigenvalues of <span class="math inline">\(A\)</span> are the roots of <span class="math inline">\((\lambda + 3)(\lambda - 3) = 0\)</span>; that is <span class="math inline">\(\lambda _{1} = -3\)</span> and <span class="math inline">\(\lambda _{2} = 3\)</span>.<br>
To find the eigenvectors corresponding to <span class="math inline">\(\lambda _{1} = -3\)</span>, we find the nullspace of</p>
<div class="center">
<p><span class="math inline">\(A - (-3)I_{2} = \left[\begin{array}{cc} 4 &amp; 2 \\ 4 &amp; 2 \end{array}\right]\)</span></p>
</div>
<p>Row reduction produces</p>
<div class="center">
<p><span class="math inline">\(\left[A+3I_{2} | \mathbf{0}\right] = \left[\begin{array}{cc|c} 4 &amp; 2 &amp; 0 \\ 4 &amp; 2 &amp; 0 \end{array}\right] \xrightarrow{R_{2} - R_{1}} \left[\begin{array}{cc|c} 4 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right]\)</span></p>
</div>
<p>Thus <span class="math inline">\(\mathbf{x} \in {\rm Null}(A+3I_{2})\)</span> if and only if <span class="math inline">\(4x_{1} + 2x_{2} = 0\)</span>.<br>
Setting the free variable <span class="math inline">\(x_{2} = t\)</span>, we see that <span class="math inline">\(x_{1} = -\frac{1}{2}t\)</span>. We take <span class="math inline">\(\left[\begin{array}{c} x_{1} \\ x_{2} \end{array}\right] = \left[\begin{array}{c} -1 \\ 2 \end{array}\right]\)</span> be our eigenvector; or indeed any nonzero multiple of <span class="math inline">\(\left[\begin{array}{c} -1 \\ 2 \end{array}\right]\)</span>.<br>
To find the eigenvectors corresponding to <span class="math inline">\(\lambda _{2} = 3\)</span>, we find the nullspace of <span class="math inline">\(A - 3I_{2}\)</span> by row reduction:</p>
<div class="center">
<p><span class="math inline">\(\left[A - 3I_{2} | \mathbf{0}\right] = \left[\begin{array}{cc|c} -2 &amp; 2 &amp; 0 \\ 4 &amp; -4 &amp; 0 \end{array}\right] \xrightarrow{R_{2} + 2R_{1}} \left[\begin{array}{cc|c} -2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right]\)</span></p>
</div>
<p>So <span class="math inline">\(\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \end{array}\right] \in {\rm Null}(A - 3I_{2})\)</span> if and only if <span class="math inline">\(-2x_{1} + 2x_{2} = 0\)</span>. Setting the free variable <span class="math inline">\(x_{2} = t\)</span>, we find <span class="math inline">\(x_{1} = t\)</span>. We take <span class="math inline">\(\left[\begin{array}{c} x_{1} \\ x_{2} \end{array}\right] = \left[\begin{array}{c} 1 \\ 1 \end{array}\right]\)</span> to be our eigenvector.</p>
</div>
<div class="example">
<p><em>Example 5.4</em>. Find the eigenvalues and the corresponding eigenvectors of</p>
<div class="center">
<p><span class="math inline">\(A = \left[\begin{array}{ccc}
            3 &amp; 2 &amp; 2 \\
            1 &amp; 4 &amp; 1 \\
            -2 &amp; -4 &amp; -1
        \end{array}\right]\)</span>.</p>
</div>
<p><strong><em>Solution</em></strong><br>
The characteristic equation is</p>
<div class="center">
<p><span class="math inline">\(\begin{alignedat}{2}
        0 = |A - \lambda I| &amp;= \left|\begin{array}{ccc}
            3-\lambda &amp; 2 &amp; 2 \\
            1 &amp; 4-\lambda &amp; 1 \\
            -2 &amp; -4 &amp; -1-\lambda
        \end{array}\right| \\
        {} &amp;= (3-\lambda ) \left|\begin{array}{cc} 4-\lambda &amp; 1 \\ -4 &amp; -1-\lambda \end{array}\right| - 2 \left|\begin{array}{cc} 1 &amp; 1 \\ -2 &amp; -1-\lambda \end{array}\right| + 2 \left|\begin{array}{cc} 1 &amp; 4-\lambda \\ -2 &amp; -4\end{array}\right| \\
        {} &amp;= (3-\lambda ) \{(4-\lambda )(-1-\lambda )+4\} - 2\{(-1-\lambda )+2\} + 2\{-4+2(4-\lambda )\} \\
        {} &amp;= -\lambda ^3 + 6\lambda ^2 - 11\lambda + 6 \\
        {} &amp;= (\lambda -1)(-\lambda ^2 + 5\lambda - 6) \\
        {} &amp;= (\lambda -1)\{-(\lambda ^2 - 5\lambda + 6)\} \\
        {} &amp;= (\lambda -1)\{-(\lambda - 2)(\lambda - 3)\} \\
    \end{alignedat}\)</span></p>
</div>
<p>Hence, the eigenvalues are <span class="math inline">\(\lambda _{1} = 1, \lambda _{2} = 2\)</span> and <span class="math inline">\(\lambda _{3} = 3\)</span>.<br>
For <span class="math inline">\(\lambda _{1}=1\)</span>, we compute</p>
<div class="center">
<p><span class="math inline">\(\left[A - I | \mathbf{0}\right] = \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{2}} &amp; 2 &amp; 2 &amp; 0 \\ 1 &amp; 3 &amp; 1 &amp; 0 \\ -2 &amp; -4 &amp; -2 &amp; 0 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_{2} - \frac{1}{2}R_{1} \\ R_{3} + R_{1} \end{subarray}} \left[\begin{array}{ccc | c} 2 &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{2}} &amp; 0 &amp; 0 \\ 0 &amp; -2 &amp; 0 &amp; 0 \end{array}\right] \xrightarrow{R_{3} + R_{2}} \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{2}} &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{2}} &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]\)</span></p>
</div>
<p>from which it follows that an eigenvector <span class="math inline">\(\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right]\)</span> satisfies <span class="math inline">\(x_{2} = 0\)</span> and <span class="math inline">\(x_{3} = -x_{1}\)</span>. We take <span class="math inline">\(\left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right] = \left[\begin{array}{c} 1 \\ 0 \\ -1 \end{array}\right]\)</span> to be our eigenvector .</p>
<p>For <span class="math inline">\(\lambda _{2} = 2\)</span>, we compute</p>
<div class="center">
<p><span class="math inline">\(\left[A - 2I | \mathbf{0}\right] = \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{1}} &amp; 2 &amp; 2 &amp; 0 \\ 1 &amp; 2 &amp; 1 &amp; 0 \\ -2 &amp; -4 &amp; -3 &amp; 0 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_{2} - R_{1} \\ R_{3} + 2R_{1} \end{subarray}} \left[\begin{array}{ccc | c} 1 &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; \textcircled{\raisebox{-0.9pt}{-1}} &amp; 0 \\ 0 &amp; 0 &amp; 3 &amp; 0 \end{array}\right] \xrightarrow{R_{3} + 3R_{2}} \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{1}} &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; \textcircled{\raisebox{-0.9pt}{-1}} &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]\)</span></p>
</div>
<p>which gives an eigenvector <span class="math inline">\(\mathbf{x} = \left[\begin{array}{c} -2 \\ 1 \\ 0 \end{array}\right]\)</span>.</p>
<p>For <span class="math inline">\(\lambda _{3} = 3\)</span>, we compute</p>
<div class="center">
<p><span class="math inline">\(\left[A - 3I | \mathbf{0}\right] = \left[\begin{array}{ccc | c} 0 &amp; 2 &amp; 2 &amp; 0 \\ 1 &amp; 1 &amp; 1 &amp; 0 \\ -2 &amp; -4 &amp; -4 &amp; 0 \end{array}\right] \xrightarrow{R_{2} \leftrightarrow R_{1}} \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{1}} &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 2 &amp; 2 &amp; 0 \\ -2 &amp; -4 &amp; -4 &amp; 0 \end{array}\right] \xrightarrow{R_{3} + 2R_{1}} \left[\begin{array}{ccc | c} 1 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{2}} &amp; 2 &amp; 0 \\ 0 &amp; -2 &amp; -2 &amp; 0 \end{array}\right]\)</span><br>
<span class="math inline">\(\xrightarrow{R_{3} + R_{2}} \left[\begin{array}{ccc | c} 1 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]\)</span></p>
</div>
<p>which gives an eigenvector <span class="math inline">\(\mathbf{x} = \left[\begin{array}{c} 0 \\ -1 \\ 1 \end{array}\right]\)</span>.</p>
</div>
<div class="example">
<p><em>Example 5.5</em>. Find the eigenvalues and the corresponding eigenspaces of</p>
<div class="center">
<p><span class="math inline">\(A = \left[\begin{array}{ccc}
        0 &amp; 1 &amp; 0 \\
        0 &amp; 0 &amp; 1 \\
        2 &amp; -5 &amp; 4
    \end{array}\right]\)</span>.</p>
</div>
<p><strong><em>Solution</em></strong><br>
The characteristic equations is</p>
<div class="center">
<p><span class="math inline">\(\begin{alignedat}{2}
        0 = |A - \lambda I| &amp;= \left|\begin{array}{ccc}
            -\lambda &amp; 1 &amp; 0 \\
            0 &amp; -\lambda &amp; 1 \\
            2 &amp; -5 &amp; 4-\lambda
        \end{array}\right| \\
        {} &amp;= -\lambda \left|\begin{array}{cc} -\lambda &amp; 1 \\ -5 &amp; 4-\lambda \end{array}\right| -  \left|\begin{array}{cc} 0 &amp; 1 \\ 2 &amp; 4-\lambda \end{array}\right| \\
        {} &amp;= -\lambda (\lambda ^2-4\lambda +5)-(-2) \\
        {} &amp;= -\lambda ^3 + 4\lambda ^2 - 5\lambda + 2 \\
        {} &amp;= (\lambda -1)(-\lambda ^2 + 3\lambda - 2) \\
        {} &amp;= -(\lambda -1)^2(\lambda -2) \\
    \end{alignedat}\)</span></p>
</div>
<p>Hence, the eigenvalues are <span class="math inline">\(\lambda _{1} = \lambda _{2} = 1\)</span> and <span class="math inline">\(\lambda _{3} = 2\)</span>.<br>
To find the eigenvectors corresponding to <span class="math inline">\(\lambda _{1} = \lambda _{2} = 1\)</span>, we compute</p>
<div class="center">
<p><span class="math inline">\(\left[A - I | \mathbf{0}\right] = \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{-1}} &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 1 &amp; 0 \\ 2 &amp; -5 &amp; 3 &amp; 0 \end{array}\right] \xrightarrow{R_{3} + 2R_{1}} \left[\begin{array}{ccc | c} -1 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{-1}} &amp; 1 &amp; 0 \\ 0 &amp; -3 &amp; 3 &amp; 0 \end{array}\right] \xrightarrow{R_{3} - 3R_{2}} \left[\begin{array}{ccc | c} -1 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]\)</span></p>
</div>
<p>Thus, <span class="math inline">\(\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right]\)</span> is in the eigenspace <span class="math inline">\(E_{1}\)</span> if and only if <span class="math inline">\(-x_{1} + x_{2} = 0\)</span> and <span class="math inline">\(-x_{2} + x_{3} = 0\)</span>. Setting the free variable <span class="math inline">\(x_{3} = t\)</span>, we see that <span class="math inline">\(x_{1} = t\)</span> and <span class="math inline">\(x_{2} = t\)</span>, from which it follows that</p>
<div class="center">
<p><span class="math inline">\(E_{1} = \left\{\left[\begin{array}{c} t \\ t \\ t \end{array}\right]\right\} = \left\{t \left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right]\right\} = {\rm span}\left(\left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right]\right)\)</span></p>
</div>
<p>To find the eigenvectors correspond to <span class="math inline">\(\lambda _{3} = 2\)</span>, we find the nullspace of <span class="math inline">\(A - 2I\)</span> by row reduction:</p>
<div class="center">
<p><span class="math inline">\(\left[A - 2I | \mathbf{0}\right] = \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{-2}} &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -2 &amp; 1 &amp; 0 \\ 2 &amp; -5 &amp; 2 &amp; 0 \end{array}\right] \xrightarrow{R_{3} + R_{1}} \left[\begin{array}{ccc | c} -2 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{-2}} &amp; 1 &amp; 0 \\ 0 &amp; -4 &amp; 2 &amp; 0 \end{array}\right] \xrightarrow{R_{3} - 2R_{2}} \left[\begin{array}{ccc | c} -2 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -2 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]\)</span></p>
</div>
<p>So <span class="math inline">\(\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right]\)</span> is in the eigenspace <span class="math inline">\(E_{2}\)</span> if and only if <span class="math inline">\(-2x_{1} + x_{2} = 0\)</span> and <span class="math inline">\(-2x_{2} + x_{3} = 0\)</span>. Setting the free variable <span class="math inline">\(x_{3} = t\)</span>, we have</p>
<div class="center">
<p><span class="math inline">\(E_{2} = \left\{\left[\begin{array}{c} \frac{1}{4}t \\ \frac{1}{2}t \\ t \end{array}\right]\right\} = \left\{t \left[\begin{array}{c} \frac{1}{4} \\ \frac{1}{2} \\ 1 \end{array}\right]\right\} = {\rm span}\left(\left[\begin{array}{c} \frac{1}{4} \\ \frac{1}{2} \\ 1 \end{array}\right]\right) = {\rm span}\left(\left[\begin{array}{c} 1 \\ 2 \\ 4 \end{array}\right]\right)\)</span></p>
</div>
</div>
<div class="defn">
<p><strong>Definition 5.4</strong>. The <strong><em>algebraic multiplicity</em></strong> of an eigenvalue is its multiplicity as a root of the characteristic equation.<br>
The <strong><em>geometric multiplicity</em></strong> of an eigenvalue <span class="math inline">\(\lambda\)</span> is <span class="math inline">\({\rm dim} (E_{\lambda })\)</span>, the dimension of its corresponding eigenspace.</p>
</div>
<p>In the above Example, <span class="math inline">\(\lambda = 1\)</span> has algebraic multiplicity <span class="math inline">\(2\)</span> and geometric multiplicity <span class="math inline">\(1\)</span>. <span class="math inline">\(\lambda = 2\)</span> has algebraic multiplicity <span class="math inline">\(1\)</span> and geometric multiplicity <span class="math inline">\(1\)</span>.</p>
<div class="thm">
<p><strong>Theorem 5.2</strong>. <em>The eigenvalues of a triangular matrix are the entries on its main diagonal.</em></p>
</div>
<div class="example">
<p><em>Example 5.6</em>. Let</p>
<div class="center">
<p><span class="math inline">\(A = \left[\begin{array}{cccc} 2 &amp; 0 &amp; 0 &amp; 0 \\ -1 &amp; 1 &amp; 0 &amp; 0 \\ 3 &amp; 0 &amp; 3 &amp; 0 \\ 5 &amp; 7 &amp; 4 &amp; -2 \end{array}\right]\)</span></p>
</div>
<p>The characteristic polynomial is:</p>
<div class="center">
<p><span class="math inline">\(\begin{alignedat}{2}
        |A - \lambda I| &amp;= \left|\begin{array}{cccc} 2-\lambda &amp; 0 &amp; 0 &amp; 0 \\ -1 &amp; 1-\lambda &amp; 0 &amp; 0 \\ 3 &amp; 0 &amp; 3-\lambda &amp; 0 \\ 5 &amp; 7 &amp; 4 &amp; -2-\lambda \end{array}\right| \\
        {} &amp;= (2-\lambda )\left|\begin{array}{ccc} 1-\lambda &amp; 0 &amp; 0 \\ 0 &amp; 3-\lambda &amp; 0 \\ 7 &amp; 4 &amp; -2-\lambda \end{array}\right| \\
        {} &amp;= (2-\lambda )(1-\lambda )\left|\begin{array}{cc} 3-\lambda &amp; 0 \\ 4 &amp; -2-\lambda \end{array}\right| \\
        {} &amp;= (2-\lambda )(1-\lambda )(3-\lambda )(-2-\lambda )
    \end{alignedat}\)</span></p>
</div>
<p>Hence, the eigenvalues are <span class="math inline">\(\lambda _{1}=2, \lambda _{2}=1, \lambda _{3}=3, \lambda _{4}=-2\)</span>.</p>
</div>
<p>Note that diagonal matrices are a special case of Theorem 4.2.</p>
<div class="thm">
<p><strong>Theorem 5.3</strong>. <em>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> matrix and let <span class="math inline">\(\lambda _{1}, \lambda _{2}, ..., \lambda _{m}\)</span> be distinct eigenvalues of <span class="math inline">\(A\)</span> with corresponding eigenvectors <span class="math inline">\(\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}\)</span>. Then <span class="math inline">\(\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}\)</span> are linearly independent.</em></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We prove this by contradiction.<br>
Suppose <span class="math inline">\(\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}\)</span> are linearly dependent. Let <span class="math inline">\(\mathbf{v_{k+1}}\)</span> be the first of the vectors <span class="math inline">\(\mathbf{v_{i}}\)</span> that can be expressed as a linear combination of the previous ones. In other words, <span class="math inline">\(\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{k}}\)</span> are linearly independent, but there are <span class="math inline">\(c_{1}, c_{2}, ..., c_{k}\)</span> such that</p>
<div class="center">
<p><span class="math inline">\(\mathbf{v_{k+1}} = c_{1}\mathbf{v_{1}} + c_{2}\mathbf{v_{2}} + ... + c_{k}\mathbf{v_{k}} \qquad (1)\)</span></p>
</div>
<p>Multiplying both sides of Equation (1) by <span class="math inline">\(A\)</span> from left and using the fact that <span class="math inline">\(A\mathbf{v_{i}} = \lambda _{i}\mathbf{v_{i}}\)</span> for each <span class="math inline">\(i\)</span>, we have</p>
<div class="center">
<p><span class="math inline">\(\begin{alignedat}{2}
        \lambda _{k+1}\mathbf{v_{k+1}} = A\mathbf{v_{k+1}} &amp;= A(c_{1}\mathbf{v_{1}} + c_{2}\mathbf{v_{2}} + ... + c_{k}\mathbf{v_{k}}) \\
        {} &amp;= c_{1}A\mathbf{v_{1}} + c_{2}A\mathbf{v_{2}} + ... + c_{k}A\mathbf{v_{k}} \\
        {} &amp;= c_{1}\lambda _{1}\mathbf{v_{1}} + c_{2}\lambda _{2}\mathbf{v_{2}} + ... + c_{k}\lambda _{k}\mathbf{v_{k}} \qquad (2)
    \end{alignedat}\)</span></p>
</div>
<p>Now we multiply both sides of Equation (1) by <span class="math inline">\(\lambda _{k+1}\)</span> to obtain</p>
<div class="center">
<p><span class="math inline">\(\lambda _{k+1}\mathbf{v_{k+1}} = c_{1}\lambda _{k+1}\mathbf{v_{1}} + c_{2}\lambda _{k+1}\mathbf{v_{2}} + ... + c_{k}\lambda _{k+1}\mathbf{v_{k}} \qquad (3)\)</span></p>
</div>
<p>When we subtract Equation (3) from Equation (2), we obtain</p>
<div class="center">
<p><span class="math inline">\(\mathbf{0} = c_{1}(\lambda _{1}-\lambda _{k+1})\mathbf{v_{1}} + c_{2}(\lambda _{2}-\lambda _{k+1})\mathbf{v_{2}} + ... + c_{k}(\lambda _{k}-\lambda _{k+1})\mathbf{v_{k}}\)</span></p>
</div>
<p>The linear independence of <span class="math inline">\(\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{k}}\)</span> implies that</p>
<div class="center">
<p><span class="math inline">\(c_{1}(\lambda _{1}-\lambda _{k+1}) = c_{2}(\lambda _{2}-\lambda _{k+1}) = ... = c_{k}(\lambda _{k}-\lambda _{k+1}) = 0\)</span></p>
</div>
<p>Since the eigenvalues <span class="math inline">\(\lambda _{i}\)</span> are all distinct, <span class="math inline">\(\lambda _{i} - \lambda _{k+1} \neq 0\)</span> for all <span class="math inline">\(i = 1, ..., k\)</span>. Hence <span class="math inline">\(c_{1} = c_{2} = ... = c_{k} = 0\)</span>. This implies that</p>
<div class="center">
<p><span class="math inline">\(\mathbf{v_{k+1}} = 0\mathbf{v_{1}} + 0\mathbf{v_{2}} + ... + 0\mathbf{v_{k}} = \mathbf{0}\)</span></p>
</div>
<p>which is impossible since the eigenvector <span class="math inline">\(\mathbf{v_{k+1}}\)</span> cannot be zero.<br>
Thus, our assumption that <span class="math inline">\(\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}\)</span> are linearly dependent is false. It follows that <span class="math inline">\(\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}\)</span> must be linearly independent.&nbsp;◻</p>
</div>
</section>
<section id="similarity-and-diagonalisation" class="level2">
<h2 class="anchored" data-anchor-id="similarity-and-diagonalisation">Similarity and Diagonalisation</h2>
<section id="introduction-1" class="level3">
<h3 class="anchored" data-anchor-id="introduction-1">Introduction</h3>
<p>In many applications, matrices represent linear mappings of vectors in a physical space, as in the example given at the start of the Eigenvector section. The choice of a coordinate system (in particular, its orientation) in this space is arbitrary, and this choice determines what the matrix looks like. In this section, we show that under certain conditions, there exists a choice of a coordinate system in which the matrix becomes a diagonal matrix. In this case, the coordinate axes have the direction of the eigenvectors of the matrix, and the diagonal elements of the matrix are the eigenvalues.</p>
</section>
<section id="similar-matrices" class="level3">
<h3 class="anchored" data-anchor-id="similar-matrices">Similar Matrices</h3>
<div class="defn">
<p><strong>Definition 5.5</strong>. Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be <span class="math inline">\(n \times n\)</span> matrices. We say that <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(B\)</span> if there is an invertible <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(P^{-1}AP = B\)</span>. If <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(B\)</span>, we write <span class="math inline">\(A \sim B\)</span>.</p>
</div>
<div class="rem">
<p><em>Remark 5.1</em>. If <span class="math inline">\(A \sim B\)</span>, we can write, equivalently, that <span class="math inline">\(A = PBP^{-1}\)</span> or <span class="math inline">\(AP = PB\)</span>.<br>
The matrix <span class="math inline">\(P\)</span> depends on <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. It is not unique for a given pair of similar matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
</div>
<div class="thm">
<p><strong>Theorem 5.4</strong>. <em>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be <span class="math inline">\(n \times n\)</span> matrices with <span class="math inline">\(A \sim B\)</span>. Then<br>
(a) <span class="math inline">\(det(A) = det(B)\)</span><br>
(b) <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> have the same rank.<br>
(c) <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> have the same characteristic polynomial.<br>
(d) <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> have the same eigenvalues.</em></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(A \sim B\)</span>, then <span class="math inline">\(P^{-1}AP = B\)</span> for some invertible matrix <span class="math inline">\(P\)</span>.<br>
(a)<br>
<span class="math inline">\(\begin{alignedat}{2}
            \qquad det(B) &amp;= det(P^{-1}AP) \\
            {} &amp;= det(P^{-1})det(A)det(P) \\
            {} &amp;= \frac{1}{det(P)}det(A)det(P) \\
            {} &amp;= det(A). \\
        \end{alignedat}\)</span></p>
<p>(c) The characteristic polynomial of <span class="math inline">\(B\)</span> is</p>
<div class="center">
<p><span class="math inline">\(\begin{alignedat}{2}
            det(B - \lambda I) &amp;= det(P^{-1}AP - \lambda I) \\
            {} &amp;= det(P^{-1}AP - \lambda P^{-1}IP) \\
            {} &amp;= det(P^{-1}AP - P^{-1}(\lambda I)P) \\
            {} &amp;= det(P^{-1}(A - \lambda I)P) \\
            {} &amp;= det(P^{-1})det(A - \lambda I)det(P) \\
            {} &amp;= \frac{1}{det(P)}det(A - \lambda I)det(P) \\
            {} &amp;= det(A - \lambda I)
        \end{alignedat}\)</span></p>
</div>
<p>&nbsp;◻</p>
</div>
<p>Theorem 4.5 is helpful in showing that two matrices are not similar, since <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> cannot be similar if any of the properties fail.</p>
<div class="example">
<p><em>Example 5.7</em>. (a) <span class="math inline">\(A = \left[\begin{array}{cc} 1 &amp; 2 \\ 2 &amp; 1 \end{array}\right]\)</span> and <span class="math inline">\(B = \left[\begin{array}{cc} 2 &amp; 1 \\ 1 &amp; 2 \end{array}\right]\)</span> are not similar, since <span class="math inline">\(det(A) = -3\)</span> but <span class="math inline">\(det(B) = 3\)</span>.<br>
(b) <span class="math inline">\(A = \left[\begin{array}{cc} 1 &amp; 3 \\ 2 &amp; 2 \end{array}\right]\)</span> and <span class="math inline">\(B = \left[\begin{array}{cc} 1 &amp; 1 \\ 3 &amp; -1 \end{array}\right]\)</span> are not similar, since <span class="math inline">\(|A - \lambda I| = \lambda ^{2} - 3\lambda - 4\)</span> while <span class="math inline">\(|B - \lambda I| = \lambda ^{2} - 4\)</span>. Note that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> have the same determinant and rank, however.</p>
</div>
</section>
<section id="diagonalisation" class="level3">
<h3 class="anchored" data-anchor-id="diagonalisation">Diagonalisation</h3>
<div class="defn">
<p><strong>Definition 5.6</strong>. An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is <strong><em>diagonalisable</em></strong> if there is a diagonal matrix <span class="math inline">\(D\)</span> such that <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(D\)</span> - that is, if there is an invertible <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(P^{-1}AP = D\)</span>.</p>
</div>
<div class="thm">
<p><strong>Theorem 5.5</strong>. <em>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> matrix. Then <span class="math inline">\(A\)</span> is diagonalisable if and only if <span class="math inline">\(A\)</span> has <span class="math inline">\(n\)</span> linearly independent eigenvectors.<br>
More precisely, there exists an invertible matrix <span class="math inline">\(P\)</span> and a diagonal matrix <span class="math inline">\(D\)</span> such that <span class="math inline">\(P^{-1}AP = D\)</span> if and only if the columns of <span class="math inline">\(P\)</span> are <span class="math inline">\(n\)</span> linearly independent eigenvectors of <span class="math inline">\(A\)</span> and the diagonal entries of <span class="math inline">\(D\)</span> are the eigenvalues of <span class="math inline">\(A\)</span> corresponding to the eigenvectors in <span class="math inline">\(P\)</span> in the same order.</em></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose first that <span class="math inline">\(A\)</span> is similar to the diagonal matrix <span class="math inline">\(D\)</span> by <span class="math inline">\(P^{-1}AP = D\)</span> or, equivalently, <span class="math inline">\(AP = PD\)</span>. Let the columns of <span class="math inline">\(P\)</span> be <span class="math inline">\(\mathbf{p_{1}}, \mathbf{p_{2}}, ..., \mathbf{p_{n}}\)</span> and let the diagonal entries of <span class="math inline">\(D\)</span> be <span class="math inline">\(\lambda _{1}, \lambda _{2}, ..., \lambda _{n}\)</span>. Then</p>
<div class="center">
<p><span class="math inline">\(A\left[\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots, \mathbf{p_{n}}\right] = \left[\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots, \mathbf{p_{n}}\right] \left[\begin{array}{cccc} \lambda _{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda _{2} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda _{n} \end{array}\right] \qquad (1)\)</span><br>
or <span class="math inline">\(\qquad \left[A\mathbf{p_{1}}, A\mathbf{p_{2}}, \cdots, A\mathbf{p_{n}}\right] = \left[\lambda _{1}\mathbf{p_{1}}, \lambda _{2}\mathbf{p_{2}}, \cdots, \lambda _{n}\mathbf{p_{n}}\right] \qquad (2)\)</span></p>
</div>
<p>Equating columns, we have</p>
<div class="center">
<p><span class="math inline">\(A\mathbf{p_{1}} = \lambda _{1}\mathbf{p_{1}}, A\mathbf{p_{2}} = \lambda _{2}\mathbf{p_{2}}, \cdots , A\mathbf{p_{n}} = \lambda _{n}\mathbf{p_{n}}\)</span></p>
</div>
<p>which proves that the column vectors of <span class="math inline">\(P\)</span> are eigenvectors of <span class="math inline">\(A\)</span> whose corresponding eigenvalues are the diagonal entries of <span class="math inline">\(D\)</span> in the same order. Since <span class="math inline">\(P\)</span> is invertible, its columns are linearly independent.<br>
Conversely, if <span class="math inline">\(A\)</span> has <span class="math inline">\(n\)</span> linearly independent eigenvectors <span class="math inline">\(\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots , \mathbf{p_{n}}\)</span> with corresponding eigenvalues <span class="math inline">\(\lambda _{1}, \lambda _{2}, \cdots , \lambda _{n}\)</span>, respectively, then</p>
<div class="center">
<p><span class="math inline">\(A\mathbf{p_{1}} = \lambda _{1}\mathbf{p_{1}}, A\mathbf{p_{2}} = \lambda _{2}\mathbf{p_{2}}, \cdots , A\mathbf{p_{n}} = \lambda _{n}\mathbf{p_{n}}\)</span></p>
</div>
<p>This implies Eq. (2), which is equivalent to Eq. (1), that is <span class="math inline">\(AP = PD\)</span>. Since the columns <span class="math inline">\(\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots , \mathbf{p_{n}}\)</span> of <span class="math inline">\(P\)</span> are linearly independent, <span class="math inline">\(P\)</span> is invertible, so <span class="math inline">\(P^{-1}AP = D\)</span>, that is, <span class="math inline">\(A\)</span> is diagonalisable.&nbsp;◻</p>
</div>
<div class="example">
<p><em>Example 5.8</em>. If possible, find a matrix <span class="math inline">\(P\)</span> that diagonalises</p>
<div class="center">
<p><span class="math inline">\(A = \left[\begin{array}{ccc}
            0 &amp; 1 &amp; 0 \\
            0 &amp; 0 &amp; 1 \\
            2 &amp; -5 &amp; 4
        \end{array}\right]\)</span></p>
</div>
<p><strong><em>Solution</em></strong><br>
We studied this matrix previously and found that it has eigenvalues <span class="math inline">\(\lambda _{1} = \lambda _{2} = 1\)</span> and <span class="math inline">\(\lambda _{3} = 2\)</span>. The eigenspaces have the following bases:<br>
For <span class="math inline">\(\lambda _{1} = \lambda _{2} = 1\)</span>, <span class="math inline">\(E_{1}\)</span> has basis <span class="math inline">\(\left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right]\)</span>.<br>
For <span class="math inline">\(\lambda _{3} = 2\)</span>, <span class="math inline">\(E_{2}\)</span> has basis <span class="math inline">\(\left[\begin{array}{c} 1 \\ 2 \\ 4 \end{array}\right]\)</span>.<br>
Since all other eigenvectors are just multiples of one of these two basis vectors, there cannot be three linearly independent eigenvectors. By Theorem 4.6, <span class="math inline">\(A\)</span> is not diagonalisable.</p>
</div>
<div class="example">
<p><em>Example 5.9</em>. If possible, find a matrix <span class="math inline">\(P\)</span> that diagonalises</p>
<div class="center">
<p><span class="math inline">\(A = \left[\begin{array}{ccc}
            2 &amp; 2 &amp; 0 \\
            0 &amp; 1 &amp; 0 \\
            -4 &amp; -8 &amp; 1
        \end{array}\right]\)</span></p>
</div>
<p><strong><em>Solution</em></strong><br>
This is the matrix of Question 3, Worksheet 6. There we found that the eigenvalues of <span class="math inline">\(A\)</span> are <span class="math inline">\(\lambda _{1} = 2\)</span> and <span class="math inline">\(\lambda _{2} = \lambda _{3} = 1\)</span>, with the following bases for the eigenspaces:<br>
For <span class="math inline">\(\lambda _{1} = 2\)</span>, <span class="math inline">\(E_{2}\)</span> has basis <span class="math inline">\(\mathbf{p_{1}} = \left[\begin{array}{c} 1 \\ 0 \\ -4 \end{array}\right]\)</span>.<br>
For <span class="math inline">\(\lambda _{2} = \lambda _{3} = 1\)</span>, <span class="math inline">\(E_{1}\)</span> has basis <span class="math inline">\(\mathbf{p_{2}} = \left[\begin{array}{c} -2 \\ 1 \\ 0 \end{array}\right]\)</span> and <span class="math inline">\(\mathbf{p_{3}} = \left[\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right]\)</span>.<br>
Now we check whether <span class="math inline">\(\left\{\mathbf{p_{1}}, \mathbf{p_{2}}, \mathbf{p_{3}}\right\}\)</span> is linearly independent.</p>
<div class="center">
<p><span class="math inline">\(\left[\begin{array}{ccc} \textcircled{\raisebox{-0.9pt}{1}} &amp; -2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -4 &amp; 0 &amp; 1 \end{array}\right] \xrightarrow{R_{3} + 4R_{1}} \left[\begin{array}{ccc} 1 &amp; -2 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{1}} &amp; 0 \\ 0 &amp; -8 &amp; 1 \end{array}\right] \xrightarrow{R_{3} + 8R_{2}} \left[\begin{array}{ccc} 1 &amp; -2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right]\)</span></p>
</div>
<p>Since <span class="math inline">\(rank = 3\)</span>, <span class="math inline">\(\left\{\mathbf{p_{1}}, \mathbf{p_{2}}, \mathbf{p_{3}}\right\}\)</span> is linearly independent. Thus, if we take</p>
<div class="center">
<p><span class="math inline">\(P = \left[\mathbf{p_{1}}, \mathbf{p_{2}}, \mathbf{p_{3}}\right] = \left[\begin{array}{ccc} 1 &amp; -2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -4 &amp; 0 &amp; 1 \end{array}\right]\)</span></p>
</div>
<p>then <span class="math inline">\(P\)</span> is invertible. Furthermore,</p>
<div class="center">
<p><span class="math inline">\(P^{-1}AP = \left[\begin{array}{ccc} 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right] = D\)</span></p>
</div>
<p>(Note: It is much easier to check the equivalent equation <span class="math inline">\(AP = PD\)</span>).</p>
</div>
<div class="rem">
<p><em>Remark 5.2</em>. Eigenvectors can be placed into the columns of <span class="math inline">\(P\)</span> in any order. However, the eigenvalues will come up on the diagonal of <span class="math inline">\(D\)</span> in the same order as their corresponding eigenvectors in <span class="math inline">\(P\)</span>. For example, if we had chosen</p>
<div class="center">
<p><span class="math inline">\(P = \left[\mathbf{p_{2}}, \mathbf{p_{3}}, \mathbf{p_{1}}\right] = \left[\begin{array}{ccc} -2 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; -4 \end{array}\right]\)</span></p>
</div>
<p>Then we would have found</p>
<div class="center">
<p><span class="math inline">\(P^{-1}AP = \left[\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{array}\right]\)</span></p>
</div>
<p>We checked that the eigenvectors <span class="math inline">\(\mathbf{p_{1}}, \mathbf{p_{2}}\)</span> and <span class="math inline">\(\mathbf{p_{3}}\)</span> were linearly independent. However, the following Theorem guarantees that linear independence is preserved when the bases of different subspaces are combined.</p>
</div>
<div class="thm">
<p><strong>Theorem 5.6</strong>. <em>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> matrix and let <span class="math inline">\(\lambda _{1}, \lambda _{2}, \cdots, \lambda _{k}\)</span> be distinct eigenvalues of <span class="math inline">\(A\)</span>. If <span class="math inline">\(B_{i}\)</span> is a basis for the eigenspace <span class="math inline">\(E_{i}\)</span>, then <span class="math inline">\(B = B_{1} \cup B_{2} \cup \cdots \cup B_{k}\)</span> (i.e.&nbsp;the total collection of basis vectors for all of the eigenspaces) is linearly independent.</em></p>
</div>
<div class="thm">
<p><strong>Theorem 5.7</strong>. <em>If <span class="math inline">\(A\)</span> is an <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\(n\)</span> distinct eigenvalues, then <span class="math inline">\(A\)</span> is diagonalisable.</em></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(\mathbf{v_{1}}, \mathbf{v_{2}}, \cdots, \mathbf{v_{n}}\)</span> be eigenvectors corresponding to the <span class="math inline">\(n\)</span> distinct eigenvalues of <span class="math inline">\(A\)</span>. By theorem 4.3, <span class="math inline">\(\mathbf{v_{1}}, \mathbf{v_{2}}, \cdots, \mathbf{v_{n}}\)</span> are linearly independent, so, by Theorem 4.6, <span class="math inline">\(A\)</span> is diagonalisable.&nbsp;◻</p>
</div>
<div class="example">
<p><em>Example 5.10</em>. The matrix</p>
<div class="center">
<p><span class="math inline">\(A = \left[\begin{array}{cccc}
            2 &amp; 0 &amp; 0 &amp; 0 \\
            -1 &amp; 1 &amp; 0 &amp; 0 \\
            3 &amp; 0 &amp; 3 &amp; 0 \\
            5 &amp; 7 &amp; 4 &amp; -2
        \end{array}\right]\)</span></p>
</div>
<p>has eigenvalues <span class="math inline">\(\lambda _{1} = 2, \lambda _{2} = 1, \lambda _{3} = 3\)</span> and <span class="math inline">\(\lambda _{4} = -2\)</span>, by Theorem 4.2. Since these are four distinct eigenvalues for a <span class="math inline">\(4\)</span>x<span class="math inline">\(4\)</span> matrix, <span class="math inline">\(A\)</span> is diagonalisable, by Theorem 4.8.</p>
</div>
<div class="lem">
<p><strong>Lemma 5.8</strong>. <em>If <span class="math inline">\(A\)</span> is an <span class="math inline">\(n \times n\)</span> matrix, then the geometric multiplicity of each eigenvalue is less than or equal to its algebraic multiplicity.</em></p>
</div>
<div class="thm">
<p><strong>Theorem 5.9</strong>. ****The Diagonalisation Theorem**<em><br>
Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times n\)</span> matrix whose distinct eigenvalues are <span class="math inline">\(\lambda _{1}, \lambda _{2}, \cdots, \lambda _{k}\)</span>, where <span class="math inline">\(1\leq k\leq n\)</span>. The following statements are equivalent:<br>
(a) <span class="math inline">\(A\)</span> is diagonalisable.<br>
(b) The union <span class="math inline">\(B\)</span> of the bases of the eigenspaces of <span class="math inline">\(A\)</span> contains <span class="math inline">\(n\)</span> vectors.<br>
(c) The algebraic multiplicity of each eigenvalue equals its geometric multiplicity.</em></p>
</div>
<div class="example">
<p><em>Example 5.11</em>. (a) The matrix <span class="math inline">\(A = \left[\begin{array}{ccc}
        0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 2 &amp; -5 &amp; 4 \end{array}\right]\)</span> has two distinct eigenvalues <span class="math inline">\(\lambda _{1} = \lambda _{2} = 1\)</span> and <span class="math inline">\(\lambda _{3} = 2\)</span>. Since the eigenvalue <span class="math inline">\(\lambda _{1} = \lambda _{2} = 1\)</span> with <span class="math inline">\(E_{1} = {\rm span}\left(\left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right]\right)\)</span> has algebraic multiplicity <span class="math inline">\(2\)</span> but geometric multiplicity <span class="math inline">\(1\)</span>, <span class="math inline">\(A\)</span> is not diagonalisable, by the Diagonalisation Theorem.<br>
(b) The matrix <span class="math inline">\(A = \left[\begin{array}{ccc}
        2 &amp; 2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -4 &amp; -8 &amp; 1 \end{array}\right]\)</span> has two distinct eigenvalues <span class="math inline">\(\lambda _{1} = 2\)</span> and <span class="math inline">\(\lambda _{2} = \lambda _{3} = 1\)</span>. We found:<br>
for <span class="math inline">\(\lambda _{1} = 2\)</span>, <span class="math inline">\(E_{2}\)</span> has basis <span class="math inline">\(\mathbf{p_{1}} = \left[\begin{array}{c} 1 \\ 0 \\ -4 \end{array}\right]\)</span><br>
for <span class="math inline">\(\lambda _{2} = \lambda _{3} = 1\)</span>, <span class="math inline">\(E_{1}\)</span> has basis <span class="math inline">\(\mathbf{p_{2}} = \left[\begin{array}{c} -2 \\ 1 \\ 0 \end{array}\right]\)</span> and <span class="math inline">\(\mathbf{p_{3}} = \left[\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right]\)</span><br>
Thus, the eigenvalue <span class="math inline">\(2\)</span> has algebraic and geometric multiplicity <span class="math inline">\(1\)</span>, and the eigenvalue <span class="math inline">\(1\)</span> has algebraic and geometric multiplicity <span class="math inline">\(2\)</span>. Thus, <span class="math inline">\(A\)</span> is diagonalisable, by the Diagonalisation Theorem.</p>
</div>
</section>
</section>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="carl-friedrich-gauss" class="level2">
<h2 class="anchored" data-anchor-id="carl-friedrich-gauss">Carl Friedrich Gauss</h2>
<p><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span id="Gauss" data-label="Gauss"></span> Carl Friedrich Gauss (Gauß) (30 April 1777 - 23 February 1855) was a German mathematician and scientist of profound genius who contributed significantly to many fields, including number theory, analysis, differential geometry, geodesy, magnetism, astronomy and optics. Sometimes known as "the prince of mathematicians" and "greatest mathematician since antiquity", Gauss had a remarkable influence in many fields of mathematics and science and is ranked as one of history’s most influential mathematicians.</p>
<p>Gauss was a child prodigy, of whom there are many anecdotes pertaining to his astounding precocity while a mere toddler, and made his first ground-breaking mathematical discoveries while still a teenager. He completed Disquisitiones Arithmeticae, his magnum opus, at the age of twenty-one (1798), though it would not be published until 1801. This work was fundamental in consolidating number theory as a discipline and has shaped the field to the present day.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Gauss.jpg" class="img-fluid figure-img" style="width:6cm"></p>
<figcaption>image</figcaption>
</figure>
</div>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>from Wikipedia, the free encyclopedia<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>