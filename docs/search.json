[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Core Mathematics III",
    "section": "",
    "text": "Introduction\nWelcome to MA21001 at the University of Dundee.\nThese notes are available at dundeemath.github.io/MA21001/ as HTML and also as a PDF.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "Core Mathematics III",
    "section": "Licence",
    "text": "Licence\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "notation.html",
    "href": "notation.html",
    "title": "1  Before we start …",
    "section": "",
    "text": "1.1 Linear Algebra\nAlgebra: The department of mathematics which investigates the relations and properties of numbers by means of general symbols; and, in a more abstract sense, a calculus of symbols combining according to certain defined laws (Oxford English Dictionary)\nLinear algebra is the branch of mathematics concerned with the study of vectors, vector spaces (also called linear spaces), linear transformations, and systems of linear equations. Vector spaces are a central theme in modern mathematics; thus, linear algebra is widely used in both abstract algebra and functional analysis. Linear algebra also has a concrete representation in analytic geometry and is generalised in operator theory. It has extensive applications in the natural sciences and the social sciences since a linear model can often approximate nonlinear models.\nThe name Algebra (from Arabic: al-jabr) is derived from the treatise written by the Persian mathematician Muhammad ibn Musa al-Kwarizmi titled Al-Kitab al-Jabr wa-l-Muqabala, meaning “The Compendious Book on Calculation by Completion and Balancing\", which provided symbolic operations for the systematic solution of linear and quadratic equations 1.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Before we start ...</span>"
    ]
  },
  {
    "objectID": "notation.html#the-language-of-mathematics",
    "href": "notation.html#the-language-of-mathematics",
    "title": "1  Before we start …",
    "section": "1.2 The Language of Mathematics",
    "text": "1.2 The Language of Mathematics\nTo formulate mathematics, we first have to define the notions we are using before we make any statement. This allows us to prove general statements, which is the essence of mathematics. Mathematical notions are defined in Axioms and Definitions, while statements are called Theorems or Corollaries:\n\n1.2.1 Axioms\na fundamental definition or statement which can’t be derived from any simpler statement. Axioms are the starting points of a mathematical theory. Often, axioms are so \"self-evident\" that they are not explicitly mentioned.\nExample: \"Peano Axioms\" for the natural numbers.\n\n0 is a natural number.\nFor every natural number n, the successor, S(n), is a natural number.\nFor every natural number n, S(n) \\neq 0. That is, there is no natural number whose successor is 0.\nFor all natural numbers m and n, if S(m) = S(n), then m = n. That is, S is an injection.\n\nThe original Peano axioms (published in 1889 by the Italian mathematician Giuseppe Peano) also included a set of axioms for the notion of equality and one further “axiom of induction\".\n\nFor every natural number x, x = x. That is, equality is reflexive.\nFor all natural numbers x and y, if x = y, then y = x. That is, equality is symmetric.\nFor all natural numbers x, y and z, if x = y and y = z, then x = z. That is, equality is transitive.\nFor all a and b, if a is a natural number and a = b, then b is also a natural number. That is, the natural numbers are closed under equality.\nIf K is a set such that 0 is in K, and for every natural number n, if n is in K, then S(n) is in K, then K contains every natural number. (axiom of induction)\n\n\n\n1.2.2 Definition\nA new mathematical notion is given (defined) in terms of existing notions.\nExample: A rational number is any number that can be expressed as the quotient a/b of two integers, with the denominator b not equal to zero. Note that this definition requires to know what integers are and the notion of division (quotient).\n\n\n1.2.3 Theorem\nA statement which relates specific (perviously defined) mathematical properties. It usually requires proof using definitions and mathematical logic to show the statement is true.\n\n\n1.2.4 Corollary, Lemma\nAn immediate consequence of a theorem (Corollary) or a minor theorem (Lemma). Sometimes, it comes with a proof; sometimes, it is omitted because it is considered trivial.\n\nExample 1.1. As an example let’s define something new :\n\nDefinition 1.1. An umpf is a natural number, which is a multiple of 3. A gumpf is a natural number which is a multiple of 6. (These are made-up notions, just for demonstration)\n\n\nLemma 1.1. Every gumpf is an umpf.\n\n\nProof. If x is a gumpf, then x= 6 n, where n is a natural number. Hence x=3 \\cdot 2 \\cdot n = 3 \\cdot (2\\cdot n). Therefore, x is an umpf. ◻\n\nThe end of a proof is often indicated by a square or the abbreviation q.e.d. (Latin: quod erat demonstrandum) “that which was to be shown”.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Before we start ...</span>"
    ]
  },
  {
    "objectID": "notation.html#mathematical-notation",
    "href": "notation.html#mathematical-notation",
    "title": "1  Before we start …",
    "section": "1.3 Mathematical notation",
    "text": "1.3 Mathematical notation\n\nSymbols\n\\begin{aligned}\n\\mathbb{N}: \\quad &  \\text{natural numbers, e.g.} 1,2,...\\\\\n\\mathbb{Z}: \\quad &  \\text{integers: } .., -2,-1,0,1,2,...\\\\\n\\mathbb{Q}: \\quad &  \\text{rational numbers: } 0, 1/3, -5/11,...\\\\\n\\mathbb{R}: \\quad &  \\text{real numbers: } 0, \\pi, \\sqrt{2}, 2/3, -2, ...\\\\\n\\mathbb{C}: \\quad &  \\text{complex numbers: } 0, i, \\pi +i, \\sqrt{2}-4i , -2/3,  ...\\\\\n\\Rightarrow \\quad &  \\text{logical implication, } A \\Rightarrow B: \\ \\text{if A then B} \\\\\n\\Leftrightarrow \\quad &  \\text{logical equivalence, }  A \\Leftrightarrow B: \\ \\text{if A then B and if B then A} \\\\\n\\vee \\quad & \\text{logical {\\it or}, }  A \\vee B: \\ \\text{either A or B or both} \\\\\n\\land \\quad & \\text{logical {\\it and},  }  A \\land B: \\ \\text{A and B} \\\\\n\\in \\quad &  \\text{\\it is an element of,} \\ x \\in \\mathbb{R} : \\ \\text{x is an element of } \\mathbb{R} \\\\\n\\forall \\quad &  \\text{{\\it for all}, e.g.: }  x^{n} \\in \\mathbb{R} \\  \\forall \\ n: \\  \\  \\ x^{n}\\text{ is real for all n} \\\\\n\\exists \\quad &  \\text{{\\it there  exist(s)}, e.g.: }  \\forall x\\in \\mathbb{Q} \\ \\exists \\ p,q \\in \\mathbb{Z} \\text{ with } x=p/q\n\\end{aligned}\nSee also the list of Greek letters in the Appendix Section 6.1.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Before we start ...</span>"
    ]
  },
  {
    "objectID": "notation.html#mathematical-proofs",
    "href": "notation.html#mathematical-proofs",
    "title": "1  Before we start …",
    "section": "1.4 Mathematical proofs",
    "text": "1.4 Mathematical proofs\nA sequence of logical arguments which shows that a statement is true in all cases, without a single exception. Types of proofs are:\n\ndirect proof.\nExample: The product of two even numbers is even. Proof: Let a,b two even numbers, that is a=2 n and b= 2 m, where m,n \\in \\mathbb{Z}. \\Rightarrow ab= 2n 2m = 2 (2nm) is an even number.\nproof by induction.\nThis type of proof is often used to prove an infinite series of statements A_n, n=1,2,3 \\ldots. If we can prove that a) A_1 is true and b) if A_n is true then A_{n+1} is true, then A_n is true for all n \\in \\mathbb{N}.\nExample: \\sum_{n=1}^N n = \\frac{1}{2} N (N+1)\n\nProof. \n\nThe statement is true for n=1: 1=\\frac{2}{2}. b) If the formula is true for N=k then it is also true for N=k+1: \\sum_{n=1}^N n = \\frac{1}{2} N (N+1) \\Rightarrow \\sum_{n=1}^{N+1} n = \\frac{1}{2} N (N+1) + (N+1) = (N+2)(N+1)/2. ◻\n\n\nproof by contradiction. The type of proof uses \\neg (\\neg A) = A; that is, it is shown that the opposite statement \\neg A is false (or \\neg (\\neg A) is true) and hence A is true.\nA famous example is the proof that \\sqrt{2} is not a rational number: A: “\\sqrt{2} is not a rational number”. Let us assume \\neg A is true, that is, \\sqrt{2} is a rational number. Then \\sqrt{2} = p/q with p,q \\in \\mathbb{Z} with no common divisior. \\Rightarrow  2q^2=p^2 \\Rightarrow p^2 is even and therefore p is even. Then p=2n \\Rightarrow p^2=2^2 n^2 and hence q^2= p^2/2=2 n^2 is even. Therefore, q must be even in contradiction to the assumption that p,q had no common divisor.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Before we start ...</span>"
    ]
  },
  {
    "objectID": "notation.html#footnotes",
    "href": "notation.html#footnotes",
    "title": "1  Before we start …",
    "section": "",
    "text": "From Wikipedia, the free encyclopaedia↩︎",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Before we start ...</span>"
    ]
  },
  {
    "objectID": "algebra3b.html",
    "href": "algebra3b.html",
    "title": "2  Vector spaces",
    "section": "",
    "text": "2.1 Motivation\nYou have used vectors in previous modules to do elementary geometry. These vectors were introduced as lines connecting two points with a direction: {\\vec a} = \\overrightarrow{PQ}, where P is the starting point of the vector and Q is the endpoint. The space of all these vectors is called the Euclidean space. It is usually introduced as a two- or three-dimensional space, but it is possible to generalise this to more dimensions. The Euclidean space was introduced as a simple model for the physical space around us. One can furthermore introduce orthogonal coordinates for the points in this space, e.g. P =  (P_x,P_y,P_z) = (3,2,1). and this then allows us to define the components of a vector: {\\vec a} = \\overrightarrow{PQ} =  \\begin{pmatrix}    Q_x- P_x \\\\ Q_y- P_y \\\\ Q_z -P_z  \\end{pmatrix}  The identification between points and vectors in an Euclidean space becomes particularly simple if we can choose P to be the origin of the coordinate system, P=(0,0,0), {\\vec a} = \\overrightarrow{OQ} =   \\begin{pmatrix}    Q_x \\\\ Q_y \\\\ Q_z  \\end{pmatrix} . Note that while the coordinates of points are usually denoted by a row of numbers (tuplet), the components of vectors are written as a column. An alternative way to write a vector is to write it as a sum of multiples of unit vectors, {\\vec v} =   v_x {\\bf e}_x + v_y {\\bf e}_y + v_z {\\bf e}_z  =  \\begin{pmatrix}    v_x \\\\ v_y \\\\ v_z  \\end{pmatrix}. An addition as well as a scalar multiplication was defined for these vectors. Later, the scalar product between two vectors and the cross-product (vector product) were also defined.\nThe operations on vectors of this Euclidean space E, the addition and scalar multiplication, had certain properties, for instance\nIn the following, we will generalise this space and define an abstract notion of a vector space, which has a much wider range of applications. To understand the crucial properties of such a definition, consider two examples.\nThe two examples above illustrate that numerous diverse examples exist where we find a structure similar to the Euclidean space. The elements in these spaces often don’t look like “vectors\"; they can be polynomials, velocities, matrices, etc.~, but addition and scalar multiplication work in the same way, so they follow the same rules as vectors. This is the motivation to define an abstract notion of a vector space and define it based on how its elements behave under addition and scalar multiplication, rather than what they represent.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector spaces</span>"
    ]
  },
  {
    "objectID": "algebra3b.html#motivation",
    "href": "algebra3b.html#motivation",
    "title": "2  Vector spaces",
    "section": "",
    "text": "\\vec{u} + \\vec{v}  \\in  E, closure condition for addition\n\\vec{u}+\\vec{v}  =  \\vec{v}+\\vec{u}, the addition is commutative\n\\lambda (\\vec{u}+ \\vec{v})  =  \\lambda \\vec{u} + \\lambda \\vec{v}, distributive law\n\\ldots\n\n\n\nExample 2.1 The captain of a sailing boat can calculate the velocity of his ship (speed and direction over ground) as the sum of two velocities: the velocity of his boat relative to the water (speed through water) and the velocity of the water (drift due to tides or ocean currents). The sum of the two velocities is found in the same way as above, as the addition of two vectors; however, the coordinate system used is usually a polar one (the angle as determined by a compass and speed in knots) rather than a Cartesian one (x, y-components). The velocities obtained this way are part of a velocity space. The elements of this space, the velocities, follow the same rules of addition and scalar multiplication as the vectors in our Euclidean space. Remark: In this example, the velocity space is a tangent space to a manifold (the surface of the ocean), a concept explained in more detail in Differential Geometry.\n\n\nExample 2.2 The set of all polynomials with degree \\leq n is denoted by \\mathcal{P}_{n}. We can add polynomials in \\mathcal{P}_{n} to obtain a polynomial in \\mathcal{P}_{n} again. Polynomials p(x)= a_{0} + a_{1}x + ...+a_{n} x^{n} and q(x)= b_{0} + b_{1}x + ...+b_{n} x^{n}\\in \\mathcal{P}_{n} add up to p(x)+q(x)  =  (a_{0}+ b_{0}) + (a_{1} + b_{1}) x + ...+(a_{n}+ b_{n}) x^{n}   \\in \\mathcal{P}_{n} . Similarly, we can multiply polynomials by a scalar \\lambda p(x)  =   \\lambda a_{0} +  \\lambda a_{1}x + ...+ \\lambda a_{n} x^{n}  \\in \\mathcal{P}_{n} Note that we can identify a polynomial of degree \\leq n with the vector of coefficients p(x) \\sim  \\scriptstyle \\begin{pmatrix}    a_{0} \\\\ a_{1} \\\\ \\vdots \\\\ a_{n}   \\end{pmatrix} \\textstyle.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector spaces</span>"
    ]
  },
  {
    "objectID": "algebra3b.html#definition-of-a-vector-space",
    "href": "algebra3b.html#definition-of-a-vector-space",
    "title": "2  Vector spaces",
    "section": "2.2 Definition of a Vector Space",
    "text": "2.2 Definition of a Vector Space\n\nDefinition 2.1: Vector Space over RA vector space over \\mathbb{R} is a set V, the elements of which are called vectors \\vec{v} \\in V along with two operations, an addition \\vec{v} + \\vec{w} and a scalar multiplication a \\vec{v}, a \\in \\mathbb{R}, subject to the following axioms for all \\vec{u},\\vec{v}, \\vec{w}\\in V and a,b\\in \\mathbb{R} : \\vec{u} + \\vec{v} \\in  V \\tag{2.1} a \\vec{u}  \\in  V \\tag{2.2} \\vec{u}+\\vec{v} = \\vec{v}+\\vec{u} \\tag{2.3} (\\vec{u}+\\vec{v})+\\vec{w}  = \\vec{u}+(\\vec{v}+\\vec{w}) \\tag{2.4} \\text{there exists a vector } {\\vec 0}  \\in V \\text{such that } \\vec{u}+\\vec{0} = \\vec{0}+\\vec{u}=\\vec{u} \\tag{2.5}  \\text{given } \\vec{v} \\text{ there exists a vector } -\\vec{v}  \\in V  \\text{ such that }\\vec{v}+ (-\\vec{v})  = \\vec{0} \\tag{2.6} 1 \\vec{u}  = \\vec{u} \\tag{2.7} a (\\vec{u}+ \\vec{v})  = a\\vec{u} +a \\vec{v} \\tag{2.8} (a+b) \\vec{u} = a\\vec{u} +b \\vec{u} \\tag{2.9} a (b \\vec{u}) =(a b) \\vec{u} \\tag{2.10}\n\n\nAxiom 2.1 and 2.2 ensure that both operations do not lead to elements not in V, 2.3 to 2.6 ensure that the addition is commutative and associative and that there is a neutral element \\vec{0} and an inverse element with respect to the addition. Axiom 2.7 states that the neutral element of the multiplication in \\mathbb{R} is also the neutral element for the scalar multiplication, while the remaining conditions are distributive laws.\n\n\n\n\n\n\nNoteRemark on Notation\n\n\n\nAlternative notations for vectors encountered in the literature are bold face symbols {\\bf v} or \\bar v (not to be confused with complex conjugation) or \\underline v. The zero vector \\vec{0} is often written just as 0. Note that there are two kinds of additions in this definition, both denoted by the same symbol +. The addition of two real numbers (a+b) and the addition of two vectors \\vec{v}+\\vec{w}. Also, for the multiplication, we have ab \\in \\mathbb{R} as well as a \\vec{v} \\in V. No symbol is used for multiplication, since the dot as well as the cross will be later used to define two different types of multiplication between two vectors.\nIf we want to avoid writing a vector as a column, as this always takes up a lot of space, we can use the transpose (denoted by a superscript T):  (1,2)^T = \\begin{pmatrix}  1 \\\\ 2 \\end{pmatrix} .\n\n\n\nExample 2.3 The set \\mathbb{R}^2 = \\{(x_{1},x_{2})^T \\vert x_{1} \\in \\mathbb{R}, x_{2} \\in \\mathbb{R}\\} is a vector space, the elements of which are written as columns \\vec{x}=\\scriptstyle \\begin{pmatrix}   x_{1}\\\\x_{2}  \\end{pmatrix} \\textstyle, if addition and multiplication are defined as  \\begin{pmatrix}   x_1 \\\\ x_2  \\end{pmatrix}\n  +\n   \\begin{pmatrix}   y_1 \\\\ y_2  \\end{pmatrix}\n  =\n   \\begin{pmatrix}   x_1+y_1 \\\\ x_2+y_2  \\end{pmatrix} ;\n  \\qquad\n  a\n   \\begin{pmatrix}   x_1 \\\\ x_2  \\end{pmatrix}\n  =\n   \\begin{pmatrix}   a x_1 \\\\  a x_2  \\end{pmatrix}  \\ . We have to check that all the axioms 2.1 to 2.10 are satisfied.\n\naxioms 2.1 and 2.2 are satisfied because the elements are again element in V.\naxioms 2.3 to 2.4 are satisfied because the addition of real numbers in each entry of the vector is associative and commutative,\n2.5 and 2.6 are correct if we use the following zero vector and negative element: \\vec{0}=\\scriptstyle \\begin{pmatrix}   0\\\\0  \\end{pmatrix} \\textstyle \\qquad   -\\vec{x} = \\scriptstyle \\begin{pmatrix}   -x_{1}\\\\-x_{2}  \\end{pmatrix} \\textstyle \\ .\naxiom 2.7 is obvious from the definition of the scalar multiplication,\naxiom 2.8 (r+s) \\begin{pmatrix}   v_1 \\\\ v_2  \\end{pmatrix}\n  =\\begin{pmatrix}   (r+s)v_1 \\\\ (r+s)v_2  \\end{pmatrix}\n  = \\begin{pmatrix}   rv_1+sv_1 \\\\ rv_2+sv_2  \\end{pmatrix}\n  =r \\begin{pmatrix}   v_1 \\\\ v_2  \\end{pmatrix} +s \\begin{pmatrix}   v_1 \\\\ v_2  \\end{pmatrix}; \naxiom 2.9 r \\left( \\begin{pmatrix}   v_1 \\\\ v_2  \\end{pmatrix} + \\begin{pmatrix}   w_1 \\\\ w_2  \\end{pmatrix} \\right)\n  =\\begin{pmatrix}   r(v_1+w_1) \\\\ r(v_2+w_2)  \\end{pmatrix}\n  =\\begin{pmatrix}   rv_1+rw_1 \\\\ rv_2+rw_2  \\end{pmatrix}\n  =r \\begin{pmatrix}   v_1 \\\\ v_2  \\end{pmatrix} +r \\begin{pmatrix}   w_1 \\\\ w_2  \\end{pmatrix}; \naxiom 2.10 (rs) \\begin{pmatrix}   v_1 \\\\ v_2  \\end{pmatrix}\n  = \\begin{pmatrix}   (rs)v_1 \\\\ (rs)v_2  \\end{pmatrix}\n  =\\begin{pmatrix}   r(sv_1) \\\\ r(sv_2)  \\end{pmatrix}\n  =r \\left(s \\begin{pmatrix}   v_1 \\\\ v_2  \\end{pmatrix} \\right).\n\n\n\nExample 2.4 The set V= \\left\\{ \\left.  \\begin{pmatrix}   v_{1}\\\\ v_{2}  \\end{pmatrix}  \\in \\mathbb{R}^2 \\right|  v_1^2+v_2^2 \\le 1\\right\\} with the addition and scalar multiplication as in the previous example is not a vector space. The first two axioms (closure conditions) are not satisfied. For example {\\bf u}=  \\begin{pmatrix}   1/2\\\\ 1/2  \\end{pmatrix}  \\in V, \\ \\text{since} \\    \\left(\\frac{1}{2}\\right)^2+\\left(\\frac{1}{2}\\right)^2 = \\frac{1}{2} \\le 1, \\  \\text{but} \\quad  2{\\bf u}= 2  \\begin{pmatrix}   1/2\\\\ 1/2  \\end{pmatrix}  = \\begin{pmatrix}   1\\\\ 1  \\end{pmatrix}   \\notin V.\n\n\nExample 2.5 The set V= \\left\\{ \\left.  \\begin{pmatrix}   v_{1}\\\\ v_{2}  \\end{pmatrix}  \\right| v_1 \\in   \\mathbb{R}, v_2 \\in  \\mathbb{R}\\right\\} with the same scalar multiplication as before a   \\begin{pmatrix}   x_1 \\\\ x_2  \\end{pmatrix}  =   \\begin{pmatrix}   a x_1 \\\\  a x_2  \\end{pmatrix}  \\ , but the addition  \\begin{pmatrix}   x_1 \\\\ x_2  \\end{pmatrix}\n  +\n  \\begin{pmatrix}   y_1 \\\\ y_2  \\end{pmatrix} \\\n  =\\begin{pmatrix}   x_1+y_1+1 \\\\ x_2+y_2+1  \\end{pmatrix} is not a vector space. Axioms 8 and 9 are not satisfied.\n\n\nExample 2.6 What we have shown for a vector space consisting of two vectors with two real components can be easily extended to n components. The vector space I\\!\\!R^{n} = \\left\\{ \\left. \\scriptscriptstyle \\begin{pmatrix}   v_{1}\\\\ v_{2} \\\\ \\vdots \\\\ v_{n}  \\end{pmatrix} \\textstyle \\right| v_{i} \\in \\mathbb{R}, i=1,2, .. ,n \\right\\} together with the operation of addition \\scriptscriptstyle \\begin{pmatrix}   v_{1}\\\\ v_{2} \\\\ \\vdots \\\\ v_{n}  \\end{pmatrix} \\textstyle +  \\scriptscriptstyle \\begin{pmatrix}   w_{1}\\\\ w_{2} \\\\ \\vdots \\\\ w_{n}  \\end{pmatrix} \\textstyle =  \\scriptscriptstyle \\begin{pmatrix}   v_{1}+w_{1}\\\\ v_{2} +w_{2}\\\\ \\vdots \\\\ v_{n}+w_{n}  \\end{pmatrix} \\textstyle and the scalar multiplication  \\alpha \\scriptscriptstyle \\begin{pmatrix}   v_{1}\\\\ v_{2} \\\\ \\vdots \\\\ v_{n}  \\end{pmatrix} \\textstyle =  \\scriptscriptstyle \\begin{pmatrix}    \\alpha v_{1}\\\\  \\alpha v_{2} \\\\ \\vdots \\\\  \\alpha v_{n}  \\end{pmatrix} \\textstyle \\quad \\alpha \\in \\mathbb{R} is called the vector space \\mathbb{R}^{n}.\nAlthough we haven’t defined yet what a basis is, we mention here for future reference that the vectors, \\vec{e}_{1} =  \\scriptscriptstyle \\begin{pmatrix}   1\\\\ 0 \\\\ \\vdots \\\\ 0   \\end{pmatrix} \\textstyle, \\quad  \\vec{e}_{2} =  \\scriptscriptstyle \\begin{pmatrix}   0\\\\ 1 \\\\ 0 \\\\ \\vdots  \\end{pmatrix} \\textstyle, ...    ,\\vec{e}_{n} =  \\scriptscriptstyle \\begin{pmatrix}   0\\\\ 0 \\\\ \\vdots \\\\ 1   \\end{pmatrix} \\textstyle form a basis of \\mathbb{R}^{n}, the so called the standard basis. That is, every vector in \\mathbb{R}^{n} can be expressed as a linear combination of the basis vectors, \\vec{r} = r_{1} \\vec{e}_{1} + r_{2} \\vec{e}_{2} + ... + r_{n} \\vec{e}_{n} , Or, in other words, the vector space is spanned by the basis vectors.\n\n\nExample 2.7 The set of all m\\times n matrices where the entries are real numbers forms a vector space. We denote this space by M^{(m \\times n)}. Addition and scalar multiplication are the usual addition and scalar multiplication of matrices. A basis of the space consists of the matrices {\\rm E}^{(i,j)} which have a 1 at entry (i,j) and zeros everywhere else. The space is of dimension m n.\n\n\nExample 2.8 Let \\mathcal{P}_{n} denote the set of all polynomials with degree \\leq n. \\mathcal{P}_{n} is a vector space if we declare an addition and scalar multiplication of polynomials p(x)= a_{0} + a_{1}x + ...+a_{n} x^{n} and q(x)= b_{0} + b_{1}x + ...+b_{n} x^{n}\\in \\mathcal{P}_{n} by \\begin{aligned}\np(x)+q(x) &= (a_{0}+ b_{0}) + (a_{1} + b_{1}) x + ...+(a_{n}+ b_{n}) x^{n}   \\in \\mathcal{P}_{n} ,  \\\\\n\\lambda p(x) &=  \\lambda a_{0} +  \\lambda a_{1}x + ...+ \\lambda a_{n} x^{n}  \\in \\mathcal{P}_{n} .\n\\end{aligned} Note that we can identify a polynomial of degree \\leq n with the vector of coefficients p(x) \\sim  (a_{0}, a_{1},.., a_{n})^{T} . Hence, a basis for the space consists of the monomials x^{r}, r=0,1, ..,n and is n+1 dimensional.\n\n\nExample 2.9 The solutions of a homogeneous linear differential equation of order n, e.g. \\frac{\\mathrm{d}^{2}y}{\\mathrm{d}x^{2}}-3\\frac{\\mathrm{d}y}{\\mathrm{d}x}\n+2y=0.\\qquad form a vector space. Addition and scalar multiplication in this vector space are the usual addition and scalar multiplication of functions. The space is spanned by n linearly independent solutions. This is an example of a so-called function space.\n\n\nExample 2.10 The set \\mathcal{F} of all infinitely differentiable functions, \\mathbb{R} \\rightarrow \\mathbb{R}, i.e., those that have derivatives of all orders. Note that (among others) the following functions belong to \\mathcal{F}: e^{nx},\\,\\cos \\left( 2\\pi nx\\right)\n,\\,\\,x^{n}\\quad \\,(\\text{for all }n=0,1,2,...). \\mathcal{F} is an example of an infinite-dimensional space because it contains an unlimited number of linearly independent elements.\n\nThe definition of a vector space over \\mathbb{R} can be extended to a vector space over a more general set of numbers, a field K. To understand what a ‘field’ is, we need to take a short detour into number systems.\n\nDefinition 2.2: Vector Space over KA vector space over a field K is a set V, the elements of which are called vectors \\vec{v} \\in V along with two operations, an addition \\vec{v} + \\vec{w} and a scalar multiplication a \\vec{v}, a \\in K, subject to the same 10 axioms Equation 2.1 to Equation 2.10 as before only that now a,b\\in K.\n\n\n\nExample 2.11 The set 2.2 \\mathbb{C}^{n} =\\{ (c_{1},...,c_{n})^{T}| c_{1},..,c_{n}\\in \\mathbb{C}\\} is a vector space over \\mathbb{C}. Addition and scalar multiplication are the usual addition and scalar multiplication of complex numbers. A basis for this space is given by the n vectors e_{1}=(1,0,...,0),..., e_{n}=(0,...,0,1). The space has n dimensions. Note that we can identify \\mathbb{C} with \\mathbb{R}^{2} due to z= a + b i with a,b \\in \\mathbb{R}. Hence \\mathbb{C}^{n}  \\sim \\mathbb{R}^{2n} is also a vector space over \\mathbb{R}, but in this case it is 2n-dimensional with two basis vectors for the real and imaginary part of each complex dimension.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector spaces</span>"
    ]
  },
  {
    "objectID": "algebra3b.html#linear-independence",
    "href": "algebra3b.html#linear-independence",
    "title": "2  Vector spaces",
    "section": "2.3 Linear independence",
    "text": "2.3 Linear independence\nIn the same way as a coordinate system is introduced on a plane to allocate a unique pair of coordinates to any point in the plane, we would like to have a coordinate system for a vector space, which uniquely assigns to any vector of this space a coordinate tuple (set). To generate such a coordinate system, we need a set of ‘basis’ vectors which are, in a sense, independent. Consider the following example in the x-y plane.\nGiven the two unit vectors in \\mathbb{R}^2, \\vec{e}_1 = \\scriptstyle \\begin{pmatrix}   1 \\\\ 0  \\end{pmatrix} \\textstyle, \\ \\vec{e}_2= \\scriptstyle \\begin{pmatrix}   0 \\\\ 1  \\end{pmatrix} \\textstyle Any point in the plane, that is, any vector in \\mathbb{R}^2, can be represented by a linear combination \\vec{v } = v_1  \\vec{e}_1  + v_2  \\vec{e}_2,  v_1,v_2 \\in \\mathbb{R} where (v_1,v_2) play the role of coordinates. These coordinates are unique; that is, there are no two different vectors with the same coordinate pair, nor are two different coordinate pairs assigned to the same vector.\nThe same is true if we replace our basis vectors by (check!) \\vec{e}_1 = \\scriptstyle \\begin{pmatrix}   1 \\\\ 0  \\end{pmatrix} \\textstyle, \\ \\vec{e}_2= \\scriptstyle \\begin{pmatrix}   1 \\\\ 1  \\end{pmatrix} \\textstyle .\nHowever, if we use a triplet of vectors: \\vec{e}_1 = \\scriptstyle \\begin{pmatrix}   1 \\\\ 0  \\end{pmatrix} \\textstyle, \\ \\vec{e}_2= \\scriptstyle \\begin{pmatrix}   0 \\\\ 1  \\end{pmatrix} \\textstyle,  \\ \\vec{e}_3= \\scriptstyle \\begin{pmatrix}   1 \\\\ 1  \\end{pmatrix} \\textstyle , \\vec{v } = v_1  \\vec{e}_1  + v_2  \\vec{e}_2 + v_3  \\vec{e}_3 ,  v_1,v_2,v_3 \\in \\mathbb{R}\nThen there are vectors with non-unique coordinates: \\vec{v}   = 2  \\vec{e}_1 + 3  \\vec{e}_2 -1 \\vec{e}_3  =   1  \\vec{e}_1 + 2  \\vec{e}_2 + 0 \\vec{e}_3\nMore generally, if such a non-unique case occurs, one can write \\begin{aligned}\n{\\bf v} & =   v_1  \\vec{e}_1 +  v_2 \\vec{e}_2 + v_3 \\vec{e}_3 \\\\\n  {\\bf v} & =   v_1'  \\vec{e}_1 + v_2'  \\vec{e}_2 + v_3' \\vec{e}_3 \\\\\n  \\Rightarrow \\vec{0} & =   (v_1-v_1')  \\vec{e}_1 + (v_2-v_2')  \\vec{e}_2 + (v_3-v_3') \\vec{e}_3, \\\\\n   \\Rightarrow   \\vec{0} & =   a_1  \\vec{e}_1 + a_2  \\vec{e}_2 + a_3 \\vec{e}_3, \\quad (a_1,a_2,a_3) \\neq (0,0,0)\n  \\end{aligned} where at least one of the three brackets is non-zero since we assumed that (v_1,v_2,v_3) \\neq (v_1',v_2',v_3'). This is called a non-trivial linear combination.\n\nDefinition 2.3: Linear CombinationAn expression of the form a_{1}{\\vec v}\n_{1}+a_{2}{\\vec v}_{2}+\\cdots +a_{n}{\\vec v}_{n} is known as a linear combination of the n vectors {\\vec v}\n_{1},{\\vec v}_{2},\\cdots ,{\\vec v}_{n}. The numbers a_{1}, a_{2},..., a_{n} are known as the coefficients of the linear combination.\n\n\n\nDefinition 2.4: Linear IndependenceA set of vectors \\vec{v_{1}}, \\vec{v_{2}},..., \\vec{v_{n}} is called linearly independent if none of its elements is a linear combination of the others. That is the equation a_{1}\\vec{v_{1}} + a_{2} \\vec{v_{2}}+  ... + a_{n}\\vec{v_{n}} = 0 has no solution (a_{1},a_{2},..,a_{n}) other than the trivial solution (0,0,...,0). Otherwise, the set is called linearly dependent.\n\n\nI.e., a set of vectors is linearly dependent if we can find a non-trivial linear combination that yields the zero vector. Note that a simple relation between just two of the vectors, e.g. \\mathbf{v}_{1}=3\\mathbf{v}_{2}, is enough to make the complete set \\mathit{\n\\{}\\mathbf{v}_{1},\\mathbf{v}_{2},...,\\mathbf{v}_{n}\\mathit{\\}} linearly dependent.\n\nExample 2.12 Are the vectors (1,-1,0)^{T},(2,1,1)^{T} and (-1,2,1)^{T} linearly independent?\nWe solve a_{1} \\begin{pmatrix}   \n1 \\\\\n-1 \\\\\n0  \\end{pmatrix} +a_{2}  \\begin{pmatrix}   \n2 \\\\\n1 \\\\\n1  \\end{pmatrix} +a_{3}  \\begin{pmatrix}   \n-1 \\\\\n2 \\\\\n1  \\end{pmatrix} = \\begin{pmatrix}   \n0 \\\\\n0 \\\\\n0  \\end{pmatrix}. for (a_{1}, a_{2}, a_{3}). We find that a_{1}=a_{2}=a_{3}=0; so the 3 given vectors are linearly independent. If we replace the first entry in the first vector by 3, the set becomes linearly dependent. A solution is a_1=1, a_2=-1, a_3=1.\n\n\nExample 2.13 Are the vectors (1,0,0)^{T}, (1,1,0)^{T} and (1,0,1)^{T} linearly independent?\nWe solve a_{1} \\scriptstyle \\begin{pmatrix}   1 \\\\ 0 \\\\ 0  \\end{pmatrix} \\textstyle + a_{2} \\scriptstyle \\begin{pmatrix}   1 \\\\ 1 \\\\ 0  \\end{pmatrix} \\textstyle + a_{3} \\scriptstyle \\begin{pmatrix}   1 \\\\ 0 \\\\ 1  \\end{pmatrix} \\textstyle = \\scriptstyle \\begin{pmatrix}   0\\\\0\\\\0  \\end{pmatrix} \\textstyle, for (a_{1},a_{2},a_{3}). The third component of this equation implies a_{3}=0. From the second and first component follow a_{2}=0 and a_{1} =0. Hence, the vectors are linearly independent.\nOn the other hand, the vectors \\scriptstyle \\begin{pmatrix}   1 \\\\ 0 \\\\ 0  \\end{pmatrix} \\textstyle, \\quad \\scriptstyle \\begin{pmatrix}   1 \\\\ 1 \\\\ 0  \\end{pmatrix} \\textstyle,  \\quad \\scriptstyle \\begin{pmatrix}   0 \\\\ 1 \\\\ 0  \\end{pmatrix} \\textstyle, are linearly dependent, since the third component reads a_{3} 0 = 0 which is satisfied for any a_{3}=\\lambda. The second component requires a_{2} = - a_{3} and the first a_{1} = a_{3}. So there are non-trivial solutions (\\lambda , -\\lambda , \\lambda ), e.g. (1,-1,1).\n\n\nExample 2.14 A set of two vectors is linearly independent if they are not parallel: a_{1} \\vec{v}_{1} \\neq a_{2} \\vec{v_{2}} for a_{1}, a_{2} non-zero. A set of three vectors in \\mathbb{R}^{2} is always linearly dependent. A set of three vectors in \\mathbb{R}^{3} is linearly independent if they do not all lie in the same plane.\n\n\nExample 2.15 Show that the vectors (1,0,2,1)^{T},(0,1,-1,2)^{T},(2,1,3,4)^{T} are linearly dependent in \\mathbb{R}^{4}.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector spaces</span>"
    ]
  },
  {
    "objectID": "algebra3b.html#span-and-basis",
    "href": "algebra3b.html#span-and-basis",
    "title": "2  Vector spaces",
    "section": "2.4 Span and Basis",
    "text": "2.4 Span and Basis\n\nDefinition 2.5: SpanThe set of vectors \\{{\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{n}\\} in V span V if every vector \\vec{v}\\in V is a linear combination of \\{{\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{n}\\}. The set of all vectors of the form {\\vec v=}a_{1}{\\vec v}_{1}+a_{2}{\\vec v}_{2}+\\cdots +a_{n}{\\vec v}_{n} is called the span of \\{{\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{n}\\} , and denoted by span({\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{n}).\n\n\n\nExample 2.16 \\begin{aligned}\n{\\rm span}\\left(\\scriptscriptstyle \\begin{pmatrix}   1\\\\0  \\end{pmatrix} \\textstyle,\\scriptscriptstyle \\begin{pmatrix}   2\\\\0  \\end{pmatrix} \\textstyle\\right) & = \\left\\{ \\left. {\\bf v}  =a_1 \\scriptscriptstyle \\begin{pmatrix}   1\\\\0  \\end{pmatrix} \\textstyle + a_2 \\scriptscriptstyle \\begin{pmatrix}   2\\\\0  \\end{pmatrix} \\textstyle\\right| a_1,a_2 \\in \\mathbb{R} \\right\\} \\\\\n&=  \\left\\{ \\left.{\\bf v} =  \\scriptscriptstyle \\begin{pmatrix}   a_1+2a_2 \\\\ 0  \\end{pmatrix} \\textstyle \\right| a_1,a_2 \\in \\mathbb{R} \\right\\}  =  \\mathbb{R}^{1},\n\\end{aligned} spans the whole of \\mathbb{R}. This space contains only the vectors with 0 in the second component.\n\n\nExample 2.17 The span of two linearly independent vectors in \\mathbb{R}^{3} is a plane. The span of three linearly independent vectors in \\mathbb{R}^{3} is the whole of \\mathbb{R}^{3}.\n\n\nExample 2.18 The set \\{(1,0)^{T},(0,1)^{T}\\} spans \\mathbb{R}^{2}. \\begin{aligned}\n{\\rm span}\\left(\\scriptscriptstyle \\begin{pmatrix}   1\\\\0  \\end{pmatrix} \\textstyle,\\scriptscriptstyle \\begin{pmatrix}   0\\\\1  \\end{pmatrix} \\textstyle\\right) & = \\left\\{ \\left. \\vec{v} = a_1 \\scriptscriptstyle \\begin{pmatrix}   1\\\\0  \\end{pmatrix} \\textstyle + a_2 \\scriptscriptstyle \\begin{pmatrix}   0\\\\1  \\end{pmatrix} \\textstyle\\right| a_1,a_2 \\in \\mathbb{R} \\right\\} \\\\\n& = \\left\\{ \\left. \\vec{v} = \\scriptscriptstyle \\begin{pmatrix}   a_1\\\\a_2  \\end{pmatrix} \\textstyle \\right| a_1,a_2 \\in \\mathbb{R} \\right\\} =    \\mathbb{R}^{2}.\n\\end{aligned} Thus, any vector (a_{1},a_{2})^{T}\\in\\mathbb{R}^{2} can be expressed as a linear combination of (1,0)^{T} and (0,1)^{T}.\n\nIf we add another vector, e.g. {\\rm span}\\left(\\scriptscriptstyle \\begin{pmatrix}   1\\\\0  \\end{pmatrix} \\textstyle,\\scriptscriptstyle \\begin{pmatrix}   0\\\\1  \\end{pmatrix} \\textstyle, \\scriptscriptstyle \\begin{pmatrix}   1\\\\1  \\end{pmatrix} \\textstyle \\right) =\\mathbb{R}^{2}, then the space may or may not become any bigger. In this case, the space remains the same since the third vector is a linear combination of the first two vectors, that is, the set is linearly dependent. The smallest linearly independent set which spans a given vector space is called a basis.\n\nDefinition 2.6: BasisA basis of a vector space is a linearly independent set of vectors which span the vector space.\n\n\n\nExample 2.19 The set of vectors \\left\\{(1,0,0)^{T}, (0,1,0)^{T}, (0,0,1)^{T}\\right\\} forms a basis of \\mathbb{R}^{3} (more generally K^{3} for a field K), the set of vectors \\left\\{(1,0,0,0)^{T}, (0,1,0,0)^{T},\\right\\}\n\\left (0,0,1,0)^{T},(0,0,0,1)^{T}\\right\\} forms a basis of \\mathbb{R}^{4} (more generally K^{4}) and so on. This is called the standard basis of \\mathbb{R}^{n} (K^{n} in more general sense) for n\\in\\mathbb{N}.\n\n\nExample 2.20 The vectors \\vec{f}_{1} =  \\scriptscriptstyle \\begin{pmatrix}   0\\\\ 1 \\\\ 1   \\end{pmatrix} \\textstyle, \\quad \\vec{f}_{2} =  \\scriptscriptstyle \\begin{pmatrix}   0\\\\ 1 \\\\ 0   \\end{pmatrix} \\textstyle, \\quad \\vec{f}_{3} =  \\scriptscriptstyle \\begin{pmatrix}   0\\\\ 0 \\\\ 1  \\end{pmatrix} \\textstyle, do not form a basis for \\mathbb{R}^{3} since they do not span the whole \\mathbb{R}^{3}. The vector \\vec{e}_{1}, for instance, has no representation in this set.\n\n\nExample 2.21 The vectors \\vec{f}_{1} =  \\scriptscriptstyle \\begin{pmatrix}   1\\\\ 0 \\\\ 0   \\end{pmatrix} \\textstyle, \\quad \\vec{f}_{2} =  \\scriptscriptstyle \\begin{pmatrix}   0\\\\ 1 \\\\ 0   \\end{pmatrix} \\textstyle, \\quad \\vec{f}_{3} =  \\scriptscriptstyle \\begin{pmatrix}   0\\\\ 0 \\\\ 1  \\end{pmatrix} \\textstyle, \\quad  \\vec{f}_{4} =  \\scriptscriptstyle \\begin{pmatrix}   1\\\\ 1 \\\\ 1   \\end{pmatrix} \\textstyle , are also not a basis. Although they span the whole \\mathbb{R}^{3}, they are not linearly independent.\n\n\nExample 2.22 Consider the set of all n\\times n matrices with real entries \\mathbb{R}^{(n\\times n)}. Let E_{ij} be the n\\times n matrix with a ‘1’ in position (i,j) and ‘0’ elsewhere. Then the collection of all such matrices (for all i,j=1,2,\\ldots,n) is a basis for \\mathbb{R}^{(n\\times n)}.\nConsider the case n=2, then the basis matrices are \\left(\\begin{array}{cc}\na & b\\\\\nc & d\n\\end{array}\\right)=aE_{11}+bE_{12}+cE_{21}+dE_{22},\\quad\\textrm{(spanning)}. Also, we can show that the four E matrices are linearly independent. \\alpha _{1}E_{11}+\\alpha _{2}E_{12}+\\alpha _{3}E_{21}+\\alpha _{4}E_{22}=\\mathbf{0} means that \\left(\\begin{array}{cc}\n\\alpha _{1} & \\alpha _{2}\\\\\n\\alpha _{3} & \\alpha _{4}\n\\end{array}\\right) = \\left(\\begin{array}{cc}\n0 & 0\\\\\n0 & 0\n\\end{array}\\right),\\quad\\textrm{i.e.   }\\alpha _{1}=\\alpha _{2}=\\alpha _{3}=\\alpha _{4}=0. Since the four matrices span the space and are linearly independent, they form a basis for \\mathbb{R}^{(n\\times n)}.\n\n\nTheorem 2.7: Uniqueness w.r.t. a basisLet \\left\\{ {\\vec v}_{1},{\\vec v}_{2},\\dots ,{\\vec v}\n_{n}\\right\\} be a basis for a vector space V. Each vector from V can be uniquely expressed as a linear combination of these vectors.\n\n\n\nProofIf there were two different representations of a vector {\\vec v} with respect to this basis: Eg. {\\vec v} =a_{1}{\\vec v}_{1} + \\dots + a_{n }{\\vec v}_{n} and {\\vec v} =b_{1}  {\\vec v}_{1} + \\dots + b_{n }{\\vec v}_{n}, then the difference {\\bf 0} =(a_{1} - b_{1}){\\vec v}_{1} + \\dots + (a_{n }-b_{n}){\\vec v}_{n} would be a non-trivial linear combination representing the zero vector, which is impossible since the vectors {\\vec v}_{1},\\dots ,{\\vec v}_{n} were linearly independent. ◻\n\n\n\nExample 2.23 In \\mathbb{R}^{3}, \\begin{pmatrix}   2 \\\\\n-3 \\\\\n4  \\end{pmatrix} =2 \\begin{pmatrix}   \n1 \\\\\n0 \\\\\n0  \\end{pmatrix} -3 \\begin{pmatrix}   \n0 \\\\\n1 \\\\\n0  \\end{pmatrix} +4 \\begin{pmatrix}   \n0 \\\\\n0 \\\\\n1  \\end{pmatrix} =2{\\vec e}_{1}-3{\\vec e}_{2}+4{\\vec e}_{3}.\nSo, any linear combination of the vectors {\\vec e}_{1},\n{\\vec e}_{2},{\\vec e}_{3} is a vector in \\mathbb{R}^{3} and any vector in \\mathbb{R}^{3} can be written as a linear combination of {\\vec e}_{1},{\\vec e}\n_{2},{\\vec e}_{3} and the expression is unique.\n\n\nExample 2.24 A different basis, other than the standard basis, for the \\mathbb{R}^{3} is, for instance \\vec{f}_{1} =  \\scriptscriptstyle \\begin{pmatrix}   1\\\\ 1 \\\\ 0   \\end{pmatrix} \\textstyle, \\quad \\vec{f}_{2} =  \\scriptscriptstyle \\begin{pmatrix}   0\\\\ 1 \\\\ 0   \\end{pmatrix} \\textstyle, \\quad \\vec{f}_{3} =  \\scriptscriptstyle \\begin{pmatrix}   0\\\\ 0 \\\\ 1  \\end{pmatrix} \\textstyle, We can check that we still can express any vector with respect to (w.r.t.) this basis. We express the arbitrary vector \\vec{r} first w.r.t. the standard basis and show that we can translate this into a representation w.r.t. the new basis: \\begin{aligned}\n\\vec{r} & = r_{1} \\vec{e}_{1} + r_{2} \\vec{e}_{2} +r_{3} \\vec{e}_{3}  \\\\\n  & = r_{1} (\\vec{e}_{1} + \\vec{e}_{2})  + (r_{2}-r_{1}) \\vec{e}_{2} +r_{3} \\vec{e}_{3}  \\\\\n  & = r_{1} \\vec{f}_{1} + (r_{2}-r_{1}) \\vec{f}_{2} +r_{3} \\vec{f}_{3}  .\n\\end{aligned} And this expression is unique. In addition, the definition of a basis requires that the new basis vectors are linearly independent, which they are; otherwise, they could not span the whole three-dimensional space.\n\nWe note that there are different bases, but they all have the same number of elements.\n\nTheorem 2.8: Number of elements of a basisEvery basis of a vector space contains the same number of vectors, this number being the largest set of linearly independent vectors in the set.\n\n\n\nProofSuppose that a vector space V has two bases, one of which is \\left\\{ {\\vec w}_{1},...,{\\vec w}_{p}\\right\\} that contains p vectors and the other \\left\\{ {\\vec z}_{1},...,{\\vec z}_{q}\\right\\} that contains q vectors. We can assume that p&gt;q. We shall show that the vectors \\left\\{ {\\vec w}_{1},...,{\\vec w}_{p}\\right\\} are linearly dependent, contradicting the fact that they form a basis.\nTo begin, since V={\\rm span}\\left( {\\vec z}_{1},...,{\\vec z}_{q}\\right) and each of the {\\vec w}’s lies in V, they must be linear combinations of the {\\vec z}’s. i.e., \\begin{aligned}\n{\\vec w}_{1} &=c_{11}{\\vec z}_{1}+c_{21}{\\vec z}_{2}+\\cdots +c_{q1}\n{\\vec z}_{q} \\\\\n&\\vdots \\\\\n{\\vec w}_{p} &=c_{1p}{\\vec z}_{1}+c_{2p}{\\vec z}_{2}+\\cdots +c_{qp}\n{\\vec z}_{q}.\n\\end{aligned} Now consider a linear combination of the {\\vec w}’s: \\begin{aligned}\n\\sum_{j=1}^{p}a_{j}{\\vec w}_{j}  = & a_{1}{\\vec w}_{1}+\\cdots +a_{p}\n{\\vec w}_{p} \\\\\n= & a_{1}\\left( c_{11}{\\vec z}_{1}+c_{21}{\\vec z}_{2}+\\cdots +c_{q1}\n{\\vec z}_{q}\\right) \\\\\n&+ a_{2} \\left( c_{12}{\\vec z}_{1}+c_{22}{\\vec z}_{2}+\\cdots +c_{q2}\n{\\vec z}_{q}\\right) \\\\\n& \\vdots \\\\\n& + a_{p}\\left( c_{1p}{\\vec z}_{1}+c_{2p}{\\vec z}_{2}+\\cdots +c_{qp}\n{\\vec z}_{q}\\right) \\\\\n= &\n\\left( c_{11}a_{1}+\\cdots +c_{1p}a_{p}\\right) {\\vec z}_{1}+\\cdots\n+\\left( c_{q1}a_{1}+\\cdots +c_{qp}a_{p}\\right) {\\vec z}_{q}.\n\\end{aligned} Can we choose the a’s so that the right-hand side is zero? Since the {\\vec z}’s are l.i., this will require that each of the coefficients be zero: \\begin{aligned}\nc_{11}a_{1}+\\cdots +c_{1p}a_{p} &=0 \\\\\n&\\vdots \\\\\nc_{q1}a_{1}+\\cdots +c_{qp}a_{p} &=0\n\\end{aligned} but this is a system of q equations in p unknowns with p&gt;q. Such a system always has non-zero solutions. Hence, there will be a choice if the coefficients a_{1},\\ldots ,a_{p} (not all of which are zero) so that a_{1}\n{\\vec w}_{1}+\\cdots +a_{p}{\\vec w}_{p}=0. But this means that the {\\vec w}’s are l.d.. We have a contradiction, since we assumed the {\\vec w}’s form a basis. A similar argument can be applied if we assume that q&gt;p. Thus, a contradiction can be avoided only when p=q. ◻\n\n\n\nDefinition 2.9: DimensionThe number of elements of a basis is called the dimension of the space.\n\n\n\nExample 2.25 \\mathbb{R}^{n} has dimension n.\n\n\nExample 2.26 The space of polynomials of degree \\leq n has basis \\{1, x, x^{2},\\ldots,x^{n}\\}, so its dimension is n+1 (not n).\n\n\n\n\n\n\n\nNoteRemark on dimension\n\n\n\nNote that the dimension of V depends on the field K. Thus the complex numbers \\mathbb{C} can be considered as a space of dimension 1 over \\mathbb{C}, or as a space of dimension 2 over \\mathbb{R}, where \\{1, i\\} is a basis for \\mathbb{C} over \\mathbb{R}.\n\n\n\nTheorem 2.10: Smaller spanning setAny linearly dependent vectors of a spanning set can be omitted without making the span smaller. {\\rm span}({\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{n}) = {\\rm span}( \\text{largest  l.i. subset of }  {\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{n})\n\n\n\nProofWithout loss of generality (w.l.o.g.) we can assume that \\{{\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{r} \\} is the largest linearly independent subset (r\\le n). Then any remaining vectors {\\vec v}_{r+1}, ... , {\\vec v}_{n} can be expressed as a linear combination of \\{{\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{r} \\} hence {\\rm span}({\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{r}) = {\\rm span}({\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{n}) ◻\n\n\n\nCorollary 2.11n linearly independent vectors in \\mathbb{R}^n span \\mathbb{R}^n.\nCorrespondingly a set of n+1 vectors in \\mathbb{R}^{n} is linearly dependent.\n\n\n\nProofWe know that the standard basis of \\mathbb{R}^{n} consists of n elements, hence the dimension is n. Suppose the n vectors would not span \\mathbb{R}^{n} then we could add vectors such that the new set spans the space and forms a basis. However, we would then have a basis with at least n+1 vectors. That can’t be as any basis of \\mathbb{R}^{n} has to have n vectors. So the assumption that the vectors don’t span of \\mathbb{R}^{n} is false and they do span the space. Similar there were n+1 vectors in \\mathbb{R}^{n} must be linearly dependent. If they were linearly independent they could form a basis with more than n elements.Again this can be true, hence they are linearly dependent. ◻",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector spaces</span>"
    ]
  },
  {
    "objectID": "algebra3b.html#subspaces",
    "href": "algebra3b.html#subspaces",
    "title": "2  Vector spaces",
    "section": "2.5 Subspaces",
    "text": "2.5 Subspaces\nSuppose that W is a nonempty subset of the vector space V, which also satisfies all the vector space axioms itself, then W is called a subspace of V. It is important to realise that for W to be subspace of V, then W must satisfy the following conditions:\n\nDefinition 2.12: SubspaceA subset W \\subseteq V of vectors in a vector space V is called a subspace of V if the following conditions hold:\n(i) {\\vec 0}\\in W,\n(ii) if {\\vec u} \\in W and {\\vec v} \\in W then {\\vec u}+{\\vec\nv\\in }W ;\n(iii) if {\\vec u\\in }W then a{\\vec u\\in }W for all a\\in K.\n\n\nAll other axioms are automatically satisfied because each element in W is already in V.\n\nExample 2.27 For any vector space V, V is always a subspace of itself.\n\n\nExample 2.28 We also always have a subspace \\{\\vec{0}\\} consisting of the zero vector alone. This is called the trivial subspace, and its dimension is 0, because it has no linearly independent sets of vectors at all.\n\n\nExample 2.29 The subspaces of \\mathbb{R}^{2} are \\{\\vec{0}\\}, lines through the origin, and \\mathbb{R}^{2} itself.\n\n\nExample 2.30 The subspaces of \\mathbb{R}^{3} are \\{\\vec{0}\\}, lines through the origin, planes through the origin, and \\mathbb{R}^{3} itself.\n\n\nExample 2.31 Let W=\\{(x,y,z)^{T}:y=2x, z=0,x,y,z\\in\\mathbb{R}\\}. This is the set of points lying on the line y=2x in the plane z=0. W is a subspace of \\mathbb{R}^{3}.\n\n\nExample 2.32 The set W=\\{(x,y,z,w)^{T}:x+y+z=1\\} is not a subspace of \\mathbb{R}^{4}. The simplest way to see this is that {\\vec 0\\not\\in }W. Another way is to choose suitable vectors \\vec{u},\\vec{v}\\in W then show that \\vec{u}+\\vec{v}\\not\\in W. A third way is to choose a suitable {\\vec u} \\in\nW then show that a{\\vec u\\not\\in }W for any a\\neq 1. Any one of the reasons suffices.\n\n\nExample 2.33 Consider the subset S of P_{2} defined so as to contain all polynomial {\\rm p}\\left( x\\right) =a+bx+cx^{2} for which a+b-2c=0. Show that S is a subspace of P_{2} and obtain a basis for S.\n\n\nTheorem 2.13: Intersection of subspacesIf W_{1} and W_{2} are subspaces of V then so is W_{1}\\cap W_{2}.\n\n\n\nProofLet \\vec{u},\\vec{v}\\in W_{1}\\cap W_{2} and a\\in K. Then \\vec{u}+\\vec{v}\\in W_{1} (because W_{1} is a subspace) and \\vec{u}+\\vec{v}\\in W_{2} (because W_{2} is a subspace). Hence \\vec{u}+\\vec{v}\\in W_{1}\\cap W_{2}. Similarly, we get a\\vec{v}\\in W_{1}\\cap W_{2} so W_{1}\\cap W_{2} is a subspace of V. ◻\n\n\n\nExample 2.34 Let V= \\mathbb{R}^{2\\times 2} and define W_{1} to be the subspace of matrices of the form \\left[\n\\begin{array}{cc}\na & 0 \\\\\nb & c\n\\end{array}\n\\right] (a,b,c real) while W_{2} is the subspace of matrices of the form \\left[\n\\begin{array}{cc}\na & a \\\\\nb & b\n\\end{array}\n\\right]. Then W_{1}\\cap W_{2} consists of all matrices of the form \\left[\n\\begin{array}{cc}\n0 & 0 \\\\\nb & b\n\\end{array}\n\\right] , which is a subspace of V.\n\n\n\n\n\n\n\nNoteRemark\n\n\n\nWhile W_{1}\\cap W_{2} is a subpace, W_{1}\\cup W_{2} is in general not a subspace. For exampe if W_1 and W_2 are the x- and y-axis of \\mathbb{R}^2, the union is not a subspace.\n\n\n\nExample 2.35 Let V=\\mathbb{R}^{2}, let W=\\{(a,0)^{T}:a\\in\\mathbb{R}\\} and W_{2}=\\{(0,b)^{T}:b\\in\\mathbb{R}\\}. Then W_{1}, W_{2} are subspaces of V, but W_{1}\\cup W_{2} is not a subspace, because (1,0)^{T}\\in W_{1}\\cup W_{2} and (0,1)^{T}\\in W_{1}\\cup W_{2}, but (1,0)^{T}+(0,1)^{T}=(1,1)^{T}\\notin W_{1}\\cup W_{2}.\n\n\nExample 2.36 The set W=\\{(x,y,z)^{T}:x^{2}+y^{2}=z\\} is not a subspace of \\mathbb{R}^{3}.\nThe vectors {\\vec u=}\\left( 0,1,1\\right) ^{T} and {\\vec v}=\\left(\n1,2,5\\right) ^{T} are both in W but {\\vec u}+{\\vec v} is not.\n\nThe three conditions in the definition of a subspace state that every linear combination of vectors of the subspace has to be an element of the subspace. This proves the following theorem:\n\nLemma 2.14: Span is a subspace{\\rm span}({\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{n}) is a subspace of V for {\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{n} \\in V.\n\n\n\nProofW={\\rm span}({\\vec v}_{1},{\\vec v}_{2},...,{\\vec v}_{n}) is by definition a subset of V. The \\vec{0} is in W, which is property (i) in the definition of a subspace, and (ii) and (iii) are also satisfied due to the span including all linear combinations. ◻\n\n\n\nExample 2.37 The set W=\\{(x,y,z,w)^{T}:x+y+z=w\\} is a subspace of \\mathbb{R}^{4}. We can show this in two ways: (a) by using the definition of a subspace (2.12) or (b) by finding a spanning set and using the above theorem. Method (a) requires checking the three conditions for a subspace:\n\n{\\bf 0} = (0,0,0,0)^{T} \\in W for x=y=z=0.\nWith {\\bf u} =  (x,y,z,x+y+z)^{T} \\in W and {\\bf v} = (x',y',z',x'+y'+z')^{T} \\in W also {\\bf u} + {\\bf v}=  (x+x',y+y',z+z',(x+x')+(y+y')+(z+z'))^{T} \\in W\nWith {\\bf u} =  (x,y,z,x+y+z)^{T} \\in W and \\lambda {\\bf u} =  (\\lambda x,\\lambda y,\\lambda z, \\lambda (x+y+z))^{T} \\in W\n\nFor method (b), we have to find vectors which span W. Note that the set W has three free parameters, which can be used to represent an arbitrary element of W: \\scriptscriptstyle \\begin{pmatrix}   x\\\\y\\\\z\\\\x+y+z  \\end{pmatrix} \\textstyle = x \\scriptscriptstyle \\begin{pmatrix}   1\\\\0\\\\0\\\\1  \\end{pmatrix} \\textstyle +y \\scriptscriptstyle \\begin{pmatrix}   0\\\\1\\\\0\\\\1  \\end{pmatrix} \\textstyle + z \\scriptscriptstyle \\begin{pmatrix}   0\\\\0\\\\1\\\\1  \\end{pmatrix} \\textstyle Hence W = {\\rm span}((1,0,0,1)^{T}, (0,1,0,1)^{T}, (0,0,1,1)^{T}). This second approach has the advantage that the spanning set can also be used to find a basis for the subspace. Indeed, the three vectors are also linearly independent (prove!), and we obtain the basis {\\vec v}_{1}=(1,0,0,1)^{T},{\\vec v}_{2}=(0,1,0,1)^{T},{\\vec v}\n_{3}=(0,0,1,1)^{T}.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector spaces</span>"
    ]
  },
  {
    "objectID": "linearequations.html",
    "href": "linearequations.html",
    "title": "3  Linear equations",
    "section": "",
    "text": "3.1 Matrices\nAnalogous to the rules of addition and scalar multiplication of vectors, we have the following rules for matrices:\nExample: \\left(\n\\begin{array}{rrr}\n1 &  2 & 5  \\\\\n  3 & 4  &   -7\n  \\end{array}\n\\right) + \\left(\n\\begin{array}{rrr}\n2 &  1 & 0  \\\\\n  -1 & 2  &   3\n  \\end{array}\n\\right) = \\left(\n\\begin{array}{rrr}\n3 &  3 & 5  \\\\\n  2 & 6  &   -4\n  \\end{array}\n\\right) 3 \\left(\n\\begin{array}{rrr}\n1 &  2 & 5  \\\\\n  3 & 4  &   -7\n  \\end{array}  \\right)  =  \\left(\n\\begin{array}{rrr}\n3 &  6 & 15  \\\\\n  9 & 12  &   -21\n  \\end{array}\n\\right) The neutral element of the addition is the zero matrix, {\\rm O}, where all entries are zero. The inverse element of the addition is the negative matrix -{\\rm A} = [-a_{i,j}]. The two operations lead to the following properties:\nIn addition to the rules above, we have a matrix multiplication:\nThe matrix C in the above example shows that vectors \\in \\mathbb{R}^{n} can be considered as n\\times1 matrices. Similarly, a 1\\times n matrix is often called a “row-vector”. We can use the operation of transposition to convert a vector to a “row vector” and vice versa. This operation is defined for arbitrary matrices.\nThis means that, for example, the first row of A^T is the first column of A and so on. Note that unless we have a square matrix (m=n), the transpose of a matrix is of a different type than the original matrix. In particular, the transpose of a column matrix (i.e. a n\\times 1 matrix ) is a row matrix, and vice versa.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear equations</span>"
    ]
  },
  {
    "objectID": "linearequations.html#matrices",
    "href": "linearequations.html#matrices",
    "title": "3  Linear equations",
    "section": "",
    "text": "Definition 3.1: MatrixAn m\\times n (read “m by n”) matrix is an array of numbers or expressions with m rows and n columns. The set of all m\\times n matrices is denoted by M_{m\\times n}.\n\n\n\nExample 3.1 A 2\\times3 matrix: {\\rm A} =\\left(\n\\begin{array}{rrr}\n1 &  2 & 5  \\\\\n  3 & 4  &   -7\n  \\end{array}\n\\right) \\in M_{2\\times3} ,  \\quad A_{1,2} =2 .\n\n\n\n\n\n\n\nNoteNotation\n\n\n\nMatrices are usually named by upper case roman letters, e.g. {\\rm A}. Individual entries are referred to by A_{i,j}, i.e. the entry in row i and column j of the array. Entries are also sometimes denoted by the corresponding lower-case letter, e.g. a_{i,j}. A matrix can be symbolically represented by its general entry: {\\rm A}=\\left[ a_{i,j}\\right].\n\n\n\n\nDefinition 3.2: Matrix sum and scalar multiplicationAddition of two matrices of the same type ({\\rm A} ,{\\rm B} \\in M_{m \\times n}) is defined by {\\rm A} + {\\rm B}=[a_{i,j}] + [b_{i,j}]= [ a_{i,j}+b_{i,j}],  and the scalar multiplication by c {\\rm A} = c [a_{i,j}] = [ c a_{i,j}] .\n\n\n\n\nCorollary 3.3: Basic Matrix Operations\\begin{aligned}\n{\\rm A} + {\\rm B} & =  {\\rm B} + {\\rm A} \\quad \\text{(addition is ccommutative)} \\\\\n  ({\\rm A} + {\\rm B}) + {\\rm C} &= {\\rm A} + ({\\rm B}  + {\\rm C}) \\quad \\text{(addition is associative)} \\\\\n  {\\rm A} + {\\rm O} & =  {\\rm A} \\quad \\text{(Existence of a neutral element for add.)} \\\\\n  {\\rm A} + {\\rm -A} & =  {\\rm O} \\quad \\text{(Existence of an inverse element for add.)} \\\\\n  a ({\\rm A} + {\\rm B})  &= a {\\rm A} + a {\\rm B} \\quad \\text{(Distributive law 1)} \\\\\n  (a+b)  {\\rm A}  &= a {\\rm A} + b {\\rm A}   \\quad \\text{(Distributive law 2)} \\\\\n  a(b {\\rm A} )&= (ab) {\\rm A}  \\quad \\text{(scalar mult. is associative)}\n\\end{aligned}\n\n\n\n\nDefinition 3.4: Matrix productThe matrix product of the m \\! \\times \\! r matrix G and the r \\! \\times \\! n matrix H is the m \\! \\times \\! n matrix P, where p_{i,j}  = g_{i,1}h_{1,j}+g_{i,2}h_{2,j}+\\dots+g_{i,r}h_{r,j} that is, the i,j-th entry of the product is the dot product of the i-th row and the j-th column. GH=\n    \\begin{pmatrix}\n              &\\vdots                     \\\\\n      g_{i,1} &g_{i,2} &\\ldots  &g_{i,r}  \\\\\n              &\\vdots\n    \\end{pmatrix}\n    \\begin{pmatrix}\n              &h_{1,j}           \\\\\n      \\ldots  &h_{2,j} &\\ldots   \\\\\n              &\\vdots            \\\\\n              &h_{r,j}\n    \\end{pmatrix}\n  =\n    \\begin{pmatrix}\n              &\\vdots            \\\\\n      \\ldots  &p_{i,j}  &\\ldots  \\\\\n              &\\vdots\n    \\end{pmatrix}\n\n\n\nExample 3.2 \\begin{pmatrix}\n     2  &0  \\\\\n     4  &6  \\\\\n     8  &2\n  \\end{pmatrix}\n  \\begin{pmatrix}\n    1  &3  \\\\\n    5  &7\n  \\end{pmatrix}\n  =\n  \\begin{pmatrix}\n   2\\cdot 1+0\\cdot 5  &2\\cdot 3+0\\cdot 7  \\\\\n   4\\cdot 1+6\\cdot 5  &4\\cdot 3+6\\cdot 7  \\\\\n   8\\cdot 1+2\\cdot 5  &8\\cdot 3+2\\cdot 7\n  \\end{pmatrix}\n  =\n  \\begin{pmatrix}\n    2  &6  \\\\\n   34  &54 \\\\\n   18  &38\n  \\end{pmatrix}\n\n\nCorollary 3.5: Matrix product rulesThe matrix multiplication is distributive with respect to the addition and scalar multiplication, and associative but not commutative.\n\n\\begin{aligned}\n({\\rm A} + {\\rm B} ) {\\rm C} & =  {\\rm A}  {\\rm C} + {\\rm B}  {\\rm C}  , \\\\\n{\\rm C}  ({\\rm A} + {\\rm B} )  & =  {\\rm C} {\\rm A}  + {\\rm C}  {\\rm B} , \\\\\n(a {\\rm A})(b {\\rm B}) &= (ab) {\\rm A}  {\\rm B} , \\\\\n({\\rm A} {\\rm B} ){\\rm C}  & = {\\rm A} ({\\rm B} {\\rm C})\n\\end{aligned}\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMatrix multiplication is not commutative in general: {\\rm A}  {\\rm B} \\neq  {\\rm B}  {\\rm A}. This is clear already for dimensional reasons. If {\\rm A} \\in M_{m\\times r} and {\\rm B} \\in M_{r\\times n} then {\\rm AB}\\in M_{m\\times n}, but the product {\\rm BA} is not even defined, unless m=n. However, even if we have quadratic matrices (m=r=n), the product is not necessarily commutative as the following example shows: \\begin{pmatrix}\n0 & 0\\\\\n1 &0\n  \\end{pmatrix}\n\\begin{pmatrix}\n2 & 0\\\\\n0 &0\n  \\end{pmatrix}\n= \\begin{pmatrix}\n0 & 0\\\\\n2 &0\n  \\end{pmatrix}  \\quad \\text{but} \\quad\n  \\begin{pmatrix}\n2 & 0\\\\\n0 &0\n  \\end{pmatrix}\n  \\begin{pmatrix}\n0 & 0\\\\\n1 &0\n  \\end{pmatrix}\n= \\begin{pmatrix}\n0 & 0\\\\\n0 &0\n  \\end{pmatrix}.\n\n\n\nExample 3.3 Let A= \\begin{pmatrix}\n2  & 1  & {-1}  \\\\\n0  & 2  & 1  \\\\\n1  & 0  & 2  \\\\\n\\end{pmatrix} ,\\quad B= \\begin{pmatrix}\n1  & 0 & 1  \\\\\n2  & 1 & 2 \\\\\n\\end{pmatrix} ,\\quad C= \\begin{pmatrix}\n1  \\\\\n2  \\\\\n1  \\\\\n\\end{pmatrix} . AB, CA, CB are not defined, but the other product are BA= \\begin{pmatrix}\n3  & 1 & 1 \\\\\n6  & 4 & 3 \\\\\n\\end{pmatrix} ,\\quad BC= \\begin{pmatrix}\n2  \\\\\n6  \\\\\n\\end{pmatrix} ,\\quad\nAC= \\begin{pmatrix}\n3  \\\\\n5  \\\\\n3  \\\\\n\\end{pmatrix} ,\\quad\nA^2= \\begin{pmatrix}\n3  & 4  & {-3}  \\\\\n1  & 4  & 4  \\\\\n4  & 1  & 3  \\\\\n\\end{pmatrix}.\n\n\n\nDefinition 3.6: Transpose of a MatrixIf A=[a_{i\\,j} ] is an m\\times n matrix, then the transpose of A, denoted by A^T, is defined as A^T=[a_{j,i}]  \\in M_{n\\times m}  .\n\n\n\n\nExample 3.4 Example Check that {\\rm A}{\\rm A}^{-1}= {\\rm I} and {\\rm A}^{-1}{\\rm A} = {\\rm I}. \\left( \\begin{array}{rrr}\n2  & 0  & 1  \\\\\n1  & 0  & 3  \n\\end{array}  \\right)^T=\\left( \\begin{array}{rrr}\n2  & 1  \\\\\n0 & 0  \\\\\n1 & 3 \\\\\n\\end{array}  \\right)    \\qquad  (2, 0 , 1)^{T} = \\scriptstyle \\begin{pmatrix}   2\\\\ 0 \\\\ 1  \\end{pmatrix} \\textstyle\n\n\nCorollary 3.7: Properties of the Transpose\\begin{aligned}\n&(i)  \\quad ({\\rm A}^T)^T  =  {\\rm A}   \\\\\n&(ii) \\quad ({\\rm A}+{\\rm B})^T  =  {\\rm A}^T+{\\rm B}^T \\\\\n&(iii)\\quad ({\\rm A}{\\rm B})^T  =  {\\rm B}^T{\\rm A}^T\n\\end{aligned}\n\n\n\nProof\n\nfollows directly from the definition.\n\\left( ({\\rm A} + {\\rm B})^{T}\\right)_{ij} =  ({\\rm A} + {\\rm B})_{ji} = {\\rm A}_{ji} + {\\rm B}_{ji}  = ({\\rm A}^{T})_{ij} + ({\\rm B}^{T})_{ij}\nThis requires, of course, that the product is defined, that is {\\rm A}\\in M_{m \\times r} and {\\rm B}\\in M_{r \\times n}. \\begin{aligned}\n({\\rm A} {\\rm B})_{ij} & = \\sum_{s=1}^{r} A_{is}B_{sj} \\\\\n\\left( ({\\rm A} {\\rm B})^{T}\\right)_{ij} & = \\sum_{s=1}^{r} A_{js}B_{si} \\\\\n& = \\sum_{s=1}^{r} ({\\rm A}^{T})_{sj}({\\rm B}^{T})_{is}  \\\\\n& =\\sum_{s=1}^{r} ({\\rm B}^{T})_{is} ({\\rm A}^{T})_{sj} = {\\rm B}^{T} {\\rm A}^{T}\n\\end{aligned}. ◻",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear equations</span>"
    ]
  },
  {
    "objectID": "linearequations.html#subsec:symmetric",
    "href": "linearequations.html#subsec:symmetric",
    "title": "3  Linear equations",
    "section": "3.2 Symmetric and Skew-symmetric Matrices",
    "text": "3.2 Symmetric and Skew-symmetric Matrices\n\nDefinition 3.8: Symmetric and skew-symmetric matricesLet {\\rm A} be a square matrix. If {\\rm A}={\\rm A}^T then {\\rm A} is said to be symmetric. If {\\rm A}= - {\\rm A}^T then {\\rm A} is said to be skew-symmetric.\n\n\nSome matrices are neither symmetric nor skew-symmetric. However, every square matrix can be written uniquely as the sum of a symmetric and skew-symmetric matrix because of the identity {\\rm A} = \\frac{{\\rm A}^{T}+{\\rm A}}{2} + \\frac{{\\rm A}- {\\rm A}^{T}}{2} , noting that (A^{T}+A)/2 is symmetric, and ({\\rm A}- {\\rm A}^{T} )/2 is skew-symmetric.\n\nExample 3.5 \\left( \\begin{array}{rrr}\n2  & 0  & {-4}  \\\\\n{-2}  & 2  & 0  \\\\\n2  & 4  & 6  \n\\end{array}  \\right) =\\left(\\begin{array}{rrr}\n2  & {-1}  & {-1}  \\\\\n{-1}  & 2  & 2  \\\\\n{-1}  & 2  & 6  \n\\end{array}  \\right)+\\left( \\begin{array}{rrr}\n0  & 1  & {-3}  \\\\\n{-1}  & 0  & {-2}  \\\\\n3  & 2  & 0  \n\\end{array}  \\right).",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear equations</span>"
    ]
  },
  {
    "objectID": "linearequations.html#inverse-matrix",
    "href": "linearequations.html#inverse-matrix",
    "title": "3  Linear equations",
    "section": "3.3 Inverse Matrix",
    "text": "3.3 Inverse Matrix\nThere exists a neutral element {\\rm I} \\in M_{n\\times n} called the identity matrix for the multiplication, which consists of 1’s down the main diagonal and 0’s everywhere else: {\\rm I} =  \\begin{pmatrix}\n          1 &0  &\\ldots   &0  \\\\\n          0 &1  &   &\\vdots   \\\\\n           \\vdots  &   & \\ddots  &0 \\\\\n          0   &  \\ldots &  0&1\n        \\end{pmatrix}\nThe existence of a neutral element with respect to matrix multiplication means: \\begin{aligned}\n{\\rm A} {\\rm I} & = {\\rm A}  \\quad \\text{and} \\quad  {\\rm I}  {\\rm A} = {\\rm A}  \\quad \\text{for all} \\ {\\rm A} \\in M_ {m\\times r}.\n\\end{aligned} Note that if {\\rm A} \\in M_{m\\times r} then in the first equation {\\rm I}\\in M_{r\\times r} while in the second equation {\\rm I}\\in M_{m\\times m}.\n\nDefinition 3.9: Inverse of a matrixGiven two square matrices {\\rm A} and {\\rm B} which satisfy {\\rm A} {\\rm B} = {\\rm B}  {\\rm A} = {\\rm I} then {\\rm B} = {\\rm A}^{-1} is called the inverse of {\\rm A}. If {\\rm A} has an inverse, it is said to be non-singular. Any matrix which does not have an inverse is said to be singular.\n\n\n\n\n\n\n\n\nNoteRemark\n\n\n\n\nNote that we have have to require both {\\rm A} {\\rm B} ={\\rm I} and {\\rm B} {\\rm A} ={\\rm I} since the matrix product is not commutative.\nNon-square matrices can never have an inverse (see remark 3.1).\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA division of matrices is not defined.\n\n\nThat we don’t have a division of matrices is due to the fact that many matrices don’t have an inverse contrary to the multiplication of real numbers where only 0 doesn’t have an inverse. And even if they do have an inverse it matters whether we multiply from the left or the right, as the multiplication is not commutative. For instance the matrix equation ({\\rm A,B,C} are n \\times n matrices)  {\\rm A}{\\rm B} = {\\rm C} can be solved for {\\rm B} using the inverse of {\\rm A}, provided it exists. We multiply from the left by {\\rm A}^{-1}:  {\\rm A}^{-1}{\\rm A}{\\rm B} = {\\rm A}^{-1}{\\rm C}  \\Rightarrow {\\rm I}{\\rm B} = {\\rm B} = {\\rm A}^{-1}{\\rm C} However, we can’t write {\\rm C}/{\\rm A} as that would not indicate whether we have to multiply with the inverse from the left or right.\n\nExample 3.6 A=\\left[ \\begin{array}{ccc}\n2 & 0 & 1 \\\\\n1 & {-2} & 1 \\\\\n3 & 1 & 1\n\\end{array}  \\right], \\quad\nA^{-1}=\\left[ \\begin{array}{ccc}\n{-3} & 1 & 2 \\\\\n2 & {-1} & {-1} \\\\\n7 & {-2} & {-4} \\\\\n\\end{array}  \\right]\n\n\nCorollary 3.10: Inverse of Tranpose and ProductIf {\\rm A} and {\\rm B} are non-singular n\\times n matrices then ({\\rm A}^{-1})^T  =  ({\\rm A}^T)^{-1} and ({\\rm A}{\\rm B})^{-1}={\\rm B}^{-1}{\\rm A}^{-1}\n\n\n\nProof {\\rm I}  =   {\\rm I}^T =  ({\\rm A}^{-1}{\\rm A})^T  =  {\\rm A}^T\\left({\\rm A}^{-1}\\right)^{T}  and  {\\rm I}  =   {\\rm I}^T =  ({\\rm A} {\\rm A}^{-1})^T  = \\left( {\\rm A}^{-1}\\right)^{T} {\\rm A}^T  imply that \\left({\\rm A}^{-1}\\right)^T  = \\left({\\rm A}^T\\right)^{-1}.\nThe second identity is proved by (AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=AI_n A^{-1}=AA^{-1}=I_n . and the corresponding reverse sequence: (B^{-1}A^{-1}(AB))=B^{-1}A^{-1}AB=I_n . ◻",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear equations</span>"
    ]
  },
  {
    "objectID": "linearequations.html#gaussian-elimination",
    "href": "linearequations.html#gaussian-elimination",
    "title": "3  Linear equations",
    "section": "3.4 Gaussian Elimination",
    "text": "3.4 Gaussian Elimination\nIn the previous sections, we considered individual equations of the type a_{1} x_{1} + a_{2} x_{2}  + ... + a_{n} x_{n} = d. In the following, we will consider systems of such equations and use a method called Gaussian elimination to determine solutions.\nWe start with a simple example. Consider the system \\begin{aligned}\n2x + 3y  & = 1,  \\\\\nx+y & = 2 .  \n\\end{aligned} Usually we would solve the system by solving e.g. the second equation for x and substituting the result in the first equation: \\begin{align*}\n2(2-y) + 3y & = 1   \\\\\n\\Leftrightarrow   (3-2\\cdot 1)y & = 1- 2\\cdot 2 \\\\\n\\Rightarrow y & =-3\n\\end{align*} The result y=-3 is then substituted back in either of the two equations, and we find x=5. When it comes to systems with large numbers of variables and equations, this approach becomes increasingly hard to follow, and we need a more systematic approach. Note that second line above can be understood as the result of subtracting twice the second equation from the first equation in the original system. That is, we converted the system \\begin{align*}\n2x + 3y  &=  1 ,\\\\\n2x+ 2y &= 4 \\ .\n\\end{align*} into the equivalent system \\begin{align*}\n2x + 3y  &=  1 ,\\\\\n      - y &= 3 \\ ,\n\\end{align*} by subtracting equations. \"Equivalent\" here means that both systems have the same solutions. This system is now trivial to solve. Operations which lead to equivalent systems are, e.g., multiplying an equation by a (non-zero) number and, as we have seen above, adding multiples of other equations.\n\nExample 3.7 Let us try this method with a more complicated system. We indicate on the right-hand side of the system the operation (multiplication and subtraction) which leads to the new system. \\begin{aligned}\nx+\\phantom{2}y-\\phantom{2}z+2t &= 12,\\\\\n2x-\\phantom{2}y+\\phantom{2}z-\\phantom{2}t &=-5,    \\qquad Eq. 2- 2 Eq. 1\\\\\nx-2y+3z+4t &= 10, \\qquad Eq. 3-  Eq. 1\\\\\n3x+3y+\\phantom{2}z+\\phantom{2}t &= 12. \\qquad Eq. 4- 3 Eq. 1\n\\end{aligned} \\begin{aligned}\nx+\\phantom{2}y-\\phantom{2}z+2t &= 12,\\\\\n\\Leftrightarrow \\qquad  0 - 3 y +3 z- 5 t &=-29,    \\qquad  \\\\\n0-3y+4z+2t &= -2, \\qquad Eq. 3-  Eq. 2\\\\\n0+\\phantom{2}0 +4 z -5 t &= -24.\n\\end{aligned} \\begin{aligned}\nx+\\phantom{2}y-\\phantom{2}z+2t &= 12,\\\\\n\\Leftrightarrow \\qquad  0 - 3 y +3 z- 5 t &=-29,    \\qquad  \\\\\n0 + \\phantom{2}0 + \\phantom{1} z+7t &= 27,\\\\\n0+\\phantom{2}0 +4 z -5 t &= -24.  \\qquad  Eq.4- 4 Eq. 3\n\\end{aligned} \\begin{aligned}\nx+\\phantom{2}y-\\phantom{2}z+2t &= 12,\\\\\n\\Leftrightarrow \\qquad   0 - 3 y +3 z- 5 t &=-29,    \\qquad  \\phantom{Eq.4- 4 Eq. 3} \\\\\n0 + \\phantom{2}0 + \\phantom{1} z+7t &= 27,\\\\\n0+\\phantom{2}0 + \\phantom{2}0  -33 t &= -132.  \n\\end{aligned} Now the system can be solved recursively: The last equation implies t=4, which can be used in the third equation to find z=-1. This, in turn, leads to y=2 in the second equation, and eventually, we get x=1 from the first equation.\n\nNote that it was now possible to easily solve the system because the non-zero entries on the left-hand side form an upper triangle, so that we can successively solve for all the variables (back-substitution).\nTo make the notation more compact, we can suppress the variables and write the system as a matrix, called augmented matrix. Instead of refering to equations we refer now to rows, and 2 R_2 - R_1 for instance means take twice row 2 and subtract row 1. For the example from above, we have e.g. \n\\begin{array}{rrrr|rl}\n1 &   1& -1 & 2& 12 & \\\\\n2 &  -1 & 1& -1 &-5 & (R_{2}-2R_{1})\\\\\n1  &   -2& 3 & 4  & 10 & (R_{3}-R_{1})\\\\\n3 & 3 &1 & 1 & 12  &   (R_{4}-3R_{1})\n\\end{array}  \\quad \\Rightarrow \\quad\n\\begin{array}{rrrr|rl}\n1 &   1& -1 & 2& 12  &\\\\\n0&  -3 & 3& -5 &-29 & \\\\\n0  &   -3& 4 & 2  & -2 & (R_{3}-R_{2}) \\\\\n0 & 0 &4 & -5 & -24 &    \n\\end{array} \\Rightarrow \\begin{array}{rrrr|rl}\n1 &   1& -1 & 2& 12 & \\\\\n0&  -3 & 3& -5 &-29 &\\\\\n0  &   0& 1 & 7  & 27 &\\\\\n0 & 0 &4 & -5 & -24   & (R_{4}-4R_{3})\n\\end{array}\n\\quad \\Rightarrow \\begin{array}{rrrr|r}\n1 &   1& -1 & 2& 12  \\\\\n0&  -3 & 3& -5 &-29 \\\\\n0  &   0& 1 & 7  & 27 \\\\\n0 & 0 &0 & -33 & -132    \n\\end{array}\nThis method is called Gauss elimination, named after Carl Friedrich Gauss (see Appendix [Gauss]). To bring the system into upper triangular form, we can use three rules which do not change the solutions of the system of equations:\n\nmultiply an equation by an arbitrary non-zero number,\nadding (or subtracting) multiples of equations to other equations (not itself!),\nchange the sequence of equations.\n\nThere are two cases where the system can fail to give a unique solution. The first case is that the system is underdetermined (or has infinitely many solutions), that is, we either have fewer equations than variables or the equations are not linearly independent. In this case, we are left with one (or more) free variables, and the best we can do is express all the other variables in terms of this free variable.\n\nExample 3.8 \\begin{aligned}\n& &\\begin{array}{rrrr|r}\n1 &   1& -1 & 2& 12  \\\\\n2 &  -1 & 1& -1 &-5 \\\\\n1  &   -2& 3 & 4  & 10 \\\\\n2 & 2 & -3 & -3 & -3    \n\\end{array}  \\quad \\Rightarrow \\quad\n\\begin{array}{rrrr|r}\n1 &   1& -1 & 2& 12  \\\\\n0&   -3 & 3& -5 &-29 \\\\\n0&  -3& 4 & 2  & -2 \\\\\n  0&  0 &-1 & -7 & -27    \n\\end{array} \\\\\n&\\Rightarrow& \\begin{array}{rrrr|r}\n1 &   1& -1 & 2& 12  \\\\\n0&   -3 & 3& -5 &-29 \\\\\n0& 0 & 1 & 7  & 27 \\\\\n0& 0 & -1 & -7 & -27    \n\\end{array} \\Rightarrow \\begin{array}{rrrr|r}\n1 &   1& -1 & 2& 12  \\\\\n0&   -3 & 3& -5 &-29 \\\\\n0& 0 & 1 & 7  & 27 \\\\\n0 & 0 & 0 &  0 & 0    \n\\end{array}\n\\end{aligned} Here, the last equation does not determine the variable t. We can, however, express all the other variables in terms of t. The third step yields z=27-7 t, the second step: y=110/3-26/3t and eventually we find x= 7/3-t/3. Note that we can write this as the parametric form of a line \\vec{r} =  \\begin{pmatrix}   7/3 \\\\ 110/3 \\\\ 27 \\\\ 0  \\end{pmatrix}  + t  \\begin{pmatrix}   -1/3 \\\\ -26/3 \\\\ -7 \\\\ 1  \\end{pmatrix}.  Indeed, we would have the same situation if we had ignored the last equation in the first place. The equation is superfluous; unfortunately, it is often not trivial to recognise which equation can be omitted if a system is underdetermined.\n\nThe other case where the method can fail to produce a unique solution is when the system is overdetermined or inconsistent.\n\nExample 3.9 \\begin{array}{rrr|r}\n  1 & 0& 1 &1 \\\\\n   0& 1 & 1  & 2 \\\\\n    1 & 2 & 3 & -3    \n\\end{array}\n\\Rightarrow\n\\begin{array}{rrr|r}\n  1 & 0& 1 &1 \\\\\n   0& 1 & 1  & 2 \\\\\n    0 & 2 & 2 & -4    \n\\end{array}\n\\Rightarrow\n\\begin{array}{rrr|r}\n  1 & 0& 1 &1 \\\\\n   0& 1 & 1  & 2 \\\\\n    0 & 0 & 0 & -8    \n\\end{array} In this case, the last equation states: 0\\cdot z=-8, which is impossible to satisfy for any z. This case often occurs if there are more equations than variables.\n\nAll three cases —the case of a unique solution, the overdetermined case, and the underdetermined case — can be understood in geometric terms. We recall that the solution to each equation is a hyperplane in \\mathbb{R}^{n}. Hence, the solution to the whole system is the intersection of all these hyperplanes. Two basic situations can be distinguished.\n\nThe set of normal vectors of the planes is linearly independent. The dimension of the space of solutions is m=n-k, where n is the number of unknowns and k is the number of equations, that is, all variables can be expressed in terms of m free variables or parameters. For the case k=n, there are no free parameters, and we obtain a unique solution.\nThe set of normal vectors of the hyperplanes is linearly dependent. This is always the case if k&gt;n.\n\nOne (or more) equations are linear combinations of the others. These equations can be removed from the system, until we are left with a set of linearly independent equations. If the normal vectors are linearly independent, case 1 applies; otherwise, we have case 2b).\nThe equations are linearly independent and the system has no solution.\n\n\nIn Example 3.7 we had the situation of case 1 with k=n and hence a unique solution.\nIn Example 3.8, we had the situation that one equation is a linear combination of the others: (Eq.~4) = (Eq.~1)+(Eq.~2) - (Eq.~3). This is case 2a) in the above scheme. If we remove this equation, we have situation 1 with n=4, k=3 and therefore m=1, which corresponds to a line as a solution.\nIn Example 3.9, we had case 2b). The normal vectors of the three planes are \\vec{n}_{1} = \\scriptscriptstyle \\begin{pmatrix}   1 \\\\ 0 \\\\ 1  \\end{pmatrix} \\textstyle, \\quad \\vec{n}_{2} = \\scriptscriptstyle \\begin{pmatrix}   0 \\\\ 1 \\\\ 1  \\end{pmatrix} \\textstyle, \\quad \\vec{n}_{1} = \\scriptscriptstyle \\begin{pmatrix}   1 \\\\ 2 \\\\ 3  \\end{pmatrix} \\textstyle . They are not linearly independent \\vec{n}_{3} = \\vec{n}_{1} +2 \\vec{n}_{2}. But the equations are linearly independent, i.e. Eq. 3 \\neq Eq_{1} + 2 Eq. 2.\n\n\n \n\n\nThe left figure is the situation in Example 3. The three hyperplanes intersect in three parallel lines. The right figure shows the situation of three linearly independent hyperplanes in ℝ3 which have one point in common. This is situation 1.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear equations</span>"
    ]
  },
  {
    "objectID": "linearequations.html#elementary-matrices",
    "href": "linearequations.html#elementary-matrices",
    "title": "3  Linear equations",
    "section": "3.5 Elementary Matrices",
    "text": "3.5 Elementary Matrices\nEach of the three transformations we performed on the augmented matrix can be achieved by multiplying the matrix on the left by an elementary matrix. The corresponding elementary matrix can be found by applying one of the three elementary row transformations to the identity matrix.\n\nDefinition 3.11An elementary matrix is an n\\times n matrix which can be obtained from the identity matrix I_{n} by performing on I_{n} a single elementary row transformation.\n\n\n\nExample 3.10 \\left(\\begin{array}{ccc}1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1\\end{array}\\right) is an elementary matrix. It can be obtained by multiplying row 2 of the identity matrix by 3. In other words, we are performing the row operation 3R_{2}\\rightarrow R_{2}.\n\n\nExample 3.11 \\left(\\begin{array}{ccc}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -2 & 0 & 1\\end{array}\\right) is an elementary matrix. It can be obtained by replacing row 3 of the identity matrix by row 3 plus -2 times row 1. In other words, we are performing the row operation R_{3}-2 R_{1}\\rightarrow R_{1}.\n\n\nExample 3.12 \\left(\\begin{array}{ccc}0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{array}\\right) is an elementary matrix. It can be obtained by switching rows 1 and 2 of the identity matrix. In other words, we are performing on the row operation R_{1}\\leftrightarrow R_{2}.\n\nSuppose we want to perform an elementary row operation on a matrix A. In that case, it is equivalent to multiplying the matrix A on the left by the elementary matrix obtained from the identity matrix by the same transformation.\n\nInterchanging Rows: R_{i}\\leftrightarrow R_{j}\n\nTo interchange rows i and j of matrix A (R_{i}\\leftrightarrow R_{j}), we multiply A on the left by the elementary matrix obtained from the identity matrix in which rows i and j have been interchanged.\n\nMultiplying a Row by a Constant: aR_{i}\\rightarrow R_{i}\n\nTo multiply row i of matrix A by a number a (aR_{i}\\rightarrow R_{i}), we multiply A on the left by the elementary matrix obtained from the identity matrix in which row i has been multiplied by a.\n\nReplacing a Row by Itself Plus a Multiple of Another: R_{i}+aR_{j}\\rightarrow R_{i}\n\nTo replace a row i by itself plus a multiple of another row j (R_{i}+aR_{j}\\rightarrow R_{i}), we multiply A on the left by the elementary matrix obtained from the identity matrix in which row i has been replaced by itself plus row j multiplied by a.\n\n\n\nExample 3.13 R_{1}\\leftrightarrow R_{3}: \\left(\\begin{array}{ccc}0 & 0 & 1\\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0\\end{array}\\right)\\left(\\begin{array}{ccc}1 & 2 & 3\\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9\\end{array}\\right)\n=\\left(\\begin{array}{ccc}7 & 8 & 9\\\\ 4 & 5 & 6 \\\\ 1 & 2 & 3\\end{array}\\right).\n\n\nExample 3.14 3R_{1}\\rightarrow R_{1}: \\left(\\begin{array}{ccc}3 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{array}\\right)\\left(\\begin{array}{ccc}1 & 2 & 3\\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9\\end{array}\\right)\n=\\left(\\begin{array}{ccc}3 & 6 & 9\\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9\\end{array}\\right).\n\n\nExample 3.15 R_{2}-R_{1}\\rightarrow R_{2}: \\left(\\begin{array}{ccc}1 & 0 & 0\\\\ -1 & 1 & 0 \\\\ 0 & 0 & 1\\end{array}\\right)\\left(\\begin{array}{ccc}1 & 2 & 3\\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9\\end{array}\\right)\n=\\left(\\begin{array}{ccc}1 & 2 & 3\\\\ 3 & 3 & 3 \\\\ 7 & 8 & 9\\end{array}\\right).\n\n\nCorollary 3.12: Inverse of elementary matricesThe elementary matrices are nonsingular. Furthermore, their inverse is also an elementary matrix.\n\n\n\nThe inverse of the elementary matrix which interchanges two rows is itself. For example \\left(\\begin{array}{ccc}0 & 1 & 0\\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{array}\\right)^{-1} =\\left(\\begin{array}{ccc}0 & 1 & 0\\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1\\end{array}\\right).\nThe inverse of the elementary matrix which multiply a row i by a constant a, i.e. aR_{i}\\rightarrow R_{i} is the elementary matrix which multiply a row i by \\frac{1}{a}, i.e. \\frac{1}{a}R_{i}\\rightarrow R_{i}. For example, \\left(\\begin{array}{ccc}1 & 0 & 0\\\\ 0 & a & 0 \\\\ 0 & 0 & 1\\end{array}\\right)^{-1} =\\left(\\begin{array}{ccc}1 & 0 & 0\\\\ 0 & \\frac{1}{a} & 0 \\\\ 0 & 0 & 1\\end{array}\\right).\nThe inverse of the elementary matrix which replaces a row i by itself plus a multiple of a row j, i.e. R_{i}+aR_{j}\\rightarrow R_{i} is the elementary matrix which replaces a row i by itself minus a multiple of a row j. For example, \\left(\\begin{array}{ccc}1 & 0 & 0\\\\ 0 & 1 & 0 \\\\ a & 0 & 1\\end{array}\\right)^{-1}=\\left(\\begin{array}{ccc}1 & 0 & 0\\\\ 0 & 1 & 0 \\\\ -a & 0 & 1\\end{array}\\right).",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear equations</span>"
    ]
  },
  {
    "objectID": "linearequations.html#calculating-the-inverse",
    "href": "linearequations.html#calculating-the-inverse",
    "title": "3  Linear equations",
    "section": "3.6 Calculating the inverse",
    "text": "3.6 Calculating the inverse\nNow we can prove that a matrix is invertible if we can convert it to the identity matrix with elementary row operations. Assume that there exists a sequence of row operations, with corresponding elementary matrices {\\rm E}_i, that converts a matrix {\\rm A} into an identity matrix.  {\\rm I} = {\\rm E}_{k} ... {\\rm E}_{2} {\\rm E}_{1} {\\rm A}  \\tag{3.1} If we call the product of these elementary matrice {\\bf B} then {\\rm I} = {\\rm B}  {\\rm A}  \\quad \\text{with} \\quad  {\\rm B} = {\\rm E}_{k} ... {\\rm E}_{2} {\\rm E}_{1} {\\rm I}  To show that {\\rm B} is the inverse, we have to show in addition {\\rm A}{\\rm B} = {\\rm I}. From Eq. 3.1 it follows by multiplying successively with the inverse of the elementary matrices {\\rm E}_{j} from the left: \\begin{aligned}\n{\\rm E}_{k}^{-1}{\\rm I} &=\\underbrace{{\\rm E}_{k}^{-1}{\\rm E}_{k}}_{={\\rm I}} {\\rm E}_{k-1} ... {\\rm E}_{2} {\\rm E}_{1} {\\rm A} \\\\\n& \\quad \\vdots  \\\\\n{\\rm E}_{1}^{-1}... {\\rm E}_{k}^{-1}{\\rm I} &=  {\\rm A}\n\\end{aligned} Ηence, \\begin{aligned}\n{\\rm A}  {\\rm B} & ={\\rm E}_{1}^{-1}... {\\rm E}_{k}^{-1}{\\rm I}  {\\rm E}_{k} ... {\\rm E}_{2} {\\rm E}_{1} {\\rm I}   \\\\\n& ={\\rm E}_{1}^{-1}... \\underbrace{{\\rm E}_{k}^{-1}{\\rm E}_{k}}_{={\\rm I}} ... {\\rm E}_{2} {\\rm E}_{1} \\\\\n& \\quad \\vdots \\\\\n&= {\\rm I}\n\\end{aligned}\nHence, we can obtain the inverse of a matrix {\\rm A} (if it exists) by applying the same row operations which convert {\\rm A} into an identity matrix. The method is usually applied to the augmented matrix {\\rm A} \\vert  {\\rm  I} where any row operations are executed simultaneously on both sides, to reduce it to {\\rm I} \\vert  {\\rm A}^{-1}.\n\nExample 3.16 To calculate {\\rm A}^{-1} when {\\rm A} =\\left( \\begin{array}{rrr}\n1  & 3  & 3  \\\\\n1  & 4  & 3  \\\\\n1  & 3  & 4  \n\\end{array} \\right), we reduce the augmented matrix to an identity matrix using elementary row operations. \\begin{aligned}\n& \\left. \\begin{array}{rrr}\n1  & 3  & 3  \\\\\n1  & 4  & 3  \\\\\n1  & 3 & 4  \n\\end{array}  \\right| \\begin{array}{rrrr}\n1  & 0  & 0  & \\qquad  \\\\\n0  & 1  & 0  & \\qquad R_{2} - R_{1}\\\\\n0  & 0  & 1  & \\qquad R_{3} - R_{1}\n\\end{array} \\\\\n\\Rightarrow \\quad  &\\left. \\begin{array}{rrr}\n1  & 3  & 3  \\\\\n0  & 1  & 0  \\\\\n0 & 0 & 1\n\\end{array}  \\right| \\begin{array}{rrrr}\n1  & 0  & 0  & \\qquad R_{1} - 3 R_{2}- 3R_{3}\\\\\n-1  & 1  & 0  & \\qquad \\\\\n-1  & 0  & 1  & \\qquad\n\\end{array}\\\\\n\\Rightarrow \\quad  &\\left. \\begin{array}{rrr}\n1  & 0  & 0  \\\\\n0  & 1  & 0  \\\\\n0 & 0 & 1\n\\end{array}  \\right| \\begin{array}{rrrr}\n7  & -3  & -3  & \\qquad  \\\\\n-1  & 1  & 0  & \\qquad \\\\\n-1  & 0  & 1  & \\qquad\n\\end{array}\n\\end{aligned}\n\n\nExample 3.17 The solution of the equations \\left( \\begin{array}{rrr}\n2  & 0  & 1  \\\\\n1  & {-2}  & 1  \\\\\n3  & 1  & 1\n\\end{array} \\right)\\left(\\begin{array}{r}\nx  \\\\\ny  \\\\\nz  \n\\end{array}  \\right)=\\left( \\begin{array}{r}\n2  \\\\\n0  \\\\\n2  \n\\end{array}  \\right) is given by \\left( \\begin{array}{r}\nx  \\\\\ny  \\\\\nz  \n\\end{array} \\right)=\\left( \\begin{array}{rrr}\n{-3}  & 1  & 2  \\\\\n2  & {-1}  & {-1}  \\\\\n7  & {-2}  & {-4}  \n\\end{array}  \\right) \\left( \\begin{array}{r}\n2  \\\\\n0  \\\\\n2  \n\\end{array} \\right)=\\left( \\begin{array}{r}\n-2 \\\\\n2  \\\\\n6  \n\\end{array}  \\right)",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear equations</span>"
    ]
  },
  {
    "objectID": "linearequations.html#factorising-matrices-the-lu-factorisation",
    "href": "linearequations.html#factorising-matrices-the-lu-factorisation",
    "title": "3  Linear equations",
    "section": "3.7 Factorising Matrices: The LU factorisation",
    "text": "3.7 Factorising Matrices: The LU factorisation\n\nDefinition 3.13: Upper/Lower Triangular MatrixA square matrix is upper (lower) triangular if all its entries below (above)the main diagonal are zero.\n\n\nSo far, we know two methods to solve a system of linear equations A\\vec{r} =  \\vec{q},  \\quad \\vec{r}, \\vec{q}\\in \\mathbb{R}^{n}, {\\rm A}\\in M_{n\\times n}, Gaussian elimination and the inversion of the matrix A. The former uses the augmented matrix A|\\vec{q} and elementary row operations to convert the system into a form where the left-hand side is an upper triangular matrix. In many applications, e.g., in algorithms to solve partial differential equations, {\\rm A} is a large matrix (1000 \\ times 1000 is not unusual), and the system has to be solved repeatedly for various right-hand sides \\ vec {q}. Here, the Gaussian elimination is very inefficient, as for every new \\vec{q}, the system has to be solved again. Inverting the matrix {\\rm A} seems to be much more efficient since we invert the matrix only once and then only apply {\\rm A}^{-1} to every new \\vec{q}. {\\rm A} \\vec{r} = \\vec{q} \\quad \\Rightarrow \\vec{r} =  {\\rm A}^{-1}  \\vec{q}. However, the matrix inversion of such large matrices itself is often numerically difficult, “numerically unstable” that is, it tends to produce very large (or very small) numbers, which in turn produce significant numerical errors.\nHere, another method, the so-called LU decomposition, has proven to be very efficient. The name is derived from the representation of the matrix A as a product {\\rm A} = {\\rm L}{\\rm U} ;  \\quad  {\\rm A},  {\\rm L}, {\\rm U} \\in M_{n\\times n} of two triangular matrices {\\rm L} and {\\rm U}, where {\\rm L} is a lower triangular matrix and {\\rm U} is an upper triangular matrix.\nThe advantage of this method is that determining {\\rm L} and {\\rm U} is faster (i.e., it uses fewer steps) than inverting {\\rm A}, and we can still solve the system comparatively easily. The methods consist of three steps:\n\nFirst step: Determine {\\rm L} and {\\rm U}.\nSecond step: Solve {\\rm L}\\underbrace{({\\rm U} \\vec{r})}_{ \\vec{s} }= \\vec{q} for \\vec{s}.\nThird step: Solve {\\rm U} \\vec{r} = \\vec{s}.\n\nStep 1: We know already (see Gauss elimination) that we can use elementary row operations to convert a matrix {\\rm A} to an upper triangular matrix. We have also seen that each of these row operations can be expressed by a matrix {\\rm E}_{j}, so that {\\rm U} =  {\\rm E}_{k} \\ldots {\\rm E}_{2} {\\rm E}_{1} {\\rm A} , where k is the number of row operations we need to bring {\\rm A} in an upper triangular form. For the following, we assume that we do not need to exchange rows in this process. (The most general case with the exchange of rows requires a more general representation of {\\rm A} as {\\rm A}= {\\rm PLU} where {\\rm P} includes all permutations of rows.) Then each of the elementary matrices {\\rm E}_{j} can be chosen as a lower triangular matrix and has an inverse {\\rm E}_{j}^{-1} which is again a lower triangular matrix, hence in \\underbrace{{\\rm E}_{1}^{-1}  {\\rm E}_{2}^{-1}\\ldots {\\rm E}_{k}^{-1}}_{:={\\rm L}} {\\rm U} =   {\\rm A} the product {\\rm E}_{1}^{-1}  {\\rm E}_{2}^{- 1}\\ldots {\\rm E}_{k}^{-1} is also a lower triangular matrix.\n\nExample 3.18 \\begin{aligned}\nA = &\\left(\\begin{array}{rrr}\na & b & c\\\\\nd & e & f\\\\\ng & h & k\n\\end{array}\\right)\n\\begin{array}{c}\n\\\\\nR_2-\\lambda R_1\\rightarrow R_2 \\\\\nR_3-\\mu R_1 \\rightarrow R_3\n\\end{array} \\\\\n\\rightarrow&\n\\left(\\begin{array}{rrr}\na & b & c \\\\\n0 & e' & f' \\\\\n0 & h' & k'\n\\end{array}\\right)\n\\begin{array}{c}\n\\\\\n\\\\\nR_3-\\omega R_1\\rightarrow R_3\n\\end{array} \\\\\n\\rightarrow&\n\\left(\\begin{array}{rrr}\na & b & c \\\\\n0 & e' & f' \\\\\n0 & 0 & k''\n\\end{array}\\right)={\\rm U}.\n\\end{aligned} where \\lambda =d/a, \\mu =g/a, \\omega =h'/e'. Note that this can be accomplished by multiplying {\\rm A} from the left by the elementary matrice {\\rm E}_1 =\\left(\n{{\\begin{array}{rrr}\n1  & 0  & 0  \\\\\n{-\\lambda }  & 1  & 0  \\\\\n0  & 0  & 1  \\\\\n\\end{array} }} \\right), \\quad  {\\rm E}_2 =\\left(\n{{\\begin{array}{rrr}\n1  & 0  & 0  \\\\\n0  & 1  & 0  \\\\\n{-\\mu }  & 0  & 1  \\\\\n\\end{array} }} \\right),  \\quad   {\\rm E}_3 =\\left(\n\\begin{array}{rrr}\n1  & 0  & 0  \\\\\n0  & 1  & 0  \\\\\n0  & - \\omega  & 1  \\\\\n\\end{array} \\right), which means:\n\nR_2-\\lambda R_1 \\rightarrow R_2: \\begin{aligned}\n\\left(\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n-\\lambda & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right)\n\\left(\\begin{array}{rrr}\na & b & c\\\\\nd & e & f \\\\\ng & h & k\n\\end{array}\\right) &=\n\\left(\\begin{array}{rrr}\na & b & c \\\\\nd-\\lambda a & e-\\lambda b & f-\\lambda c \\\\\ng & h & k\n\\end{array}\\right) =\n\\left(\\begin{array}{rrr}\na & b & c \\\\\n0 & e' & f' \\\\\ng & h & k\n\\end{array}\\right),\n\\end{aligned} R_3-\\mu R_1 \\rightarrow R_3: \\begin{aligned}\n\\left(\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n-\\mu & 0 & 1\n\\end{array}\\right)\n\\left(\\begin{array}{rrr}\na & b & c\\\\\n0 & e' & f' \\\\\ng & h & k\n\\end{array}\\right) &=\n\\left(\\begin{array}{rrr}\na & b & c \\\\\n0 & e' & f' \\\\\ng-\\mu a & h-\\mu b & k-\\mu c\n\\end{array}\\right) =\n\\left(\\begin{array}{rrr}\na & b & c \\\\\n0 & e' & f' \\\\\n0 & h' & k'\n\\end{array}\\right),\n\\end{aligned} R_3-\\omega R_1\\rightarrow R_3: \\begin{aligned}\n\\left(\\begin{array}{rrr}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & -\\omega & 1\n\\end{array}\\right)\n\\left(\\begin{array}{rrr}\na & b & c\\\\\n0 & e' & f' \\\\\n0 & h' & k'\n\\end{array}\\right) &=\n\\left(\\begin{array}{rrr}\na & b & c \\\\\n0 & e' & f' \\\\\n0 & h'-\\omega e' & k'-\\omega f'\n\\end{array}\\right) =\n\\left(\\begin{array}{rrr}\na & b & c \\\\\n0 & e' & f' \\\\\n0 & 0 & k''\n\\end{array}\\right).\n\\end{aligned}\nAll three elementary matrices are of the lower triangular type and have an inverse {\\rm E}_1^{-1} =\\left(\n{{\\begin{array}{rrr}\n1  & 0  & 0  \\\\\n{\\lambda }  & 1  & 0  \\\\\n0  & 0  & 1  \\\\\n\\end{array} }} \\right), \\quad  {\\rm E}_2^{-1} =\\left(\n{{\\begin{array}{rrr}\n1  & 0  & 0  \\\\\n0  & 1  & 0  \\\\\n{\\mu }  & 0  & 1  \\\\\n\\end{array} }} \\right), \\quad   {\\rm E}_3^{-1} =\\left(\n\\begin{array}{rrr}\n1  & 0  & 0  \\\\\n0  & 1  & 0  \\\\\n0  &  \\omega  & 1  \\\\\n\\end{array} \\right)  .\nWhen we perform the matrix multiplication to obtain {\\rm L}: {\\rm L} ={\\rm E}_{1}^{-1}  {\\rm E}_{2}^{- 1} {\\rm E}_{3}^{-1}  =\\left(\n\\begin{array}{rrr}\n1  & 0  & 0  \\\\\n\\lambda & 1  & 0  \\\\\n\\mu &  \\omega  & 1  \\\\\n\\end{array} \\right), We notice that this matrix multiplication can be performed by just putting the nonzero off-diagonal entries of the inverses of the elementary matrices into the appropriate positions in the matrix {\\ rm L}. This means that the entries of {\\rm L}, which are the multiplying factors in the Gaussian elimination process, can be easily stored during the process of Gaussian elimination.\nThus, the matrix A was factorised into the product of the lower triangular matrix {\\rm L} and the upper triangular matrix {\\rm U} as follows \\left(\\begin{array}{rrr}\na & b & c\\\\\nd & e & f\\\\\ng & h & k\n\\end{array}\\right) =\n\\left(\\begin{array}{rrr}\n1 & 0  & 0 \\ \\\\\n\\lambda & 1 & 0  \\\\\n\\mu &  \\omega & 1  \\\\\n\\end{array} \\right)\n\\left(\\begin{array}{rrr}\na & b & c \\\\\n0 & e' & f' \\\\\n0 & 0 & k''\n\\end{array}\\right)\n\nWe illustrate the method by a simple example.\n\nExample 3.19 Let {\\rm A} =\\left(\\begin{array}{rrr}\n1  & 2  & 1  \\\\\n2  & {-1}  & 3  \\\\\n{-1}  & 2  & 2  \n\\end{array} \\right). We reduce A to upper triangular form in the usual way, but record the multiplying factors in a lower triangular matrix L: \\begin{aligned}\n& \\left(\\begin{array}{rrr}\n1 & 2 & 1 \\\\\n2 & -1 & 3 \\\\\n-1 & 2 & 2\n\\end{array}\\right)\n\\begin{array}{r}\n\\\\\nR_2-2R_1 \\\\\nR_3+R_1\n\\end{array} \\\\\n\\rightarrow &\n\\left(\\begin{array}{rrr}\n1 & 2 & 1 \\\\\n0 & -5 & 1 \\\\\n0 & 4 & 3\n\\end{array}\\right)\n\\begin{array}{c}\n\\\\\n\\\\\nR_3+\\frac{4}{5}R_2\n\\end{array} \\\\\n\\rightarrow &\n\\left(\\begin{array}{rrr}\n1 & 2 & 1\\\\\n0 & -5 & 1\\\\\n0 & 0 & 19/5\n\\end{array}\\right)\n\\end{aligned}\nso L becomes {\\rm L} = \\left(\\begin{array}{rrr}\n1  & 0  & 0  \\\\\n2  & 1  & 0  \\\\\n{-1}  & {-4/5}  & 1  \n\\end{array} \\right). Now check that \\left(\\begin{array}{rrr}\n1  & 0  & 0  \\\\\n2  & 1  & 0  \\\\\n{-1}  & {-4/5}  & 1  \\\\\n\\end{array} \\right)\\left(\\begin{array}{rrr}\n1  & 2  & 1  \\\\\n0  & {-5}  & 1  \\\\\n0  & 0  & {19/5}  \n\\end{array} \\right)=\\left(\\begin{array}{rrr}\n1  & 2  & 1  \\\\\n2  & {-1}  & 3  \\\\\n{-1}  & 2  & 2  \n\\end{array} \\right).\n\nOnce we have this factorisation, we can make use of it to solve {\\rm A}\\vec{r}=\\vec{q} as follows: {\\rm A}\\vec{r}={\\rm L}{\\rm U}\\vec{r}=\\vec{q} is equivalent to solving {\\rm L}\\vec{s}=\\vec{q}\\quad\\textrm{for}\\quad\\vec{s}={\\rm U}\\vec{r}, using forward substitution, followed by {\\rm U}\\vec{r}=\\vec{s} using backwards substitution.\n\nExample 3.20 Example 3.9. To solve {\\rm A}\\vec{r}= \\scriptscriptstyle \\begin{pmatrix}    2\\\\ 9 \\\\ 0  \\end{pmatrix} \\textstyle, first we solve {\\rm L}\\vec{s}=\\vec{q} for \\vec{s}=(s_{x},s_{y},s_{z})^{T}: \\begin{aligned}\n& \\left( \\begin{array}{rrr}\n1  & 0  & 0  \\\\\n2  & 1  & 0  \\\\\n-1 & -4/5 & 1\n\\end{array} \\right) \\scriptstyle \\begin{pmatrix}   s_{x}\\\\ s_{y} \\\\ s_{z}  \\end{pmatrix} \\textstyle = \\scriptstyle \\begin{pmatrix}    2\\\\ 9 \\\\ 0  \\end{pmatrix} \\textstyle \\\\\n\\Rightarrow & \\left\\{ \\begin{array}{l}\ns_{x} =2, \\\\\ns_{y} = 9-2s_{x} = 5,\\\\\ns_{z} = s_{x}+\\frac{4}{5}s_{y}=6,\n\\end{array} \\right. \\\\\n\\Rightarrow &\n\\vec{s}= \\scriptstyle \\begin{pmatrix}    2\\\\ 5 \\\\ 6  \\end{pmatrix} \\textstyle.\n\\end{aligned} Then we solve {\\rm U}\\vec{r}=\\vec{s} for \\vec{r}=(x,y,z)^{T}: \\begin{aligned}\n& \\left( \\begin{array}{rrr}\n1  & 2  & 1  \\\\\n0  & -5 & 1  \\\\\n0  & 0  & 19/5\n\\end{array} \\right) \\scriptstyle \\begin{pmatrix}   x\\\\ y \\\\ z  \\end{pmatrix} \\textstyle = \\scriptstyle \\begin{pmatrix}    2\\\\ 5 \\\\ 6  \\end{pmatrix} \\textstyle  \\\\\n\\Rightarrow &\n\\left\\{\\begin{array}{l}\nz = 30/19,\\\\\ny=-\\frac{1}{5}(5-z)=-13/19,\\\\\nx=2-2y-z=34/19,\n\\end{array} \\right. \\\\\n\\Rightarrow &\n\\vec{r}=\\scriptstyle \\begin{pmatrix}    34/19\\\\ -13/19 \\\\ 30/19  \\end{pmatrix} \\textstyle.\n\\end{aligned}",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear equations</span>"
    ]
  },
  {
    "objectID": "linearequations.html#range-and-nullspace-of-a-matrix",
    "href": "linearequations.html#range-and-nullspace-of-a-matrix",
    "title": "3  Linear equations",
    "section": "3.8 Range and Nullspace of a Matrix",
    "text": "3.8 Range and Nullspace of a Matrix\nConsider a system of linear equations, A\\mathbf{x} = \\mathbf{y} where A is an m \\times n matrix. We can consider the matrix A as a mapping from the space of all \\mathbf{x} vectors in \\mathbb{R}^{n} onto the space of all possible \\mathbf{y} vectors, which is a subset of \\mathbb{R}^{m}. A: \\mathbf{x} \\in \\mathbb{R}^{n} \\longrightarrow \\mathbf{y} = A \\mathbf{x} \\in \\mathbb{R}^{m}.\n\nDefinition 3.14: Range and RankLet A be an m \\times n matrix. The image of \\mathbb{R}^{n} under A is a subspace of \\mathbb{R}^{m} called the range (or column space) of A, denoted by {\\rm range}(A), and  {\\rm range}(A) = {\\rm span}({\\rm columns \\, of } A). The dimension of the column space is called the rank and is denoted by {\\rm rank}(A).\n\n\n\nExample 3.21 Calculate the range and rank of  {\\rm A} = \\left[\\begin{array}{ccc}\n            1 & 2 & 3 \\\\\n            2 & 3 & 4\n        \\end{array}\\right].\n\n\nSolution{\\rm range(A)}= {\\rm span} \\left( \\begin{pmatrix} 1 \\\\ 2  \\end{pmatrix} , \\begin{pmatrix} 2 \\\\ 3  \\end{pmatrix},\\begin{pmatrix} 3 \\\\ 4  \\end{pmatrix} \\right) However, the set of vectors (columns of {\\rm A}) is not linearly independent. A test for linear independence yields:  c_1   \\begin{pmatrix} 1 \\\\ 2  \\end{pmatrix}+ c_2 \\begin{pmatrix} 2 \\\\ 3  \\end{pmatrix} +c_3 \\begin{pmatrix} 3 \\\\ 4  \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0  \\end{pmatrix}  leads to  \\left\\{\\begin{array}{l}\nc_1+2 c_2+3 c_3=0 \\\\\n2 c_1+ 3 c_2+ 4 c_3=0\n        \\end{array}  \\right.    \\Leftrightarrow  \\left\\{ \\begin{array}{l}\nc_1 = -2 c_2 -3c_3 \\\\\n2(-2 c_2 -3c_3) + 3c_2+ 4c_3=0\n        \\end{array}  \\right.   \\Leftrightarrow  \\left\\{ \\begin{array}{l}\nc_1 = -2 c_2 -3c_3 \\\\\n- c_2+ 2c_3=0\n        \\end{array}  \\right.   \\Leftrightarrow  \\left\\{ \\begin{array}{l}\nc_1 = c_3 \\\\\nc_2 =- 2 c_3\n        \\end{array}  \\right.  That means we have infinitely many solutions. For every choice of c_3 there is a c_2 and c_1, e.g. (c_1,c_2,c_3)=(1,-2,1) is a solution. We can express for instance the third vector as  \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} =  2 \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}  so  {\\rm span} \\left( \\begin{pmatrix} 1 \\\\ 2  \\end{pmatrix} , \\begin{pmatrix} 2 \\\\ 3  \\end{pmatrix},\\begin{pmatrix} 3 \\\\ 4  \\end{pmatrix} \\right) ={\\rm span} \\left( \\begin{pmatrix} 1 \\\\ 2  \\end{pmatrix} , \\begin{pmatrix} 2 \\\\ 3  \\end{pmatrix} \\right).  This means the two vectors form a basis of the column space (range) and the rank is the dimension of the column space, i.e. the number of elements of the basis: 2.\n\n\nIn order for A\\mathbf{x} = \\mathbf{b} to have a solution, \\mathbf{b} has to be in the range of {\\rm A}: \\mathbf{b} \\in {\\rm range}(A).\n\nExample 3.22 Let A = \\left[\\begin{array}{ccc}\n            1 & 0 & 1 \\\\\n            1 & 1 & 3 \\\\\n            0 & 1 & 2\n        \\end{array}\\right]\nCheck whether the system A\\mathbf{x} = \\mathbf{b} consistent when\n\n\\mathbf{b} = \\left[1,1,1\\right]^T,\n\\mathbf{b} = \\left[1,2,1\\right]^T.\n\nUnder what general conditions on \\mathbf{b} = \\left[\\mathbf{b_1},\\mathbf{b_2},\\mathbf{b_3}\\right]^T is the system consistent?\n\nSolutionWe reduce A to a row echelon form: \\left[\\begin{array}{ccc} 1 & 0 & 1 \\\\ 1 & 1 & 3 \\\\ 0 & 1 & 2 \\end{array}\\right] \\xrightarrow{R_2 - R_1} \\left[\\begin{array}{ccc} 1 & 0 & 1 \\\\ 0 & 1 & 2 \\\\ 0 & 1 & 2 \\end{array}\\right] \\xrightarrow{R_3 - R_2} \\left[\\begin{array}{ccc} 1 & 0 & 1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{array}\\right]\nHence, for example, the first two columns span {\\rm range}(A) and hence \\mathbf{b} = \\lambda \\left[1,1,0\\right]^T + \\mu \\left[0,1,1\\right]^T = \\left[\\mathbf{b_1}, \\mathbf{b_1} + \\mathbf{b_3}, \\mathbf{b_3}\\right]^T is necessary for the system to be consistent. This is satisfied for \\mathbf{b} = \\left[1,2,1\\right]^T, but not for \\mathbf{b} = \\left[1,1,1\\right]^T. Note that the first and the third columns of A also span {\\rm range}(A). Even the last two columns can be used.\n\n\n\n\nDefinition 3.15: Nullspace and NullityLet A be an m \\times n matrix. The nullspace of A, is the set of vectors {\\bf x} such that A\\mathbf{x} = \\mathbf{0}. The nullspace is also called kernel and denoted by {\\rm Null}(A).\nThe dimension of the nullspace is called the nullity and is denoted by {\\rm nullity}(A).\n\n\n\nCorollary 3.16: Nullspace is a subspaceLet A be an m \\times n matrix. The nullspace of A is a subspace of \\mathbb{R}^{n}.\n\n\n\nProof\nWe have to check the three condition in the definition for a subspace (2.12):\n\nSince A\\mathbf{0_n} = \\mathbf{0_m},  \\mathbf{0_n} \\in {\\rm Null}(A).\nLet \\mathbf{u} and \\mathbf{v} be in {\\rm Null}(A). Therefore A\\mathbf{u} = \\mathbf{0} and A\\mathbf{v} = \\mathbf{0}. It follows that A(\\mathbf{u} + \\mathbf{v}) = A\\mathbf{u} + A\\mathbf{v}  = \\mathbf{0} + \\mathbf{0} = \\mathbf{0} Hence, \\mathbf{u} + \\mathbf{v} \\in {\\rm Null}(A).\nFinally, for any scalar \\lambda, A(\\lambda \\mathbf{u}) = \\lambda (A\\mathbf{u}) = \\lambda \\mathbf{0} = \\mathbf{0} and therefore \\lambda \\mathbf{u} \\in {\\rm Null}(A). It follows that {\\rm Null}(A) is a subspace of \\mathbb{R}^{n}.  ◻\n\n\n\n\nExample 3.23 Calculate the nullspace and nullity of  {\\rm A} = \\left(\\begin{array}{ccc}\n            1 & 2 & 3 \\\\\n            2 & 3 & 4\n        \\end{array}\\right).\n\n\nSolution {\\rm A} \\mathbf{x} = \\left(\\begin{array}{ccc}\n            1 & 2 & 3 \\\\\n            2 & 3 & 4\n        \\end{array}\\right)\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} =\\begin{pmatrix} x_1 + 2 x_2+ 3 x_3 \\\\ 2 x_1 + 3 x_2 +4 x_3\\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n The vector equation is equivalent to two scalar equations for the components:  \\left\\{\\begin{array}{l}\nx_1 + 2 x_2 + 3 x_3=0 \\\\\n2 x_1+ 3 x_2+ 4 x_3=0\n        \\end{array}  \\right.    \\Leftrightarrow  \\left\\{ \\begin{array}{l}\nx_1 = -2 x_2 -3 x_3 \\\\\n2(-2 x_2 -3 x_3) + 3 x_2+ 4 x_3=0\n        \\end{array}  \\right.   \\Leftrightarrow  \\left\\{ \\begin{array}{l}\nx_1 = -2 x_2 -3 x_3 \\\\\n- x_2+ 2 x_3=0\n        \\end{array}  \\right.   \\Leftrightarrow  \\left\\{ \\begin{array}{l}\nx_1 = x_3 \\\\\nx_2 =- 2 x_3\n        \\end{array}  \\right.  This indicates that there are infinitely many solutions. We can choose one of the variables, e.g.~x_3 as a free parameter.  \\mathbb{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} x_3 \\\\ -2 x_3 \\\\ x_3 \\end{pmatrix} = x_3 \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}, \\quad \\lambda \\in \\mathbb{R}  In the last step we replaced x_3 by \\lambda, as it is convention to use Greek letters for parameters. Thus  {\\rm Null(A)} = {\\rm span}\\left(\\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix} \\right) and the nullity of {\\rm A} is 1.\n\n\n\nDefinition 3.17: Row and column spaceThe span of the columns of a m \\times n matrix {\\rm A} is called the column space. It is a subspace of \\mathbb{R}^n. The span of the rows of {\\rm A} is called the row space and it is a subspace of \\mathbb{R}^m.\n\n\n\nTheorem 3.18: Dimension of row and column spacesThe row and column spaces of a matrix A have the same dimension.\n\n\n\nProofLet R be a row echelon form of A, row(A) = row(R), as we used only row operations to convert A to R. \\begin{aligned}\n{\\rm dim}({\\rm row}(A)) &=  {\\rm dim}({\\rm row}(R)) \\\\\n             & = \\text{number of nonzero rows of}\\ R \\\\\n             & =  \\text{number of pivots of}\\  R\n\\end{aligned} \nLet this number be called \\gamma.\nNow {rm col(A)} \\neq {\\rm col(R)}, but the columns of {\\rm A} and {\\rm R} have the same dependence relationships. Therefore, {\\rm dim(col(A))} = {\\rm dim(col(R))}. Since there are \\gamma pivots, {\\rm R} has \\gamma columns that are linearly independent, and the remaining columns of {\\rm R} are linear combinations of them. Thus, {\\rm dim(col(R))} = \\gamma. It follows that {\\rm dim(row(A))} = \\gamma = {\\rm dim(col(A))}, as we wished to prove. ◻\n\n\n\nTheorem 3.19: Rank TheoremIf A is an m \\times n matrix, then {\\rm rank}(A) + {\\rm nullity}(A) = n, where n is the number of columns of A.\n\n\n\nProofLet R be a row echelon form of A, and suppose that rank(A) = \\gamma. Then R has \\gamma pivots, so there are \\gamma variables corresponding to the leading entries and n-\\gamma free variables in the solution to A\\mathbf{x} = \\mathbf{0}. Since {\\rm dim}({\\rm Null}(A)) = n - \\gamma, we have {\\rm rank}(A) + {\\rm nullity}(A) = \\gamma + (n- \\gamma ) = n.\n\n\n\nExample 3.24 Find the rank and nullity, and then verify the rank theorem for the matrix:  {\\rm A} = \\left(\\begin{array}{ccc}\n            1 & 1 & 0 \\\\\n            1 & 0 & -1 \\\\\n            0 & 2 & 2\n        \\end{array}\\right).\n\n\nSolutionIn order to find the rank we have to check whether the columns are linearly independent. For this we solve c_1 \\begin{pmatrix} 1 \\\\ 1 \\\\ 0  \\end{pmatrix}+ c_2 \\begin{pmatrix} 1 \\\\ 0 \\\\ 2  \\end{pmatrix} + c_3 \\begin{pmatrix} 0 \\\\ -1 \\\\ 2  \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0  \\end{pmatrix}  This is a system of three equations for the three unkowns c_1,c_2,c_3 and we can either try to directly solve the equations:  \\Leftrightarrow \\left\\{\\begin{array}{l}\nc_1  = - c_2 \\\\\nc_1 - c_3=0 \\\\\n2 c_2 + 2 c_3 =0\n        \\end{array}  \\right.    \\Leftrightarrow  \\left\\{ \\begin{array}{l}\nc_1 = - c_2  \\\\\nc_1 = c_3\n        \\end{array}  \\right.  or use Gaussian elimination to solve this: \\left[\\begin{array}{ccc|c} 1 & 1 & 0 & 0\\\\ 1 & 0 & -1 & 0 \\\\ 0 & 2 & 2 & 0\\end{array}\\right] \\xrightarrow{R_2 - R_1} \\left[\\begin{array}{ccc|c} 1 & 1 & 0 & 0\\\\ 0 & -1 & -1 & 0\\\\ 0 & 2 & 2 & 0 \\end{array}\\right] \\xrightarrow{R_3 + 2R_2} \\left[\\begin{array}{ccc|c} 1 & 1 & 0 & 0 \\\\ 0 & -1 & -1 & 0\\\\ 0 & 0 & 0 & 0 \\end{array}\\right] In either case we see that there are non-trivial solutions, e.g.~(c_1,c_2,c_3)=(1,-1,1). So the system is linearly dependent. We can remove , e.g.~ the third column vector to obtain a linearly independent set (the first two columns are obviously linearly independent). Hence these can form a basis of the column space and the rank is 2.\nThe nullity, that is the dimension of the nullspace, is found by an almost identical calculation:  {\\rm A} \\mathbf{x} = \\left(\\begin{array}{ccc}\n             1 & 1 & 0 \\\\\n            1 & 0 & -1 \\\\\n            0 & 2 & 2\n        \\end{array}\\right)\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} =\\begin{pmatrix} x_1 + x_2\\\\ x_1 - x_3 \\\\ 2 x_2 + 2 x_3\\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}  as this is the same system that we solved for the linear independence only the c_i are now x_i:\n \\Leftrightarrow \\left\\{\\begin{array}{l}\nx_1  = - x_2 \\\\\nx_1 - x_3=0 \\\\\n2 x_2 + 2 x_3 =0\n        \\end{array}  \\right. \\Rightarrow \\left\\{ \\begin{array}{l}\nx_1 = - x_2  \\\\\nx_1 = x_3   \\end{array} \\right.   \\Rightarrow  \\mathbb{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ - x_1 \\\\ x_1 \\end{pmatrix} = x_1 \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}, \\quad \\lambda \\in \\mathbb{R}  Hence,  {\\rm Null(A)} = {\\rm span}\\left(\\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} \\right), and the nullity of {\\rm A} is 1.\n\n\n\nExample 3.25 Find the nullity of each of the following matrices:\nM = \\left[\\begin{array}{cc} 2 & 3 \\\\ 1 & 5 \\\\ 4 & 7 \\\\ 3 & 6 \\end{array}\\right]; \\qquad N = \\left[\\begin{array}{cccc} 2 & 1 & -2 & -1 \\\\ 4 & 4 & -3 & 1 \\\\ 2 & 7 & 1 & 8 \\end{array}\\right]\n\nSolutionSince the two columns of M are linearly independent, {\\rm rank}(M)=2. Thus, by the Rank Theorem, {\\rm nullity}(M) = 2 - {\\rm rank}(M) = 2 - 2 = 0.\nTo find the nullity of {\\rm N} we first determine the dimension of the row space and the use Theorem 3.18 for find the rank. To find the dimension of the row space we can apply row operations to reduce {\\rm N} to a row echelon form, as row operations don’t change the span of the row vectors: \\left[\\begin{array}{cccc} 2 & 1 & -2 & -1 \\\\ 4 & 4 & -3 & 1 \\\\ 2 & 7 & 1 & 8 \\end{array}\\right] \\xrightarrow{\\begin{subarray}{c} R_2 - 2R_1 \\\\ R_3 - R_1 \\end{subarray}} \\left[\\begin{array}{cccc} 2 & 1 & -2 & -1 \\\\ 0 & 2 & 1 & 3 \\\\ 0 & 6 & 3 & 9 \\end{array}\\right] \\xrightarrow{R_3 - 2R_2} \\left[\\begin{array}{cccc} 2 & 1 & -2 & -1 \\\\ 0 & 2 & 1 & 3 \\\\ 0 & 0 & 0 & 0 \\end{array}\\right] We see that there are only two nonzero rows, so {\\rm dim(row\\ space)} = {\\rm rank}(N)=2. Hence, {\\rm nullity}(N) = 4 - {\\rm rank}(N) = 4 - 2 = 2.\n\n\n\n\nExample 3.26 Let A = \\left[\\begin{array}{cccc}\n            1 & 0 & 1 & 2 \\\\\n            2 & 1 & 2 & 5 \\\\\n            1 & 1 & 0 & 2 \\\\\n            0 & 1 & 0 & 1\n        \\end{array}\\right] Verify the Rank Theorem.\n\nSolutionA row echelon form is given by\n\\left[\\begin{array}{cccc} 1 & 0 & 1 & 2 \\\\ 2 & 1 & 2 & 5 \\\\ 1 & 1 & 0 & 2 \\\\ 0 & 1 & 0 & 1 \\end{array}\\right] \\xrightarrow{\\begin{subarray}{c} R_2 - 2R_1 \\\\ R_3 - R_1 \\end{subarray}} \\left[\\begin{array}{cccc} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{array}\\right] \\xrightarrow{\\begin{subarray}{c} R_3 - R_2 \\\\ R_4 - R_2 \\end{subarray}} \\left[\\begin{array}{cccc} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & -1 & -1 \\\\ 0 & 0 & 0 & 0 \\end{array}\\right]\nNotice that columns 1, 2 and 3 have pivots, but not column 4, so columns 1, 2 and 3 are linearly independent and the {\\rm rank}(A) = 3.\nSolving the equation A\\mathbf{x} = \\mathbf{0}, we see that all solutions are of the form \\mathbf{x} = c \\left[1, 1, 1, -1 \\right]^{T}, so that the dimension of the nullspace is 1. Thus, {\\rm rank}(A) + {\\rm nullity}(A) = 3 + 1 = 4, verifying the Rank Theorem.\n\n\n\n\nExample 3.27 Find the rank and nullity of A = \\left[\\begin{array}{ccccc}\n            1 & 1 & 0 & 1 & 1 \\\\\n            0 & 1 & 1 & 2 & 0 \\\\\n            1 & 2 & 1 & 3 & 1 \\\\\n            2 & 3 & 1 & 4 & 2\n        \\end{array}\\right]\nIdentify bases for both the range and nullspace of A.\n\nSolution\n A row echelon form is given by \\left[\\begin{array}{ccccc} 1 & 1 & 0 & 1 & 1 \\\\ 0 & 1 & 1 & 2 & 0 \\\\ 1 & 2 & 1 & 3 & 1 \\\\ 2 & 3 & 1 & 4 & 2 \\end{array}\\right] \\xrightarrow{\\begin{subarray}{c} R_{3} - R_{1} \\\\ R_{4} - 2R_{1} \\end{subarray}} \\left[\\begin{array}{ccccc} 1 & 1 & 0 & 1 & 1 \\\\ 0 & 1 & 1 & 2 & 0 \\\\ 0 & 1 & 1 & 2 & 0 \\\\ 0 & 1 & 1 & 2 & 0 \\end{array}\\right] \\xrightarrow{\\begin{subarray}{c} R_{3} - R_{2} \\\\ R_{4} - R_{2} \\end{subarray}} \\left[\\begin{array}{ccccc} 1 & 1 & 0 & 1 & 1 \\\\ 0 & 1 & 1 & 2 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\end{array}\\right]\nHence {\\rm rank}(A) = 2 and {\\rm nullity}(A) = 5 - {\\rm rank}(A) = 5 - 2 = 3. Columns 1 and 2 have pivots, therefore a basis for {\\rm range}(A) is \\left\\{\\left[1,0,1,2\\right]^T, \\left[1,1,2,3\\right]^T\\right\\}.\nLet x_3 = r, x_4 = s and x_5 = t, then x_1 = r + s - t, x_2 = -r - 2s, so that \\mathbf{x} = r \\left[\\begin{array}{c} 1 \\\\ -1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array}\\right] + s \\left[\\begin{array}{c} 1 \\\\ -2 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right] + t \\left[\\begin{array}{c} -1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array}\\right] So a basis for the nullspace is \\left\\{\\left[1,-1,1,0,0\\right]^T, \\left[1,-2,0,1,0\\right]^T, \\left[-1,0,0,0,1\\right]^T\\right\\}\nSummary of the procedure to find a basis for the nullspace of A.\n\nFind a row echelon form R of A.\n\nSolve for the leading variables (x_1 and x_2 above) of R\\mathbf{x} = \\mathbf{0} in terms of the free variables (x_3, x_4 and x_5 in the example).\nSet the free variables equal to parameters, substitute back into \\mathbf{x}, and write the results as a linear combination of f vectors (where f is the number of free variables). These f vectors form a basis for {\\rm Null}(A).\n\n\n\n\n\n\n\n\n\n\nNote 3.1: Remark: Nullspace of non-square matrices\n\n\n\nA non-square matrix, m \\times n, with n&gt;m must always have a non-trivial nullspace, i.e. a nullity &gt; 0. The reason is that the rank can be at most m as the dimension of column space and row space are the same (Theorem 3.18). Then the rank theorem implies that the nullity is at least 1. This is the reason why a non-square matrix can’t have an inverse.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear equations</span>"
    ]
  },
  {
    "objectID": "determinants.html",
    "href": "determinants.html",
    "title": "4  Determinants",
    "section": "",
    "text": "4.1 Introduction\nDeterminants occur in many situations, geometrically they can beinterpreted as the volume of the parallelepiped spanned by a set of n vectors. This is easily visualised in two dimensions (x-y-plane). The area (2-dimensional volume) of a parallelepiped spanned by the two vectors {\\bf b}=(b_{1},b_{2})^{T} and {\\bf c}=(c_{1},c_{2})^{t} is up to a sign given by area = \\pm(b_{1}c_{2}- b_{2} c_{1} ) We call this the determinant of the matrix {\\rm A} = [{\\bf b},{\\bf c}] and write {\\rm det(A)}= |{\\rm A}|=\\left|\n\\begin{array}{cc}\nb_{1} & c_{1} \\\\\nb_{2} & c_{2}\n\\end{array}\n\\right| = b_{1}c_{2}- b_{2} c_{1} . The determinant of a square matrix A, denoted by \\det(A), or \\left|\nA\\right|, is a number (scalar). The now-standard notation was first introduced by Cayley in 1841.\nIn three dimensions, the volume of the cell spanned by the vectors {\\bf a}, {\\bf b}, and {\\bf c} is given by {\\bf a}\\cdot ( {\\bf b} \\times {\\bf c} )= a_{1}(b_{2}c_{3}- b_{3}c_{2}) + a_{2} (b_{3}c_{1}-b_{1}c_{3}) + a_{3} (b_{1}c_{2}-b_{2}c_{1}). Note that we can write this in terms of the determinants of 2x2 matrices: {\\bf a}\\cdot ({\\bf b} \\times {\\bf c}) = \\left|\n\\begin{array}{ccc}\na_{1} & b_{1} & c_{1} \\\\\na_{2} & b_{2} & c_{2} \\\\\na_{3} & b_{3} & c_{3}\n\\end{array}\n\\right|  = a_{1} \\left|\n\\begin{array}{cc}\nb_{2} & c_{2} \\\\\nb_{3} & c_{3}\n\\end{array}\n\\right|  - a_{2} \\left|\n\\begin{array}{cc}\nb_{1} & c_{1} \\\\\nb_{3} & c_{3}\n\\end{array}\n\\right| + a_{3} \\left|\n\\begin{array}{cc}\nb_{1} & c_{1} \\\\\nb_{2} & c_{2}\n\\end{array}\n\\right| , where each of the 2 \\times 2 matrices is obtained by deleting the first column and the corresponding row of the coefficient a_{j} from the matrix {\\rm A} = [{\\bf a},{\\bf b}, {\\bf c}]. This suggests a recursive scheme where determinants of arbitrary n \\times n matrices are defined in terms of determinants of (n-1)\\times(n-1) matrices, which again are expanded in determinants of (n-2)\\times(n-2) and so on.\nThe previous example should illustrate the large amount of computation needed by using a “brute force” approach to evaluating determinants. We will now look at smarter ways, using two fundamental theorems to develop a simple numerical procedure closely related to Gaussian elimination. The following example shows that it is much more efficient to calculate determinants with many zeros in columns or rows:",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determinants</span>"
    ]
  },
  {
    "objectID": "determinants.html#introduction",
    "href": "determinants.html#introduction",
    "title": "4  Determinants",
    "section": "",
    "text": "Example 4.1 Evaluate \\left|\n\\begin{array}{ccc}\n1 & 2 & 3 \\\\\n2 & 3 & 2 \\\\\n3 & 3 & 4\n\\end{array}\n\\right| .\n\n\nDefinition 4.1: Minor and CofactorCorresponding to each entry a_{i,j} in an n\\times n matrix A, we define a number M_{ij}, called the minor of a_{ij}, which is the \\left( n-1\\right)th order determinant obtained by deleting the i-th row and j-th column from A.\nThe cofactor C_{ij} corresponding to an entry a_{ij} in an n\\times n matrix A is the product of its minor and the sign \\left( -1\\right) ^{i+j}: C_{ij}=\\left( -1\\right) ^{i+j}M_{ij}. The n^{2} cofactors form a matrix of cofactors.\n\n\n\nDefinition 4.2: DeterminantsA determinant of an n \\times n matrix can be expanded in terms of (n-1)\\times(n-1) determinants using either a column or a row. Expansion along the i-th column: \\left| A\\right| =a_{1,i}C_{1,i}+a_{2,i}C_{2,i}+a_{3,i}C_{3,i}+\\cdots\n+a_{n,i}C_{n,i}. Expansion along the i-th row: \\left| A\\right| =a_{i,1}C_{i,1}+a_{i,2}C_{i,2}+a_{i,3}C_{i,3}+\\cdots\n+a_{i,n}C_{i,n}. The (n-1)\\times(n-1) determinants can then be recursively further reduced down to 2\\times2 matrices.\n\n\n\nExample 4.2 Find the expansion for the matrix A of the previous Example. Expansion along the first row: \\left|\n\\begin{array}{ccc}\n1 & 2 & 3 \\\\\n2 & 3 & 2 \\\\\n3 & 3 & 4\n\\end{array}\n\\right| =  1  \\left| \\begin{array}{cc}\n3 & 2 \\\\\n3 & 4\n\\end{array} \\right| - 2  \\left| \\begin{array}{cc}\n2 & 2 \\\\\n3 & 4\n\\end{array}  \\right| +3 \\left| \\begin{array}{cc}\n2 & 3 \\\\\n3 & 3\n\\end{array}  \\right| =  6-4 -9 =-7 Expansion along the first column: \\left|\n\\begin{array}{ccc}\n1 & 2 & 3 \\\\\n2 & 3 & 2 \\\\\n3 & 3 & 4\n\\end{array}\n\\right| =  1  \\left| \\begin{array}{cc}\n3 & 2 \\\\\n3 & 4\n\\end{array} \\right| - 2  \\left| \\begin{array}{cc}\n2 & 3 \\\\\n3 & 4\n\\end{array}  \\right| + 3\\left| \\begin{array}{cc}\n2 & 3 \\\\\n3 & 2\n\\end{array}  \\right| =6+2-15=-7 Expansion along the second column: \\left|\n\\begin{array}{ccc}\n1 & 2 & 3 \\\\\n2 & 3 & 2 \\\\\n3 & 3 & 4\n\\end{array}\n\\right| =  - 2  \\left| \\begin{array}{cc}\n2 & 2 \\\\\n3 & 4\n\\end{array}  \\right| +3 \\left| \\begin{array}{cc}\n1 & 3 \\\\\n3 & 4\n\\end{array}  \\right| -3 \\left| \\begin{array}{cc}\n1 & 3 \\\\\n2 & 2\n\\end{array}  \\right| =  -4 -15 +12=-7\n\n\nExample 4.3 Evaluate \\begin{aligned}\n\\left|\n\\begin{array}{cccc}\n1 & 2 & 1 & 3 \\\\\n1 & 1 & 2 & 1 \\\\\n3 & 2 & 0 & 2 \\\\\n2 & 2 & 1 & 0\n\\end{array}\n\\right| & = 1\\left|\n\\begin{array}{ccc}\n1 & 2 & 1 \\\\\n2 & 0 & 2 \\\\\n2 & 1 & 0\n\\end{array}\n\\right| - 1 \\left|\n\\begin{array}{ccc}\n2 & 1 & 3 \\\\\n2 & 0 & 2 \\\\\n2 & 1 & 0\n\\end{array}\n\\right|  + 3 \\left|\n\\begin{array}{ccc}\n2 & 1 & 3 \\\\\n1 & 2 & 1 \\\\\n2 & 1 & 0\n\\end{array}\n\\right| - 2 \\left|\n\\begin{array}{ccc}\n2 & 1 & 3 \\\\\n1 & 2 & 1 \\\\\n2 & 0 & 2 \\end{array}\n\\right| \\\\\n& = 1\\left|\n\\begin{array}{cc}\n0 & 2 \\\\\n1 & 0\n\\end{array}\n\\right| -2  \\left|\\begin{array}{cc}\n2& 2 \\\\\n2 & 0\n\\end{array}\n\\right|  - ... \\text{10 more terms}\n\\end{aligned}\n\n\n\nExample 4.4 Evaluate \\left|\n\\begin{array}{cccc}\n1 & 2 & 1 & 3 \\\\\n0 & 1 & 1 & -2 \\\\\n0 & 0 & 7 & -1 \\\\\n0 & 0 & 3 & 2\n\\end{array}\n\\right| \n\n\nSolutionExpanding along the first column, twice:  \\left|\n\\begin{array}{cccc}\n1 & 2 & 1 & 3 \\\\\n0 & 1 & 1 & -2 \\\\\n0 & 0 & 7 & -1 \\\\\n0 & 0 & 3 & 2\n\\end{array}\n\\right| =\\left|\n\\begin{array}{ccc}\n1 & 1 & -2 \\\\\n0 & 7 & -1 \\\\\n0 & 3 & 2\n\\end{array}\n\\right| =\\left|\n\\begin{array}{cc}\n7 & -1 \\\\\n3 & 2\n\\end{array}\n\\right| =\\allowbreak 17.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determinants</span>"
    ]
  },
  {
    "objectID": "determinants.html#simplification-of-determinants",
    "href": "determinants.html#simplification-of-determinants",
    "title": "4  Determinants",
    "section": "4.2 Simplification of Determinants",
    "text": "4.2 Simplification of Determinants\nThe following rules help to simplify determinants efficiently:\n\nCorollary 4.3: Calculation of Determinants\n\nIf two rows (columns) of A are interchanged to give a matrix B then \\left| B\\right| =-\\left| A\\right|.\nIf two rows (columns) of A are equal then \\left| A\\right| =0\nIf all the entries in any row (column) of A are multiplied by a scalar k, the determinant is also multiplied by k\nIf one row (column) of A is a multiple of another row (column), then \\left|\nA\\right| =0\nIf the matrix B is obtained from A by taking multiple of one row (column) and adding it to another row (column), then \\left| B\\right|\n=\\left| A\\right| .\n\\left| A\\right| =\\left| A^{T}\\right| .\n\n\n\n\nProofThe proof of (1) and (6) is omitted.\n(2) is proved by using (1): If we interchange the two equal columns of {\\rm A}, then we obtain {\\rm A} again, hence |{\\rm A}| remains the same, but according to (1) it should change its sign. This is only possible if |{\\rm A}|=0.\n(3) If B is obtained by multiplying the ith column of A by k: \\left| A\\right| =\\left|\n\\begin{array}{cccc}\na_{1} & b_{1} & c_{1} & \\cdots \\\\\na_{2} & b_{2} & c_{2} &  \\\\\n\\vdots &  &  &  \\\\\na_{i} & b_{i} & c_{i} &  \\\\\n\\vdots &  &  &\n\\end{array}\n\\right| ,\\quad \\left| B\\right| =\\left|\n\\begin{array}{cccc}\na_{1} & b_{1} & c_{1} & \\cdots \\\\\na_{2} & b_{2} & c_{2} &  \\\\\n\\vdots &  &  &  \\\\\nka_{i} & kb_{i} & kc_{i} &  \\\\\n\\vdots &  &  &\n\\end{array}\n\\right| and expand both determinants about their ith rows: \\begin{aligned}\n\\left| A\\right| &=a_{i}C_{i,1}+b_{i}C_{i,2}+\\cdots \\\\\n\\left| B\\right| &=\\left( ka_{i}\\right) C_{i,1}+\\left( kb_{i}\\right)\nC_{i,2}+\\cdots \\\\\n&=k\\left( a_{i}C_{i,1}+b_{i}C_{i,2}+\\cdots \\right) =k\\left| A\\right| .\n\\end{aligned} (4) We use (3) and (2).\n(5) Suppose we add k\\times row j to row i of \\left|\nA\\right| to give \\left| B\\right| : \\left| A\\right| =\\left|\n\\begin{array}{cccc}\na_{1} & b_{1} & c_{1} & \\cdots \\\\\na_{2} & b_{2} & c_{2} &  \\\\\n\\vdots &  &  &  \\\\\na_{i} & b_{i} & c_{i} &  \\\\\n\\vdots &  &  &\n\\end{array}\n\\right| ,\\quad \\left| B\\right| =\\left|\n\\begin{array}{cccc}\na_{1} & b_{1} & c_{1} & \\cdots \\\\\na_{2} & b_{2} & c_{2} &  \\\\\n\\vdots &  &  &  \\\\\na_{i}+ka_{j} & b_{i}+kb_{j} & c_{i}+kc_{j} &  \\\\\n\\vdots &  &  &\n\\end{array}\n\\right| and we now expand \\left| B\\right| by its ith row \\begin{aligned}\n\\left| A\\right| &=a_{i}C_{i,1}+b_{i}C_{i,2}+\\cdots \\\\\n\\left| B\\right| &=\\left( a_{i}+ka_{j}\\right) C_{i,1}+\\left(\nb_{i}+kb_{j}\\right) C_{i,2}+\\cdots \\\\\n&=\\left( a_{i}C_{i,1}+b_{i}C_{i,2}+\\cdots \\right) +k\\left(\na_{j}C_{i,1}+b_{i}C_{j,2}+\\cdots \\right)\n\\end{aligned} =\\left|\n\\begin{array}{cccc}\na_{1} & b_{1} & c_{1} & \\cdots \\\\\na_{2} & b_{2} & c_{2} &  \\\\\n\\vdots &  &  &  \\\\\na_{i} & b_{i} & c_{i} &  \\\\\n\\vdots &  &  &\n\\end{array}\n\\right| +k\\left|\n\\begin{array}{cccc}\na_{1} & b_{1} & c_{1} & \\cdots \\\\\na_{2} & b_{2} & c_{2} &  \\\\\n\\vdots &  &  &  \\\\\na_{j} & b_{j} & c_{j} &  \\\\\n\\vdots &  &  &\n\\end{array}\n\\right| =\\left| A\\right| since rows i and j in the determinant \\left|\n\\begin{array}{cccc}\na_{1} & b_{1} & c_{1} & \\cdots \\\\\na_{2} & b_{2} & c_{2} &  \\\\\n\\vdots &  &  &  \\\\\na_{j} & b_{j} & c_{j} &  \\\\\n\\vdots &  &  &\n\\end{array}\n\\right| are identical, so it is zero (Corollary 4.3, item 4). ◻\n\n\n\n\n\n\n\n\nNoteRemember\n\n\n\nThe rules are similar to, but not the same as, Gaussian elimination; adding k times row 2 to row 1 is OK if the result goes in row 1, and row 2 is left unchanged, but if you replace row 2 by k times row 2 plus row 1, you will change the value of the determinant by a factor of k (item 3 of Corollary 4.3 tells us this). A further difference is that a multiple of a column can be added to another column (not allowed in Gaussian elimination).\n\n\n\nExample 4.5 Evaluate the determinant \\left|\n\\begin{array}{cccc}\n1 & 2 & -1 & 3 \\\\\n2 & 0 & 1 & 1 \\\\\n1 & 3 & 2 & 1 \\\\\n2 & 0 & 0 & 1\n\\end{array}\n\\right| .\n\n\nSolutionFirst expand by the 4th row: = -2\\times \\left|\n\\begin{array}{ccc}\n2 & -1 & 3 \\\\\n0 & 1 & 1 \\\\\n3 & 2 & 1\n\\end{array}\n\\right| +1\\times \\left|\n\\begin{array}{ccc}\n1 & 2 & -1 \\\\\n2 & 0 & 1 \\\\\n1 & 3 & 2\n\\end{array}\n\\right| ,\nnow subtract c_{3} from c_{2} in the 1st , subtract 2\\times\nc_{3} from c_{1} in the 2nd and expand both by their 2nd rows. \\begin{aligned}\n&=-2\\times \\left|\n\\begin{array}{ccc}\n2 & -4 & 3 \\\\\n0 & 0 & 1 \\\\\n3 & 1 & 1\n\\end{array}\n\\right| +1\\times \\left|\n\\begin{array}{ccc}\n3 & 2 & -1 \\\\\n0 & 0 & 1 \\\\\n-3 & 3 & 2\n\\end{array}\n\\right| \\\\\n&=-2\\times \\left( -1\\right) \\left|\n\\begin{array}{cc}\n2 & -4 \\\\\n3 & 1\n\\end{array}\n\\right| +1\\times \\left( -1\\right) \\left|\n\\begin{array}{cc}\n3 & 2 \\\\\n-3 & 3\n\\end{array}\n\\right| \\\\\n&=-2\\times \\left( -1\\right) \\times 14+1\\times \\left( -1\\right) \\times 15=13.\n\\end{aligned}\n\n\n\nExample 4.6 Evaluate the determinant \\left|\n\\begin{array}{rrrr}\n4 & 1 & 3 & -1 \\\\\n2 & 0 & 1 & 2 \\\\\n1 & -1 & 2 & 5 \\\\\n2 & 1 & 3 & 1\n\\end{array}\n\\right|.\n\n\nSolutionSubtract r_{4} from r_{1}; then add c_{1} to c_{4}, then expand about the 1st row:  r_{1}-r_{4}\\rightarrow r_{1}: \\quad \\left|\n\\begin{array}{rrrr}\n2 & 0 & 0 & -2 \\\\\n2 & 0 & 1 & 2 \\\\\n1 & -1 & 2 & 5 \\\\\n2 & 1 & 3 & 1\n\\end{array}\n\\right| , c_{4}+c_{1}\\rightarrow c_{4}: \\quad \\left|\n\\begin{array}{rrrr}\n2 & 0 & 0 & 0 \\\\\n2 & 0 & 1 & 4 \\\\\n1 & -1 & 2 & 6 \\\\\n2 & 1 & 3 & 3\n\\end{array}\n\\right| =2\\times \\left|\n\\begin{array}{rrr}\n0 & 1 & 4 \\\\\n-1 & 2 & 6 \\\\\n1 & 3 & 3\n\\end{array}\n\\right|  add r_{3} to r_{2}: r_{2}+r_{3}\\rightarrow r_{2}: \\quad 2\\times \\left|\n\\begin{array}{rrr}\n0 & 1 & 4 \\\\\n0 & 5 & 9 \\\\\n1 & 3 & 3\n\\end{array}\n\\right| =2\\times 1\\times \\left|\n\\begin{array}{rr}\n1 & 4 \\\\\n5 & 9\n\\end{array}\n\\right| =2\\times 1\\times \\left( -11\\right) =-22.\n\n\n\nShow that D=\\left|\n\\begin{array}{cccc}\n1 & a & a^{2} & b+c+d \\\\\n1 & b & b^{2} & c+d+a \\\\\n1 & c & c^{2} & d+a+b \\\\\n1 & d & d^{2} & a+b+c\n\\end{array}\n\\right| =0 for all values of a,b,c,d.\n\n\nSolve for x \\left|\n\\begin{array}{ccc}\n1 & 2 & x \\\\\nx & 0 & 3 \\\\\n1 & x & 2\n\\end{array}\n\\right| =0\n\n\nCorollary 4.4If A is either lower or upper triangular, then \\det(A) =a_{1,1}a_{2,2}\\cdots a_{n,n}, is the product of its diagonal entries.\n\n\n\nExample 4.7 \\left|\\begin{array}{rrr}\n1 & 7 & -2 \\\\\n0 & 14 & 32 \\\\\n0 & 0 & -2\n\\end{array}\\right| = 1\\times 14\\times (-2)=-28.\nWe can see this by expanding down the first column:\n1\\left|\\begin{array}{rr} 14 & 32 \\\\\n0 & -2\n\\end{array}\\right|=1(14\\times (-2))=-28.\n\n\nTheorem 4.5: Product RuleFor any two matrices A and B, \\det (AB) = \\det (A) \\det (B).\n\n\nThe general proof of this result uses elementary matrices. Although it is a key result, a proof will not be given (See Poole, p. 268).\nNote that \\textrm{det}(A+B)\\neq \\textrm{det}(A)+\\textrm{det}(B).\n\nCorollary 4.6If A is n\\times n, we have, by Corollary 4.3 \\begin{aligned}\n\\det \\left( kA\\right) &= \\det \\left( kI \\  A\\right) \\\\\n&=\n\\det \\left( kI\\right)  \\det \\left( A\\right) \\\\\n&= k^{n}\n\\det \\left( A\\right) .\n\\end{aligned}\n\n\n\nCorollary 4.7When A is non-singular (i.e., invertible) \\det \\left(A^{-1}\\right) =\\dfrac{1}{\\det \\left( A\\right) }.\n\n\n\nProof1=\\textrm{det(I)}=\\textrm{det}(AA^{-1})=\\textrm{det}(A)\\textrm{det}(A^{-1}). ◻\n\n\n\nExample 4.8 Let A be an n\\times n matrix with \\textrm{det}(A)=5. Then \\det \\left( A^{4}\\right)\n=5^{4} \\det \\left( A^{-1}\\right) =\\frac{1}{5}, \\det \\left( 3A^{2}\\right) =3^{n}\\times 25 .\n\n\nCorollary 4.8The matrix A has an inverse if, and only if, \\left| A\\right| \\neq 0.\n\n\n\nProofFirst, we show that if the matrix A has an inverse, then |A|\\neq 0. If {\\rm A} has an inverse then |{\\rm A}{\\rm A^{-1}}| = |{\\rm A}||{\\rm A^{-1}}| =| I | = 1 and hence |{\\rm A}| \\neq 0.\nNext, we show the reverse. Assume that |A|\\neq 0. We want to show that A has an inverse.\nBy using elementary row operations, that is by either (i) adding a multiple of one row to another or (ii) interchanging rows we can reduce A to upper triangular form—call this U. Operations of type i) do not affect the value of \\left| A\\right| while operations of type (ii) may cause a change of sign. Hence, \\left| A\\right| =\\pm \\left| U\\right|, and so, in particular |A|\\neq0.\n\\begin{aligned}\nU&=\\left[\n\\begin{array}{cccc}\nu_{11} & u_{12} & \\cdots & u_{1n} \\\\\n0 & u_{22} &  & u_{2n} \\\\\n0 & 0 & \\ddots &  \\\\\n0 & 0 &  & u_{nn}\n\\end{array}\n\\right],\\\\\n\\left| U\\right| &= u_{11}u_{22}\\cdots u_{nn}\\neq 0,\n\\end{aligned} so u_{ij}\\neq 0 for all i,j=1,2,\\dots,n. Hence A has n pivots and is invertible. ◻\n\n\n\nCorollary 4.9If A is a square matrix, the system A\\mathbf{x=0} has non-trivial solutions if, and only if, \\left| A\\right| =0 (so A is a singular matrix).\n\n\n\n\n\n\n\n\nNoteNote:\n\n\n\nVarious authors define non-singular matrices to be either\n\ninvertible matrices\nmatrices A for which \\det(A)\\neq 0.\nmatrices for which A\\mathbf{x}=\\mathbf{0} has only the trivial (zero) solution.\n\nThe above theorem shows that they are all equivalent definitions.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determinants</span>"
    ]
  },
  {
    "objectID": "determinants.html#the-adjoint-matrix",
    "href": "determinants.html#the-adjoint-matrix",
    "title": "4  Determinants",
    "section": "4.3 The Adjoint Matrix",
    "text": "4.3 The Adjoint Matrix\n\nDefinition 4.10The transpose of the matrix of cofactors of {\\rm A} is called the adjugate or adjoint of {\\rm A} and denoted by \\rm{adj}({\\rm A}) =(\\text{Cofactors})^{T}.\n\n\n\nTheorem 4.11: Inverse and AdjointIf \\det({\\rm A}) \\neq 0 then {\\rm A}^{-1} =\\frac{1}{\\det({\\rm A})} \\rm{adj}({\\rm A}).\n\n\n\nProofLet {\\rm C} be the matrix of cofactors of {\\rm A}. \\begin{aligned}\n({\\rm A} \\  \\rm{adj}({\\rm A}))_{ij} &= \\sum_{k=1}^{n} a_{ik} ({\\rm C}^{T})_{kj} =  \\sum_{k=1}^{n} a_{ik} {\\rm C}_{jk} \\\\\n&= \\left\\{\n\\begin{array}{cc}\n0 & i \\neq j \\\\\n\\det(\\rm{A}) & i=j\n\\end{array}\n\\right. \\\\\n\\Rightarrow {\\rm A} \\  \\rm{adj}({\\rm A}) &= \\det(\\rm{A}) {\\rm I}\n\\end{aligned} Here, the result for i\\neq j is obtained from the fact that \\sum_{k=1}^{n} a_{ik} {\\rm C}_{jk} is the expansion of the determinant of a matrix with two identical rows. For the case i=j, however, this is exactly the definition of the expansion of a determinant along row i of {\\rm A}. So \\begin{aligned}\n&& A\\textrm{adj}(A)=\\det(A)I \\\\\n&\\Leftrightarrow& \\frac{A \\textrm{adj}(A)}{\\det(A)}=I.\n\\end{aligned} Multiplying A^{-1} from the left gives \\frac{1}{\\det(A)} \\textrm{adj}(A)=A^{-1}. ◻\n\n\n\nExample 4.9 Find the adjoint of the matrix {\\rm A}=\\left[\n\\begin{array}{ccc}\n1 & -1 & 2\\\\\n3 & 2 & -1\\\\\n2 & -1 & 2\\\\\n\\end{array}\n\\right]\n\n\nSolution(i) Matrix of minors is \\left[\n\\begin{array}{rrr}\n3 & 8 & -7\\\\\n0 & -2 & 1\\\\\n-3 & -7 & 5\n\\end{array}\n\\right]. (ii) The matrix of cofactors is \\left[\n\\begin{array}{rrr}\n3 & -8 & -7\\\\\n0 & -2 & -1\\\\\n-3 & 7 & 5\n\\end{array}\n\\right]. (iii) Adjoint is {\\rm adj}({\\rm A}) = \\left[\n\\begin{array}{rrr}\n3 & 0 & -3\\\\\n-8 & -2 & 7\\\\\n-7 & -1 & 5\n\\end{array}\n\\right].\nNotice that {\\rm A} \\,  {\\rm adj}({\\rm A}) = \\left[\n\\begin{array}{rrr}\n1 & -1 & 2\\\\\n3 & 2 & -1\\\\\n2 & -1 & 2\\\\\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{rrr}\n3 & 0 & -3\\\\\n-8 & -2 & 7\\\\\n-7 & -1 & 5\n\\end{array}\n\\right] = \\left[\n\\begin{array}{rrr}\n-3 & 0 & 0\\\\\n0  & -3 & 0\\\\\n0 & 0 & -3\n\\end{array}\n\\right] = -3 I = det(A) I.\n\n\n\nExample 4.10 Use the adjoint matrix to find the inverse of the matrix A=\\left(\\begin{array}{rrr}1 & -1 & 2 \\\\ 3 & 2 & -1 \\\\ 2 & -1 & 2\\end{array}\\right) in the previous example.\n\n\nSolutionThe determinant of the matrix A is \\begin{aligned}\n&&1\\left|\\begin{array}{rr}2 & -1 \\\\ -1 & 2\\end{array}\\right|+\n1\\left|\\begin{array}{rr}3 & -1 \\\\ 2 & 2\\end{array}\\right|\n-2\\left|\\begin{array}{rr}3 & 2 \\\\ 2 & -1\\end{array}\\right| \\\\\n&= (4-1)+(6+2)+2(-3-4)\\\\\n&= 3+8 -14\\\\\n&=-3.\n\\end{aligned} Thus, the inverse is A^{-1}=\\frac{-1}{3}\\left(\\begin{array}{rrr}\n3 & 0 & -3 \\\\\n-8 & -2 & 7 \\\\\n-7 & -1 & 5\n\\end{array}\\right).\n\n\n\nExample 4.11 Find the inverse of A=\\left(\\begin{array}{rrr}\n2 & 3 & 4 \\\\\n5 & 6 & 7 \\\\\n8 & 9 & 1\n\\end{array}\\right).\n\n\nSolution\\begin{aligned}\n\\det(A) &= 2\\left|\\begin{array}{rr}\n6 & 7 \\\\\n9 & 1\n\\end{array}\\right|\n-3\\left|\\begin{array}{rr}\n5 & 7 \\\\\n8 & 1\n\\end{array}\\right|\n+4\\left|\\begin{array}{rr}\n5 & 6 \\\\\n8 & 9\n\\end{array}\\right| \\\\\n&= 2(6-63)-3(5-56)+4(45-48) \\\\\n&= 2(-57)-3(-51)+4(-3) \\\\\n&= -114+153-12 \\\\\n&= 27.\n\\end{aligned}\n\\begin{aligned}\nA^{-1} &= \\frac{1}{27}\n\\left(\\begin{array}{rrr}\n-57 & 33 & -3 \\\\\n51 & -30 & 6 \\\\\n-3 & 6 & -3\n\\end{array}\\right) \\\\\n&= \\frac{1}{9}\\left(\\begin{array}{rrr}\n-19 & 11 & -1 \\\\\n17 & -10 & 2 \\\\\n-1 & 2 & -1\n\\end{array}\\right).\n\\end{aligned}\n\n\n\nExample 4.12 In the 2\\times 2 case, if \\begin{aligned}\nA&=\\left[\n\\begin{array}{cc}\na & b \\\\\nc & d\n\\end{array}\n\\right] \\ \\Rightarrow  \\  {\\rm adj}({\\rm A}) = \\left[\\begin{array}{rr}\nd & -b \\\\\n-c & a\n\\end{array}\n\\right]  \\\\\n\\Rightarrow A^{-1} &= \\frac{1}{ad-bc}\\left[\n\\begin{array}{rr}\nd & -b \\\\\n-c & a\n\\end{array}\n\\right].  \n\\end{aligned}",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determinants</span>"
    ]
  },
  {
    "objectID": "eigenvalues.html",
    "href": "eigenvalues.html",
    "title": "5  Eigenvalues and Eigenvectors",
    "section": "",
    "text": "5.1 Introduction to Eigenvalues and Eigenvectors\nConsider an elastic body, e.g., a rubber ball. If we compress the ball in one direction, it will be elongated in the plane perpendicular to that direction. The figure below shows, in a cross-section, how individual points on the surface move under the compression.\nThere are some vectors in these plots that don’t change direction, but only their length under the deformation. We can find those by superimposing the two plots:\nThese directions are characteristic of the deformation, as is the factor of elongation/compression along these directions. These directions can be found mathematically as the eigenvectors of a matrix, the deformation matrix, which maps the original vectors to the vectors of the deformed ball. The eigenvalues are the factors of compression/elongation along these directions. The word \"eigen\" is German for \"own\" and was likely introduced by David Hilbert (also known for Hilbert spaces).\nEigenvalues and eigenvectors play an essential role in mathematics, physics and engineering. The stability of an equilibrium of a dynamical system is determined by the eigenvalues of a matrix that describes the linearised system at the equilibrium point. The values of a measurement in quantum mechanics are the eigenvalues of an operator. The principal axes of a rigid body are the eigenvectors of the moment of inertia tensor.\nNotice that \\left[\\begin{array}{c} \\alpha \\\\ 0 \\end{array}\\right] and \\left[\\begin{array}{c} 0 \\\\ \\alpha \\end{array}\\right] are eigenvectors for any \\alpha \\neq 0.\nIn general, if \\mathbf{v} is an eigenvector of A, then so is \\alpha \\mathbf{v} for any nonzero scalar \\alpha.\nTherefore, in the above Example, E_{5} = \\left\\{t\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right]\\right\\}, where t \\in \\mathbb{R}.\nIn the above Example, \\lambda = 1 has algebraic multiplicity 2 and geometric multiplicity 1. \\lambda = 2 has algebraic multiplicity 1 and geometric multiplicity 1.\nNote that diagonal matrices are a special case of Corollary 5.6.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "eigenvalues.html#introduction-to-eigenvalues-and-eigenvectors",
    "href": "eigenvalues.html#introduction-to-eigenvalues-and-eigenvectors",
    "title": "5  Eigenvalues and Eigenvectors",
    "section": "",
    "text": "Figure 5.1: Effect of a deformation on the points (vectors) in an elastic sphere.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: There are directions in which the vectors only change their length.\n\n\n\n\n\n\nDefinition 5.1: Eigenvalue and EigenvectorLet A be an n \\times n matrix. A scalar \\lambda is called an eigenvalue of A if there is a nonzero vector \\mathbf{x} such that A\\mathbf{x} = \\lambda \\mathbf{x}. Such a vector \\mathbf{x} is called an eigenvector of A corresponding to \\lambda.\n\n\n\nExample 5.1 Suppose A\\left[\\begin{array}{c} x \\\\ y \\end{array}\\right] = \\left[\\begin{array}{c} 2x \\\\ 0 \\end{array}\\right]. Then A\\left[\\begin{array}{c} 1 \\\\ 0 \\end{array}\\right] = \\left[\\begin{array}{c} 2 \\\\ 0 \\end{array}\\right] = 2\\left[\\begin{array}{c} 1 \\\\ 0 \\end{array}\\right] so 2 is an eigenvalue and \\left[\\begin{array}{c} 1 \\\\ 0 \\end{array}\\right] a corresponding eigenvector. Also, A\\left[\\begin{array}{c} 0 \\\\ 1 \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ 0 \\end{array}\\right] = 0\\left[\\begin{array}{c} 0 \\\\ 1 \\end{array}\\right], so 0 is an eigenvalue and \\left[\\begin{array}{c} 0 \\\\ 1 \\end{array}\\right] a corresponding eigenvector.\n\n\n\nExample 5.2 Show that 5 is an eigenvalue of A = \\left[\\begin{array}{cc} 1 & 2 \\\\ 4 & 3 \\end{array}\\right] and determine all eigenvectors corresponding to this eigenvalue.\n\n\nSolutionWe must show that there is a nonzero vector \\mathbf{x} such that A\\mathbf{x} = 5\\mathbf{x}, which is equivalent to the equation (A - 5I)\\mathbf{x} = \\mathbf{0}. We compute the nullspace by:\n\\left[A - 5I | \\mathbf{0}\\right] = \\left[\\begin{array}{cc|c} -4 & 2 & 0 \\\\ 4 & -2 & 0 \\end{array}\\right] \\xrightarrow{R_{2} + R_{1}} \\left[\\begin{array}{cc|c} -4 & 2 & 0 \\\\ 0 & 0 & 0 \\end{array}\\right]\nThus \\mathbf{x} = \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\end{array}\\right] \\in {\\rm Null}(A-5I) satisfies -4x_{1} + 2x_{2} = 0, or x_{2} = 2x_{1}.\nThus, A\\mathbf{x} = 5\\mathbf{x} has a nontrivial solution of the form \\mathbf{x} = \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\end{array}\\right] = \\left[\\begin{array}{c} x_{1} \\\\ 2x_{1} \\end{array}\\right] = x_{1}\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right], so 5 is an eigenvalue of A and the corresponding eigenvectors are the nonzero multiples of \\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right].\n\n\n\nDefinition 5.2: EigenspaceLet A be an n \\times n matrix and let \\lambda be an eigenvalue of A. The collection of all eigenvectors corresponding to \\lambda, together with the zero vector, is called the eigenspace of \\lambda and is denoted by E_{\\lambda }.\n\n\n\n\nTheorem 5.3: Characteristic EquationLet A be an n \\times n matrix. Then \\lambda is an eigenvalue of A if and only if |A-\\lambda I_{n}| = 0 (or det(A - \\lambda I_{n}) = 0).\n\n\n\nProofSuppose that \\lambda is an eigenvalue of A. Then A\\mathbf{v} = \\lambda \\mathbf{v} for some nonzero \\mathbf{v} \\in \\mathbb{R}^{n}. This is equivalent to A\\mathbf{v} = \\lambda I_{n} \\mathbf{v} or (A - \\lambda I_{n})\\mathbf{v} = \\mathbf{0}. But this means that \\mathbf{v} is a nonzero solution to the homogeneous system of equations defined by the matrix A - \\lambda I_{n}. This means A - \\lambda I_{n} is singular, and so |A - \\lambda I_{n}| = 0.\nConversely, if |A - \\lambda I_{n}| = 0 then A - \\lambda I_{n} is singular, and so the system of equations defined by A - \\lambda I_{n} has nonzero solutions. Hence there exists a nonzero \\mathbf{v} \\in \\mathbb{R}^{n} with (A - \\lambda I_{n})\\mathbf{v} = \\mathbf{0}, which is equivalent to A \\mathbf{v} = \\lambda \\mathbf{v}, and so \\lambda is an eigenvalue of A. ◻\n\n\n\nDefinition 5.4: Characteristic Equation/PolynomialFor an n \\times n matrix A, the equation |A - \\lambda I_{n}| = 0 is called the characteristic equation of A, and |A - \\lambda I_{n}| is called the characteristic polynomial of A.\n\n\n\nExample 5.3 Find the eigenvalues and the corresponding eigenvectors of A = \\left[\\begin{array}{cc}\n        1 & 2 \\\\\n        4 & -1\n    \\end{array}\\right].\n\nSolutionThe characteristic polynomial is\n\\begin{alignedat}{2}\n        |A - \\lambda I_{2}| &= \\left|\\begin{array}{cc}\n            1 - \\lambda & 2 \\\\\n            4 & -1-\\lambda\n        \\end{array}\\right| \\\\\n        {} &= (1-\\lambda )(-1-\\lambda ) - 8 \\\\\n        {} &= \\lambda ^{2} - 9 \\\\\n        {} &= (\\lambda + 3)(\\lambda - 3). \\\\\n    \\end{alignedat} Hence the eigenvalues of A are the roots of (\\lambda + 3)(\\lambda - 3) = 0; that is \\lambda _{1} = -3 and \\lambda _{2} = 3.\nTo find the eigenvectors corresponding to \\lambda _{1} = -3, we find the nullspace of A - (-3)I_{2} = \\left[\\begin{array}{cc} 4 & 2 \\\\ 4 & 2 \\end{array}\\right]\nRow reduction produces \\left[A+3I_{2} | \\mathbf{0}\\right] = \\left[\\begin{array}{cc|c} 4 & 2 & 0 \\\\ 4 & 2 & 0 \\end{array}\\right] \\xrightarrow{R_{2} - R_{1}} \\left[\\begin{array}{cc|c} 4 & 2 & 0 \\\\ 0 & 0 & 0 \\end{array}\\right]\nThus \\mathbf{x} \\in {\\rm Null}(A+3I_{2}) if and only if 4x_{1} + 2x_{2} = 0.\nSetting the free variable x_{2} = t, we see that x_{1} = -\\frac{1}{2}t. We take \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\end{array}\\right] = \\left[\\begin{array}{c} -1 \\\\ 2 \\end{array}\\right] be our eigenvector; or indeed any nonzero multiple of \\left[\\begin{array}{c} -1 \\\\ 2 \\end{array}\\right].\nTo find the eigenvectors corresponding to \\lambda _{2} = 3, we find the nullspace of A - 3I_{2} by row reduction: \\left[A - 3I_{2} | \\mathbf{0}\\right] = \\left[\\begin{array}{cc|c} -2 & 2 & 0 \\\\ 4 & -4 & 0 \\end{array}\\right] \\xrightarrow{R_{2} + 2R_{1}} \\left[\\begin{array}{cc|c} -2 & 2 & 0 \\\\ 0 & 0 & 0 \\end{array}\\right]\nSo \\mathbf{x} = \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\end{array}\\right] \\in {\\rm Null}(A - 3I_{2}) if and only if -2x_{1} + 2x_{2} = 0. Setting the free variable x_{2} = t, we find x_{1} = t. We take \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\end{array}\\right] = \\left[\\begin{array}{c} 1 \\\\ 1 \\end{array}\\right] to be our eigenvector.\n\n\n\n\nExample 5.4 Find the eigenvalues and the corresponding eigenvectors of A = \\left[\\begin{array}{ccc}\n            3 & 2 & 2 \\\\\n            1 & 4 & 1 \\\\\n            -2 & -4 & -1\n        \\end{array}\\right].\n\n\nSolutionThe characteristic equation is \\begin{alignedat}{2}\n        0 = |A - \\lambda I| &= \\left|\\begin{array}{ccc}\n            3-\\lambda & 2 & 2 \\\\\n            1 & 4-\\lambda & 1 \\\\\n            -2 & -4 & -1-\\lambda\n        \\end{array}\\right| \\\\\n        {} &= (3-\\lambda ) \\left|\\begin{array}{cc} 4-\\lambda & 1 \\\\ -4 & -1-\\lambda \\end{array}\\right| - 2 \\left|\\begin{array}{cc} 1 & 1 \\\\ -2 & -1-\\lambda \\end{array}\\right| + 2 \\left|\\begin{array}{cc} 1 & 4-\\lambda \\\\ -2 & -4\\end{array}\\right| \\\\\n        {} &= (3-\\lambda ) \\{(4-\\lambda )(-1-\\lambda )+4\\} - 2\\{(-1-\\lambda )+2\\} + 2\\{-4+2(4-\\lambda )\\} \\\\\n        {} &= -\\lambda ^3 + 6\\lambda ^2 - 11\\lambda + 6 \\\\\n        {} &= (\\lambda -1)(-\\lambda ^2 + 5\\lambda - 6) \\\\\n        {} &= (\\lambda -1)\\{-(\\lambda ^2 - 5\\lambda + 6)\\} \\\\\n        {} &= (\\lambda -1)\\{-(\\lambda - 2)(\\lambda - 3)\\} \\\\\n    \\end{alignedat}\nHence, the eigenvalues are \\lambda _{1} = 1, \\lambda _{2} = 2 and \\lambda _{3} = 3.\nFor \\lambda _{1}=1, we compute \\left[A - I | \\mathbf{0}\\right] = \\left[\\begin{array}{ccc | c} \\textcircled{\\raisebox{-0.9pt}{2}} & 2 & 2 & 0 \\\\ 1 & 3 & 1 & 0 \\\\ -2 & -4 & -2 & 0 \\end{array}\\right] \\xrightarrow{\\begin{subarray}{c} R_{2} - \\frac{1}{2}R_{1} \\\\ R_{3} + R_{1} \\end{subarray}} \\left[\\begin{array}{ccc | c} 2 & 2 & 2 & 0 \\\\ 0 & \\textcircled{\\raisebox{-0.9pt}{2}} & 0 & 0 \\\\ 0 & -2 & 0 & 0 \\end{array}\\right]  \\xrightarrow{R_{3} + R_{2}} \\left[\\begin{array}{ccc | c} \\textcircled{\\raisebox{-0.9pt}{2}} & 2 & 2 & 0 \\\\ 0 & \\textcircled{\\raisebox{-0.9pt}{2}} & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{array}\\right] from which it follows that an eigenvector \\mathbf{x} = \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{array}\\right] satisfies x_{2} = 0 and x_{3} = -x_{1}. We take \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{array}\\right] = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ -1 \\end{array}\\right] to be our eigenvector .\nFor \\lambda _{2} = 2, we compute \\left[A - 2I | \\mathbf{0}\\right] = \\left[\\begin{array}{ccc | c} \\textcircled{\\raisebox{-0.9pt}{1}} & 2 & 2 & 0 \\\\ 1 & 2 & 1 & 0 \\\\ -2 & -4 & -3 & 0 \\end{array}\\right] \\xrightarrow{\\begin{subarray}{c} R_{2} - R_{1} \\\\ R_{3} + 2R_{1} \\end{subarray}} \\left[\\begin{array}{ccc | c} 1 & 2 & 2 & 0 \\\\ 0 & 0 & \\textcircled{\\raisebox{-0.9pt}{-1}} & 0 \\\\ 0 & 0 & 3 & 0 \\end{array}\\right] \\xrightarrow{R_{3} + 3R_{2}} \\left[\\begin{array}{ccc | c} \\textcircled{\\raisebox{-0.9pt}{1}} & 2 & 2 & 0 \\\\ 0 & 0 & \\textcircled{\\raisebox{-0.9pt}{-1}} & 0 \\\\ 0 & 0 & 0 & 0 \\end{array}\\right] which gives an eigenvector \\mathbf{x} = \\left[\\begin{array}{c} -2 \\\\ 1 \\\\ 0 \\end{array}\\right].\nFor \\lambda _{3} = 3, we compute \\left[A - 3I | \\mathbf{0}\\right] = \\left[\\begin{array}{ccc | c} 0 & 2 & 2 & 0 \\\\ 1 & 1 & 1 & 0 \\\\ -2 & -4 & -4 & 0 \\end{array}\\right] \\xrightarrow{R_{2} \\leftrightarrow R_{1}} \\left[\\begin{array}{ccc | c} \\textcircled{\\raisebox{-0.9pt}{1}} & 1 & 1 & 0 \\\\ 0 & 2 & 2 & 0 \\\\ -2 & -4 & -4 & 0 \\end{array}\\right]  \\xrightarrow{R_{3} + 2R_{1}} \\left[\\begin{array}{ccc | c} 1 & 1 & 1 & 0 \\\\ 0 & \\textcircled{\\raisebox{-0.9pt}{2}} & 2 & 0 \\\\ 0 & -2 & -2 & 0 \\end{array}\\right] \\xrightarrow{R_{3} + R_{2}} \\left[\\begin{array}{ccc | c} 1 & 1 & 1 & 0 \\\\ 0 & 2 & 2 & 0 \\\\ 0 & 0 & 0 & 0 \\end{array}\\right] which gives an eigenvector \\mathbf{x} = \\left[\\begin{array}{c} 0 \\\\ -1 \\\\ 1 \\end{array}\\right].\n\n\n\nExample 5.5 Find the eigenvalues and the corresponding eigenspaces of A = \\left[\\begin{array}{ccc}\n        0 & 1 & 0 \\\\\n        0 & 0 & 1 \\\\\n        2 & -5 & 4\n    \\end{array}\\right]\n\n\nSolutionThe characteristic equations is\n\\begin{alignedat}{2}\n        0 = |A - \\lambda I| &= \\left|\\begin{array}{ccc}\n            -\\lambda & 1 & 0 \\\\\n            0 & -\\lambda & 1 \\\\\n            2 & -5 & 4-\\lambda\n        \\end{array}\\right| \\\\\n        {} &= -\\lambda \\left|\\begin{array}{cc} -\\lambda & 1 \\\\ -5 & 4-\\lambda \\end{array}\\right| -  \\left|\\begin{array}{cc} 0 & 1 \\\\ 2 & 4-\\lambda \\end{array}\\right| \\\\\n        {} &= -\\lambda (\\lambda ^2-4\\lambda +5)-(-2) \\\\\n        {} &= -\\lambda ^3 + 4\\lambda ^2 - 5\\lambda + 2 \\\\\n        {} &= (\\lambda -1)(-\\lambda ^2 + 3\\lambda - 2) \\\\\n        {} &= -(\\lambda -1)^2(\\lambda -2) \\\\\n    \\end{alignedat}\nHence, the eigenvalues are \\lambda _{1} = \\lambda _{2} = 1 and \\lambda _{3} = 2.\nTo find the eigenvectors corresponding to \\lambda _{1} = \\lambda _{2} = 1, we compute \\left[A - I | \\mathbf{0}\\right] = \\left[\\begin{array}{ccc | c} \\textcircled{\\raisebox{-0.9pt}{-1}} & 1 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 2 & -5 & 3 & 0 \\end{array}\\right] \\xrightarrow{R_{3} + 2R_{1}} \\left[\\begin{array}{ccc | c} -1 & 1 & 0 & 0 \\\\ 0 & \\textcircled{\\raisebox{-0.9pt}{-1}} & 1 & 0 \\\\ 0 & -3 & 3 & 0 \\end{array}\\right] \\xrightarrow{R_{3} - 3R_{2}} \\left[\\begin{array}{ccc | c} -1 & 1 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{array}\\right]\nThus, \\mathbf{x} = \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{array}\\right] is in the eigenspace E_{1} if and only if -x_{1} + x_{2} = 0 and -x_{2} + x_{3} = 0. Setting the free variable x_{3} = t, we see that x_{1} = t and x_{2} = t, from which it follows that E_{1} = \\left\\{\\left[\\begin{array}{c} t \\\\ t \\\\ t \\end{array}\\right]\\right\\} = \\left\\{t \\left[\\begin{array}{c} 1 \\\\ 1 \\\\ 1 \\end{array}\\right]\\right\\} = {\\rm span}\\left(\\left[\\begin{array}{c} 1 \\\\ 1 \\\\ 1 \\end{array}\\right]\\right)\nTo find the eigenvectors correspond to \\lambda _{3} = 2, we find the nullspace of A - 2I by row reduction: \\left[A - 2I | \\mathbf{0}\\right] = \\left[\\begin{array}{ccc | c} \\textcircled{\\raisebox{-0.9pt}{-2}} & 1 & 0 & 0 \\\\ 0 & -2 & 1 & 0 \\\\ 2 & -5 & 2 & 0 \\end{array}\\right] \\xrightarrow{R_{3} + R_{1}} \\left[\\begin{array}{ccc | c} -2 & 1 & 0 & 0 \\\\ 0 & \\textcircled{\\raisebox{-0.9pt}{-2}} & 1 & 0 \\\\ 0 & -4 & 2 & 0 \\end{array}\\right]  \\xrightarrow{R_{3} - 2R_{2}} \\left[\\begin{array}{ccc | c} -2 & 1 & 0 & 0 \\\\ 0 & -2 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{array}\\right]\nSo \\mathbf{x} = \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{array}\\right] is in the eigenspace E_{2} if and only if -2x_{1} + x_{2} = 0 and -2x_{2} + x_{3} = 0. Setting the free variable x_{3} = t, we have E_{2} = \\left\\{\\left[\\begin{array}{c} \\frac{1}{4}t \\\\ \\frac{1}{2}t \\\\ t \\end{array}\\right]\\right\\} = \\left\\{t \\left[\\begin{array}{c} \\frac{1}{4} \\\\ \\frac{1}{2} \\\\ 1 \\end{array}\\right]\\right\\} = {\\rm span}\\left(\\left[\\begin{array}{c} \\frac{1}{4} \\\\ \\frac{1}{2} \\\\ 1 \\end{array}\\right]\\right) = {\\rm span}\\left(\\left[\\begin{array}{c} 1 \\\\ 2 \\\\ 4 \\end{array}\\right]\\right)\n\n\n\nDefinition 5.5: Algebraic and Geometric MultiplicityThe algebraic multiplicity of an eigenvalue is its multiplicity as a root of the characteristic equation.\nThe geometric multiplicity of an eigenvalue \\lambda is {\\rm dim} (E_{\\lambda }), the dimension of its corresponding eigenspace.\n\n\n\n\nCorollary 5.6The eigenvalues of a triangular matrix are the entries on its main diagonal.\n\n\n\nExample 5.6 Let A = \\left[\\begin{array}{cccc} 2 & 0 & 0 & 0 \\\\ -1 & 1 & 0 & 0 \\\\ 3 & 0 & 3 & 0 \\\\ 5 & 7 & 4 & -2 \\end{array}\\right] The characteristic polynomial is: \\begin{alignedat}{2}\n        |A - \\lambda I| &= \\left|\\begin{array}{cccc} 2-\\lambda & 0 & 0 & 0 \\\\ -1 & 1-\\lambda & 0 & 0 \\\\ 3 & 0 & 3-\\lambda & 0 \\\\ 5 & 7 & 4 & -2-\\lambda \\end{array}\\right| \\\\\n        {} &= (2-\\lambda )\\left|\\begin{array}{ccc} 1-\\lambda & 0 & 0 \\\\ 0 & 3-\\lambda & 0 \\\\ 7 & 4 & -2-\\lambda \\end{array}\\right| \\\\\n        {} &= (2-\\lambda )(1-\\lambda )\\left|\\begin{array}{cc} 3-\\lambda & 0 \\\\ 4 & -2-\\lambda \\end{array}\\right| \\\\\n        {} &= (2-\\lambda )(1-\\lambda )(3-\\lambda )(-2-\\lambda )\n    \\end{alignedat}\nHence, the eigenvalues are \\lambda _{1}=2, \\lambda _{2}=1, \\lambda _{3}=3, \\lambda _{4}=-2.\n\n\n\nTheorem 5.7Let A be an n \\times n matrix and let \\lambda _{1}, \\lambda _{2}, ..., \\lambda _{m} be distinct eigenvalues of A with corresponding eigenvectors \\mathbf{v_{1}}, \\mathbf{v_{2}}, ..., \\mathbf{v_{m}}. Then \\mathbf{v_{1}}, \\mathbf{v_{2}}, ..., \\mathbf{v_{m}} are linearly independent.\n\n\n\nProofWe prove this by contradiction.\nSuppose \\mathbf{v_{1}}, \\mathbf{v_{2}}, ..., \\mathbf{v_{m}} are linearly dependent. Let \\mathbf{v_{k+1}} be the first of the vectors \\mathbf{v_{i}} that can be expressed as a linear combination of the previous ones. In other words, \\mathbf{v_{1}}, \\mathbf{v_{2}}, ..., \\mathbf{v_{k}} are linearly independent, but there are c_{1}, c_{2}, ..., c_{k} such that \\mathbf{v_{k+1}} = c_{1}\\mathbf{v_{1}} + c_{2}\\mathbf{v_{2}} + ... + c_{k}\\mathbf{v_{k}} \\qquad (1)\nMultiplying both sides of Equation (1) by A from left and using the fact that A\\mathbf{v_{i}} = \\lambda _{i}\\mathbf{v_{i}} for each i, we have \\begin{alignedat}{2}\n        \\lambda _{k+1}\\mathbf{v_{k+1}} = A\\mathbf{v_{k+1}} &= A(c_{1}\\mathbf{v_{1}} + c_{2}\\mathbf{v_{2}} + ... + c_{k}\\mathbf{v_{k}}) \\\\\n        {} &= c_{1}A\\mathbf{v_{1}} + c_{2}A\\mathbf{v_{2}} + ... + c_{k}A\\mathbf{v_{k}} \\\\\n        {} &= c_{1}\\lambda _{1}\\mathbf{v_{1}} + c_{2}\\lambda _{2}\\mathbf{v_{2}} + ... + c_{k}\\lambda _{k}\\mathbf{v_{k}} \\qquad (2)\n    \\end{alignedat}\nNow we multiply both sides of Equation (1) by \\lambda _{k+1} to obtain \\lambda _{k+1}\\mathbf{v_{k+1}} = c_{1}\\lambda _{k+1}\\mathbf{v_{1}} + c_{2}\\lambda _{k+1}\\mathbf{v_{2}} + ... + c_{k}\\lambda _{k+1}\\mathbf{v_{k}} \\qquad (3)\nWhen we subtract Equation (3) from Equation (2), we obtain \\mathbf{0} = c_{1}(\\lambda _{1}-\\lambda _{k+1})\\mathbf{v_{1}} + c_{2}(\\lambda _{2}-\\lambda _{k+1})\\mathbf{v_{2}} + ... + c_{k}(\\lambda _{k}-\\lambda _{k+1})\\mathbf{v_{k}}\nThe linear independence of \\mathbf{v_{1}}, \\mathbf{v_{2}}, ..., \\mathbf{v_{k}} implies that c_{1}(\\lambda _{1}-\\lambda _{k+1}) = c_{2}(\\lambda _{2}-\\lambda _{k+1}) = ... = c_{k}(\\lambda _{k}-\\lambda _{k+1}) = 0\nSince the eigenvalues \\lambda _{i} are all distinct, \\lambda _{i} - \\lambda _{k+1} \\neq 0 for all i = 1, ..., k. Hence c_{1} = c_{2} = ... = c_{k} = 0. This implies that \\mathbf{v_{k+1}} = 0\\mathbf{v_{1}} + 0\\mathbf{v_{2}} + ... + 0\\mathbf{v_{k}} = \\mathbf{0}\nwhich is impossible since the eigenvector \\mathbf{v_{k+1}} cannot be zero.\nThus, our assumption that \\mathbf{v_{1}}, \\mathbf{v_{2}}, ..., \\mathbf{v_{m}} are linearly dependent is false. It follows that \\mathbf{v_{1}}, \\mathbf{v_{2}}, ..., \\mathbf{v_{m}} must be linearly independent. ◻\n\n\n\nExample 5.7 (Coupled oscillators, normal modes) Consider a system of two coupled oscillators connected by springs as shown in the figure.\n\n\n\nCoupled Oscillators\n\n\nWe can write down a system of equations for the dynamics of these two oscillators:  m \\ddot{x_1} = - k x_1 + k (x_2-x_1)   m \\ddot{x_x} = - k (x_2 -x_1) - k x_2  We can write this as a vector equation for the vector \\vec{x} =\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \n m \\ddot{\\vec{x}} = \\begin{pmatrix} -2 k & k \\\\ k & -2k  \\end{pmatrix} \\vec{x}\nIf we are looking for the normal modes of the system, we are looking of harmonic oscillations with a single frequency. So we assume\n\\vec{x} = \\begin{pmatrix} x_{10} \\\\ x_{20} \\end{pmatrix} e^{i \\omega t} \\quad \\ddot{\\vec{x}} = - \\omega^2 \\begin{pmatrix} x_{10} \\\\ x_{20} \\end{pmatrix} e^{i \\omega t} Substituting in the vector equation above leads to an eigenvalue problem  \\Rightarrow - m \\omega^2 \\vec{x_0} = \\begin{pmatrix} -2 k & k \\\\ k & -2k  \\end{pmatrix} \\vec{x_0}  \\Rightarrow  \\begin{pmatrix} 2 k/m & - k/m \\\\ - k/m & 2k/m  \\end{pmatrix} \\vec{x_0} = \\omega^2 \\vec{x_0}   for the frequency \\omega. The eigenvalues and eigenvectors of the matrix are\n \\lambda_1 = 3 k/m \\quad \\vec{x_0} = \\begin{pmatrix} -1\\\\ 1 \\end{pmatrix},  and  \\lambda_1 = k/m \\quad \\vec{x_0} = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix},  corresponding to an oscillation with frequency \\omega = \\sqrt{3k/m} where the two oscillators are 180 degree out of phase, and an oscillation with frequency \\omega = \\sqrt{k/m} where the oscillators are in phase.\nOther solutions for the system can now be obtained as a linear combination of these two modes. The modes are a basis of the solution space.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "eigenvalues.html#similarity-and-diagonalisation",
    "href": "eigenvalues.html#similarity-and-diagonalisation",
    "title": "5  Eigenvalues and Eigenvectors",
    "section": "5.2 Similarity and Diagonalisation",
    "text": "5.2 Similarity and Diagonalisation\n\n5.2.1 Introduction\nIn many applications, matrices represent linear mappings of vectors in a physical space, as in the example given at the start of the Eigenvector section. The choice of a coordinate system (in particular, its orientation) in this space is arbitrary, and this choice determines what the matrix looks like. In this section, we show that under certain conditions, there exists a choice of a coordinate system in which the matrix becomes a diagonal matrix. In this case, the coordinate axes have the direction of the eigenvectors of the matrix, and the diagonal elements of the matrix are the eigenvalues.\n\n\n5.2.2 Similar Matrices\n\nDefinition 5.8: Similar MatricesLet A and B be n \\times n matrices. We say that A is similar to B if there is an invertible n \\times n matrix P such that P^{-1}AP = B. If A is similar to B, we write A \\sim B.\n\n\n\n\n\n\n\n\nNoteRemark\n\n\n\nIf A \\sim B, we can write, equivalently, that A = PBP^{-1} or AP = PB. The matrix P depends on A and B. It is not unique for a given pair of similar matrices A and B.\n\n\n\nTheorem 5.9: Properties of Similar MatricesLet A and B be n \\times n matrices with A \\sim B. Then\n(a) det(A) = det(B)\n(b) A and B have the same rank.\n(c) A and B have the same characteristic polynomial.\n(d) A and B have the same eigenvalues.\n\n\n\nProofIf A \\sim B, then P^{-1}AP = B for some invertible matrix P.\n(a)\n\\begin{alignedat}{2}\n            \\qquad det(B) &= det(P^{-1}AP) \\\\\n            {} &= det(P^{-1})det(A)det(P) \\\\\n            {} &= \\frac{1}{det(P)}det(A)det(P) \\\\\n            {} &= det(A). \\\\\n        \\end{alignedat}\n(c) The characteristic polynomial of B is \\begin{alignedat}{2}\n            det(B - \\lambda I) &= det(P^{-1}AP - \\lambda I) \\\\\n            {} &= det(P^{-1}AP - \\lambda P^{-1}IP) \\\\\n            {} &= det(P^{-1}AP - P^{-1}(\\lambda I)P) \\\\\n            {} &= det(P^{-1}(A - \\lambda I)P) \\\\\n            {} &= det(P^{-1})det(A - \\lambda I)det(P) \\\\\n            {} &= \\frac{1}{det(P)}det(A - \\lambda I)det(P) \\\\\n            {} &= det(A - \\lambda I)  \n        \\end{alignedat}  ◻\n\n\nTheorem 5.9 is helpful in showing that two matrices are not similar, since A and B cannot be similar if any of the properties fail.\n\nExample 5.8  \n\nThe two matrices A = \\left[\\begin{array}{cc} 1 & 2 \\\\ 2 & 1 \\end{array}\\right], \\quad B = \\left[\\begin{array}{cc} 2 & 1 \\\\ 1 & 2 \\end{array}\\right],  are not similar since det(A) = -3 but det(B) = 3.\n\nThe two matrices A = \\left[\\begin{array}{cc} 1 & 3 \\\\ 2 & 2 \\end{array}\\right], \\quad B = \\left[\\begin{array}{cc} 1 & 1 \\\\ 3 & -1 \\end{array}\\right] are not similar, since |A - \\lambda I| = \\lambda ^{2} - 3\\lambda - 4 while |B - \\lambda I| = \\lambda ^{2} - 4. Note that A and B have the same determinant and rank, however.\n\n\n\n\n5.2.3 Diagonalisation\n\nDefinition 5.10: Diagonalisable MatricesAn n \\times n matrix A is diagonalisable if there is a diagonal matrix D such that A is similar to D - that is, if there is an invertible n \\times n matrix P such that P^{-1}AP = D.\n\n\n\nTheorem 5.11: Condition for DiagonalisabilityLet A be an n \\times n matrix. Then A is diagonalisable if and only if A has n linearly independent eigenvectors.\nMore precisely, there exists an invertible matrix P and a diagonal matrix D such that P^{-1}AP = D if and only if the columns of P are n linearly independent eigenvectors of A and the diagonal entries of D are the eigenvalues of A corresponding to the eigenvectors in P in the same order.*\n\n\n\nProofSuppose first that A is similar to the diagonal matrix D by P^{-1}AP = D or, equivalently, AP = PD. Let the columns of P be \\mathbf{p_{1}}, \\mathbf{p_{2}}, ..., \\mathbf{p_{n}} and let the diagonal entries of D be \\lambda _{1}, \\lambda _{2}, ..., \\lambda _{n}. Then A\\left[\\mathbf{p_{1}}, \\mathbf{p_{2}}, \\cdots, \\mathbf{p_{n}}\\right] = \\left[\\mathbf{p_{1}}, \\mathbf{p_{2}}, \\cdots, \\mathbf{p_{n}}\\right] \\left[\\begin{array}{cccc} \\lambda _{1} & 0 & \\cdots & 0 \\\\ 0 & \\lambda _{2} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda _{n} \\end{array}\\right] \\qquad (1) or \\qquad \\left[A\\mathbf{p_{1}}, A\\mathbf{p_{2}}, \\cdots, A\\mathbf{p_{n}}\\right] = \\left[\\lambda _{1}\\mathbf{p_{1}}, \\lambda _{2}\\mathbf{p_{2}}, \\cdots, \\lambda _{n}\\mathbf{p_{n}}\\right] \\qquad (2)\nEquating columns, we have A\\mathbf{p_{1}} = \\lambda _{1}\\mathbf{p_{1}}, A\\mathbf{p_{2}} = \\lambda _{2}\\mathbf{p_{2}}, \\cdots , A\\mathbf{p_{n}} = \\lambda _{n}\\mathbf{p_{n}} which proves that the column vectors of P are eigenvectors of A whose corresponding eigenvalues are the diagonal entries of D in the same order. Since P is invertible, its columns are linearly independent.\nConversely, if A has n linearly independent eigenvectors \\mathbf{p_{1}}, \\mathbf{p_{2}}, \\cdots , \\mathbf{p_{n}} with corresponding eigenvalues \\lambda _{1}, \\lambda _{2}, \\cdots , \\lambda _{n}, respectively, then A\\mathbf{p_{1}} = \\lambda _{1}\\mathbf{p_{1}}, A\\mathbf{p_{2}} = \\lambda _{2}\\mathbf{p_{2}}, \\cdots , A\\mathbf{p_{n}} = \\lambda _{n}\\mathbf{p_{n}}\nThis implies Eq. (2), which is equivalent to Eq. (1), that is AP = PD. Since the columns \\mathbf{p_{1}}, \\mathbf{p_{2}}, \\cdots , \\mathbf{p_{n}} of P are linearly independent, P is invertible, so P^{-1}AP = D, that is, A is diagonalisable. ◻\n\n\n\nExample 5.9 If possible, find a matrix P that diagonalises A = \\left[\\begin{array}{ccc}\n            0 & 1 & 0 \\\\\n            0 & 0 & 1 \\\\\n            2 & -5 & 4\n        \\end{array}\\right]\n\n\nSolutionWe studied this matrix previously and found that it has eigenvalues \\lambda _{1} = \\lambda _{2} = 1 and \\lambda _{3} = 2. The eigenspaces have the following bases:\nFor \\lambda _{1} = \\lambda _{2} = 1, E_{1} has basis \\left[\\begin{array}{c} 1 \\\\ 1 \\\\ 1 \\end{array}\\right].\nFor \\lambda _{3} = 2, E_{2} has basis \\left[\\begin{array}{c} 1 \\\\ 2 \\\\ 4 \\end{array}\\right].\nSince all other eigenvectors are just multiples of one of these two basis vectors, there cannot be three linearly independent eigenvectors. By Theorem 4.6, A is not diagonalisable.\n\n\n\nExample 5.10 If possible, find a matrix P that diagonalises A = \\left[\\begin{array}{ccc}\n            2 & 2 & 0 \\\\\n            0 & 1 & 0 \\\\\n            -4 & -8 & 1\n        \\end{array}\\right]\n\n\nSolutionThis is the matrix of Question 3, Worksheet 6. There we found that the eigenvalues of A are \\lambda _{1} = 2 and \\lambda _{2} = \\lambda _{3} = 1, with the following bases for the eigenspaces:\nFor \\lambda _{1} = 2, E_{2} has basis \\mathbf{p_{1}} = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ -4 \\end{array}\\right].\nFor \\lambda _{2} = \\lambda _{3} = 1, E_{1} has basis \\mathbf{p_{2}} = \\left[\\begin{array}{c} -2 \\\\ 1 \\\\ 0 \\end{array}\\right] and \\mathbf{p_{3}} = \\left[\\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\end{array}\\right].\nNow we check whether \\left\\{\\mathbf{p_{1}}, \\mathbf{p_{2}}, \\mathbf{p_{3}}\\right\\} is linearly independent. \\left[\\begin{array}{ccc} \\textcircled{\\raisebox{-0.9pt}{1}} & -2 & 0 \\\\ 0 & 1 & 0 \\\\ -4 & 0 & 1 \\end{array}\\right] \\xrightarrow{R_{3} + 4R_{1}} \\left[\\begin{array}{ccc} 1 & -2 & 0 \\\\ 0 & \\textcircled{\\raisebox{-0.9pt}{1}} & 0 \\\\ 0 & -8 & 1 \\end{array}\\right] \\xrightarrow{R_{3} + 8R_{2}} \\left[\\begin{array}{ccc} 1 & -2 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{array}\\right]\nSince rank = 3, \\left\\{\\mathbf{p_{1}}, \\mathbf{p_{2}}, \\mathbf{p_{3}}\\right\\} is linearly independent. Thus, if we take P = \\left[\\mathbf{p_{1}}, \\mathbf{p_{2}}, \\mathbf{p_{3}}\\right] = \\left[\\begin{array}{ccc} 1 & -2 & 0 \\\\ 0 & 1 & 0 \\\\ -4 & 0 & 1 \\end{array}\\right] then P is invertible. Furthermore, P^{-1}AP = \\left[\\begin{array}{ccc} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{array}\\right] = D\n(Note: It is much easier to check the equivalent equation AP = PD).\n\n\n\n\n\n\n\n\nNoteRemark\n\n\n\nEigenvectors can be placed into the columns of P in any order. However, the eigenvalues will come up on the diagonal of D in the same order as their corresponding eigenvectors in P. For example, if we had chosen P = \\left[\\mathbf{p_{2}}, \\mathbf{p_{3}}, \\mathbf{p_{1}}\\right] = \\left[\\begin{array}{ccc} -2 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & -4 \\end{array}\\right] Then we would have found P^{-1}AP = \\left[\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{array}\\right]\nWe checked that the eigenvectors \\mathbf{p_{1}}, \\mathbf{p_{2}} and \\mathbf{p_{3}} were linearly independent. However, the following Theorem guarantees that linear independence is preserved when the bases of different subspaces are combined.\n\n\n\nTheorem 5.12Let A be an n \\times n matrix and let \\lambda _{1}, \\lambda _{2}, \\cdots, \\lambda _{k} be distinct eigenvalues of A. If B_{i} is a basis for the eigenspace E_{i}, then B = B_{1} \\cup B_{2} \\cup \\cdots \\cup B_{k} (i.e. the total collection of basis vectors for all of the eigenspaces) is linearly independent.\n\n\n\nTheorem 5.13If A is an n \\times n matrix with n distinct eigenvalues, then A is diagonalisable.\n\n\n\nProofLet \\mathbf{v_{1}}, \\mathbf{v_{2}}, \\cdots, \\mathbf{v_{n}} be eigenvectors corresponding to the n distinct eigenvalues of A. By theorem 5.7, \\mathbf{v_{1}}, \\mathbf{v_{2}}, \\cdots, \\mathbf{v_{n}} are linearly independent, so, by Theorem 5.11, A is diagonalisable. ◻\n\n\n\nExample 5.11 The matrix A = \\left[\\begin{array}{cccc}\n            2 & 0 & 0 & 0 \\\\\n            -1 & 1 & 0 & 0 \\\\\n            3 & 0 & 3 & 0 \\\\\n            5 & 7 & 4 & -2\n        \\end{array}\\right] has eigenvalues \\lambda _{1} = 2, \\lambda _{2} = 1, \\lambda _{3} = 3 and \\lambda _{4} = -2, by Corollary 5.6. Since these are four distinct eigenvalues for a 4 \\times 4 matrix, A is diagonalisable, by Theorem 5.13.\n\n\nCorollary 5.14If A is an n \\times n matrix, then the geometric multiplicity of each eigenvalue is less than or equal to its algebraic multiplicity.\n\n\n\nTheorem 5.15: Diagonalisation Theorem\nLet A be an n \\times n matrix whose distinct eigenvalues are \\lambda _{1}, \\lambda _{2}, \\cdots, \\lambda _{k}, where 1\\leq k\\leq n. The following statements are equivalent:\n\nA is diagonalisable.\nThe union B of the bases of the eigenspaces of A contains n vectors.\nThe algebraic multiplicity of each eigenvalue equals its geometric multiplicity.\n\n\n\n\nExample 5.12  \n\nThe matrix A = \\left[\\begin{array}{ccc}\n    0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 2 & -5 & 4 \\end{array}\\right] has two distinct eigenvalues \\lambda _{1} = \\lambda _{2} = 1 and \\lambda _{3} = 2. Since the eigenvalue \\lambda _{1} = \\lambda _{2} = 1 with E_{1} = {\\rm span}( (1,1,1)^T) has algebraic multiplicity 2 but geometric multiplicity 1, A is not diagonalisable, by the Diagonalisation Theorem.\n\nThe matrix A = \\left[\\begin{array}{ccc}\n    2 & 2 & 0 \\\\ 0 & 1 & 0 \\\\ -4 & -8 & 1 \\end{array}\\right] has two distinct eigenvalues \\lambda _{1} = 2 and \\lambda _{2} = \\lambda _{3} = 1. We found: for \\lambda _{1} = 2, E_{1} has basis \\mathbf{p_{1}} = (1, 0, -4)^T, and for \\lambda _{2} = \\lambda _{3} = 1, E_{2} has basis \\mathbf{p_{2}} = (-2,1, 0)^T and \\mathbf{p_{3}} = (0,0,1)^T. Thus, the eigenvalue 2 has algebraic and geometric multiplicity 1, and the eigenvalue 1 has algebraic and geometric multiplicity 2. Thus, A is diagonalisable, by the Diagonalisation Theorem.",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "6  Appendix",
    "section": "",
    "text": "6.1 Greek Letters",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#sec-greekletters",
    "href": "appendix.html#sec-greekletters",
    "title": "6  Appendix",
    "section": "",
    "text": "Lower case Greek alphabet\n\n\nalpha\n\\alpha\niota\n\\iota\nrho\n\\rho\n\n\nbeta\n\\beta\nkappa\n\\kappa\nsigma\n\\sigma\n\n\ngamma\n\\gamma\nlambda\n\\lambda\ntau\n\\tau\n\n\ndelta\n\\delta\nmu\n\\mu\nupsilon\n\\upsilon\n\n\nepsilon\n\\epsilon\nnu\n\\nu\nphi\n\\phi\n\n\nzeta\n\\zeta\nxi\n\\xi\nchi\n\\chi\n\n\neta\n\\eta\nomicron\n\\omicron\npsi\n\\psi\n\n\ntheta\n\\theta\npi\n\\pi\nomega\n\\omega",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#carl-friedrich-gauss",
    "href": "appendix.html#carl-friedrich-gauss",
    "title": "6  Appendix",
    "section": "6.2 Carl Friedrich Gauss",
    "text": "6.2 Carl Friedrich Gauss\nCarl Friedrich Gauss (Gauß) (30 April 1777 - 23 February 1855) was a German mathematician and scientist of profound genius who contributed significantly to many fields, including number theory, analysis, differential geometry, geodesy, magnetism, astronomy and optics. Sometimes known as \"the prince of mathematicians\" and \"greatest mathematician since antiquity\", Gauss had a remarkable influence in many fields of mathematics and science and is ranked as one of history’s most influential mathematicians.\nGauss was a child prodigy, of whom there are many anecdotes pertaining to his astounding precocity while a mere toddler, and made his first ground-breaking mathematical discoveries while still a teenager. He completed Disquisitiones Arithmeticae, his magnum opus, at the age of twenty-one (1798), though it would not be published until 1801. This work was fundamental in consolidating number theory as a discipline and has shaped the field to the present day. 1\n\n\n\nCF Gauss",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#footnotes",
    "href": "appendix.html#footnotes",
    "title": "6  Appendix",
    "section": "",
    "text": "From Wikipedia, the free encyclopaedia↩︎",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "determinants.html#inverse-matrix-theorem",
    "href": "determinants.html#inverse-matrix-theorem",
    "title": "4  Determinants",
    "section": "4.4 Inverse Matrix theorem",
    "text": "4.4 Inverse Matrix theorem\nThe following theorem combines a number of topics, inverse matrices, determinants, range, and nullspace.\n\nTheorem 4.12: Inverse Matrix theorem\nThe following statements are equivalent for a n \\times n matrix {\\rm A}:\n\nthe matrix {\\rm A} has an inverse\nthe determinant of {\\rm A} is non-zero\nthe columns of {\\rm A} are linearly independent\nthe range of {\\rm A} is \\mathbb{R}^n\nthe rank of {\\rm A} is n\nthe nullspace of {\\rm A} is \\{\\vec{0} \\}\nthe equation {\\rm A}\\vec{x}= \\vec{0} has only the trivial solution \\vec{x}=\\vec{0}\nthe nullity of {\\rm A} is 0\nthe equation {\\rm A}\\vec{x}= \\vec{b} has a unique solution for any \\vec{b} \\in \\mathbb{R}^n\nthe mapping \\vec{x} \\rightarrow \\vec{y} = {\\rm A}\\vec{x} is one-to-one\nthe transpose of {\\rm A} is invertible\nthe rows of {\\rm A} are linearly independent\n\n\n\n\nProof\n\n1) \\Leftrightarrow 2): Corollary 4.8.\n2) \\Leftrightarrow 3): Definition of a determinant as the volume spanned by n (column vectors).\n3) \\Leftrightarrow 4): n linearly independent vectors in \\mathbb{R}^n span \\mathbb{R}^n (Corollary 2.11)\n4) \\Leftrightarrow 5): Definition of rank as the dimension of the range (\\Rightarrow). For the reverse (\\Leftarrow) note that rank = n means ther exists a basis for the space with n elements.\n5) \\Leftrightarrow 6): The rank theorem 3.19.\n6) \\Leftrightarrow 7): Definition of the nullspace\n7) \\Leftrightarrow 8): Definition of nullity as the dimension of the nullspace. Vice versa if the dimension of the nullspace is zero, then the nullspace must consist of \\{0 \\}, as the zero-vector is always in the nullspace.\n8) \\Leftrightarrow 9): We know from 4) that the range is the whole of \\mathbb{R}^n and if the solutions were not unique then there would be non-trivial solutions in the nullspace. (assume two different $} map to the same \\vec{b}: {\\rm A}\\vec{x}_1= \\vec{b} and {\\rm A}\\vec{x}_2= \\vec{b} then {\\rm A}(\\vec{x}_1  -\\vec{x}_2)= \\vec{0} and (\\vec{x}_1  -\\vec{x}_2) would be a non-trivial vector in the nullspace.\n9) \\Leftrightarrow 10): same statement.\n10) \\Leftrightarrow 11): The transpose has the same determinant as {\\rm A} and we can then use the equivalence of 1) and 2).\n11) \\Leftrightarrow 12): Statement 3) for the transpose, together with 11).",
    "crumbs": [
      "Algebra",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determinants</span>"
    ]
  }
]