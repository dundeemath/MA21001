<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.21">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Notes for MA21001.">

<title>5&nbsp; Eigenvalues and Eigenvectors â€“ Core Mathematics III</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./appendix.html" rel="next">
<link href="./determinants.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea1d7ac60288e0f1efdbc993fd8432ae.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-7e9717d9caa9b4d114d189eaeb260ada.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea1d7ac60288e0f1efdbc993fd8432ae.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b680963197a85bbb632cc40c4e408ceb.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-2a6abe4c656e78118c5caf32e94a2824.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-b680963197a85bbb632cc40c4e408ceb.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    var sidebar = document.getElementById('quarto-sidebar');
    var logo = sidebar.getElementsByTagName('img')[0];
    
    sidebar.appendChild(logo);
  });
</script>
<style>
.DONE {
  --color1: #cce7b1;
  --color2: #86b754;
}
.Conjecture {
  --color1: #948bde;
  --color2: #584eab;
}
.Corollary {
  --color1: #948bde;
  --color2: #584eab;
}
.Feature {
  --color1: #c0c0c0;
  --color2: #808080;
}
.Theorem {
  --color1: #948bde;
  --color2: #584eab;
}
.Definition {
  --color1: #d999d3;
  --color2: #a01793;
}
.Solution {
  --color1: #c0c0c0;
  --color2: #808080;
}
.Lemma {
  --color1: #948bde;
  --color2: #584eab;
}
.Proof {
  --color1: #c0c0c0;
  --color2: #808080;
}
.TODO {
  --color1: #e7b1b4;
  --color2: #8c3236;
}
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./notation.html">Algebra</a></li><li class="breadcrumb-item"><a href="./eigenvalues.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Eigenvalues and Eigenvectors</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./assets/images/uod_shield_rgb.png" alt="University of Dundee shield" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="./assets/images/uod_shield_rgb.png" alt="University of Dundee shield" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Core Mathematics III</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/dundeemath/MA21001/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Core-Mathematics-III.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Algebra</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Before we start â€¦</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./algebra3b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Vector spaces</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linearequations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear equations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./determinants.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Determinants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eigenvalues.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Eigenvalues and Eigenvectors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-to-eigenvalues-and-eigenvectors" id="toc-introduction-to-eigenvalues-and-eigenvectors" class="nav-link active" data-scroll-target="#introduction-to-eigenvalues-and-eigenvectors"><span class="header-section-number">5.1</span> Introduction to Eigenvalues and Eigenvectors</a></li>
  <li><a href="#similarity-and-diagonalisation" id="toc-similarity-and-diagonalisation" class="nav-link" data-scroll-target="#similarity-and-diagonalisation"><span class="header-section-number">5.2</span> Similarity and Diagonalisation</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">5.2.1</span> Introduction</a></li>
  <li><a href="#similar-matrices" id="toc-similar-matrices" class="nav-link" data-scroll-target="#similar-matrices"><span class="header-section-number">5.2.2</span> Similar Matrices</a></li>
  <li><a href="#diagonalisation" id="toc-diagonalisation" class="nav-link" data-scroll-target="#diagonalisation"><span class="header-section-number">5.2.3</span> Diagonalisation</a></li>
  </ul></li>
  <li><a href="#adjoint-operator" id="toc-adjoint-operator" class="nav-link" data-scroll-target="#adjoint-operator"><span class="header-section-number">5.3</span> Adjoint Operator</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/dundeemath/MA21001/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./notation.html">Algebra</a></li><li class="breadcrumb-item"><a href="./eigenvalues.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Eigenvalues and Eigenvectors</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Eigenvalues and Eigenvectors</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-to-eigenvalues-and-eigenvectors" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="introduction-to-eigenvalues-and-eigenvectors"><span class="header-section-number">5.1</span> Introduction to Eigenvalues and Eigenvectors</h2>
<p>Consider an elastic body, e.g., a rubber ball. If we compress the ball in one direction, it will be elongated in the plane perpendicular to that direction. The figure below shows, in a cross-section, how individual points on the surface move under the compression.</p>
<div id="fig-deformation" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deformation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="eigenv1.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="eigenv2.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-deformation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: Effect of a deformation on the points (vectors) in an elastic sphere.
</figcaption>
</figure>
</div>
<p>There are some vectors in these plots that donâ€™t change direction, but only their length under the deformation. We can find those by superimposing the two plots:</p>
<div id="fig-deformation2" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deformation2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="eigenv3.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="eigenv4.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-deformation2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: There are directions in which the vectors only change their length.
</figcaption>
</figure>
</div>
<p>These directions are characteristic of the deformation, as is the factor of elongation/compression along these directions. These directions can be found mathematically as the eigenvectors of a matrix, the deformation matrix, which maps the original vectors to the vectors of the deformed ball. The eigenvalues are the factors of compression/elongation along these directions. The word "eigen" is German for "own" and was likely introduced by David Hilbert (also known for Hilbert spaces).</p>
<p>Eigenvalues and eigenvectors play an essential role in mathematics, physics and engineering. The stability of an equilibrium of a dynamical system is determined by the eigenvalues of a matrix that describes the linearised system at the equilibrium point. The values of a measurement in quantum mechanics are the eigenvalues of an operator. The principal axes of a rigid body are the eigenvectors of the moment of inertia tensor.</p>
<div id="eigenvalue-and-eigenvector" class="Definition" title="Eigenvalue and Eigenvector">
<p></p><details class="Definition fbx-simplebox fbx-default" open=""><summary><strong>Definition 5.1: </strong>Eigenvalue and Eigenvector</summary><div>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> matrix. A scalar <span class="math inline">\lambda</span> is called an eigenvalue of <span class="math inline">A</span> if there is a nonzero vector <span class="math inline">\mathbf{x}</span> such that <span class="math inline">A\mathbf{x} = \lambda \mathbf{x}</span>. Such a vector <span class="math inline">\mathbf{x}</span> is called an eigenvector of <span class="math inline">A</span> corresponding to <span class="math inline">\lambda</span>.<p></p>
</div></details>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1</strong></span> Suppose <span class="math inline">A\left[\begin{array}{c} x \\ y \end{array}\right] = \left[\begin{array}{c} 2x \\ 0 \end{array}\right]</span>. Then <span class="math display">A\left[\begin{array}{c} 1 \\ 0 \end{array}\right] = \left[\begin{array}{c} 2 \\ 0 \end{array}\right] = 2\left[\begin{array}{c} 1 \\ 0 \end{array}\right]</span> so <span class="math inline">2</span> is an eigenvalue and <span class="math inline">\left[\begin{array}{c} 1 \\ 0 \end{array}\right]</span> a corresponding eigenvector. Also, <span class="math display">A\left[\begin{array}{c} 0 \\ 1 \end{array}\right] = \left[\begin{array}{c} 0 \\ 0 \end{array}\right] = 0\left[\begin{array}{c} 0 \\ 1 \end{array}\right],</span> so <span class="math inline">0</span> is an eigenvalue and <span class="math inline">\left[\begin{array}{c} 0 \\ 1 \end{array}\right]</span> a corresponding eigenvector.</p>
</div>
<p>Notice that <span class="math inline">\left[\begin{array}{c} \alpha \\ 0 \end{array}\right]</span> and <span class="math inline">\left[\begin{array}{c} 0 \\ \alpha \end{array}\right]</span> are eigenvectors for any <span class="math inline">\alpha \neq 0</span>.<br>
In general, if <span class="math inline">\mathbf{v}</span> is an eigenvector of <span class="math inline">A</span>, then so is <span class="math inline">\alpha \mathbf{v}</span> for any nonzero scalar <span class="math inline">\alpha</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.2</strong></span> Show that <span class="math inline">5</span> is an eigenvalue of <span class="math inline">A = \left[\begin{array}{cc} 1 &amp; 2 \\ 4 &amp; 3 \end{array}\right]</span> and determine all eigenvectors corresponding to this eigenvalue.</p>
</div>
<div id="Solution*-5.1" class="Solution">
<p></p><details class="Solution fbx-default closebutton"><summary><strong>Solution</strong></summary><div>We must show that there is a nonzero vector <span class="math inline">\mathbf{x}</span> such that <span class="math inline">A\mathbf{x} = 5\mathbf{x}</span>, which is equivalent to the equation <span class="math inline">(A - 5I)\mathbf{x} = \mathbf{0}</span>. We compute the nullspace by:<p></p>
<p><span class="math display">\left[A - 5I | \mathbf{0}\right] = \left[\begin{array}{cc|c} -4 &amp; 2 &amp; 0 \\ 4 &amp; -2 &amp; 0 \end{array}\right] \xrightarrow{R_{2} + R_{1}} \left[\begin{array}{cc|c} -4 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right]</span></p>
<p>Thus <span class="math inline">\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \end{array}\right] \in {\rm Null}(A-5I)</span> satisfies <span class="math inline">-4x_{1} + 2x_{2} = 0</span>, or <span class="math inline">x_{2} = 2x_{1}</span>.<br>
Thus, <span class="math inline">A\mathbf{x} = 5\mathbf{x}</span> has a nontrivial solution of the form <span class="math inline">\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \end{array}\right] = \left[\begin{array}{c} x_{1} \\ 2x_{1} \end{array}\right] = x_{1}\left[\begin{array}{c} 1 \\ 2 \end{array}\right]</span>, so <span class="math inline">5</span> is an eigenvalue of <span class="math inline">A</span> and the corresponding eigenvectors are the nonzero multiples of <span class="math inline">\left[\begin{array}{c} 1 \\ 2 \end{array}\right]</span>.</p>
</div></details>
</div>
<div id="eigenspace" class="Definition" title="Eigenspace">
<p></p><details class="Definition fbx-simplebox fbx-default" open=""><summary><strong>Definition 5.2: </strong>Eigenspace</summary><div>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> matrix and let <span class="math inline">\lambda</span> be an eigenvalue of <span class="math inline">A</span>. The collection of all eigenvectors corresponding to <span class="math inline">\lambda</span>, together with the zero vector, is called the eigenspace of <span class="math inline">\lambda</span> and is denoted by <span class="math inline">E_{\lambda }</span>.<p></p>
</div></details>
</div>
<p>Therefore, in the above Example, <span class="math inline">E_{5} = \left\{t\left[\begin{array}{c} 1 \\ 2 \end{array}\right]\right\}</span>, where <span class="math inline">t \in \mathbb{R}</span>.</p>
<div id="characteristic-equation" class="Theorem" title="Characteristic Equation">
<p></p><details class="Theorem fbx-simplebox fbx-default" open=""><summary><strong>Theorem 5.3: </strong>Characteristic Equation</summary><div>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> matrix. Then <span class="math inline">\lambda</span> is an eigenvalue of <span class="math inline">A</span> if and only if <span class="math inline">|A-\lambda I_{n}| = 0</span> (or <span class="math inline">{\rm det}(A - \lambda I_{n}) = 0</span>).<p></p>
</div></details>
</div>
<div id="Proof*-5.2" class="Proof">
<p></p><details class="Proof fbx-default closebutton"><summary><strong>Proof</strong></summary><div>Suppose that <span class="math inline">\lambda</span> is an eigenvalue of <span class="math inline">A</span>. Then <span class="math inline">A\mathbf{v} = \lambda \mathbf{v}</span> for some nonzero <span class="math inline">\mathbf{v} \in \mathbb{R}^{n}</span>. This is equivalent to <span class="math inline">A\mathbf{v} = \lambda I_{n} \mathbf{v}</span> or <span class="math inline">(A - \lambda I_{n})\mathbf{v} = \mathbf{0}</span>. But this means that <span class="math inline">\mathbf{v}</span> is a nonzero solution to the homogeneous system of equations defined by the matrix <span class="math inline">A - \lambda I_{n}</span>. This means <span class="math inline">A - \lambda I_{n}</span> is singular, and so <span class="math inline">|A - \lambda I_{n}| = 0</span>.<br>
Conversely, if <span class="math inline">|A - \lambda I_{n}| = 0</span> then <span class="math inline">A - \lambda I_{n}</span> is singular, and so the system of equations defined by <span class="math inline">A - \lambda I_{n}</span> has nonzero solutions. Hence there exists a nonzero <span class="math inline">\mathbf{v} \in \mathbb{R}^{n}</span> with <span class="math inline">(A - \lambda I_{n})\mathbf{v} = \mathbf{0}</span>, which is equivalent to <span class="math inline">A \mathbf{v} = \lambda \mathbf{v}</span>, and so <span class="math inline">\lambda</span> is an eigenvalue of <span class="math inline">A</span>.&nbsp;â—»<p></p>
</div></details>
</div>
<div id="characteristic-equationpolynomial" class="Definition" title="Characteristic Equation/Polynomial">
<p></p><details class="Definition fbx-simplebox fbx-default" open=""><summary><strong>Definition 5.4: </strong>Characteristic Equation/Polynomial</summary><div>For an <span class="math inline">n \times n</span> matrix <span class="math inline">A</span>, the equation <span class="math inline">|A - \lambda I_{n}| = 0</span> is called the characteristic equation of <span class="math inline">A</span>, and <span class="math inline">|A - \lambda I_{n}|</span> is called the characteristic polynomial of <span class="math inline">A</span>.<p></p>
</div></details>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.3</strong></span> Find the eigenvalues and the corresponding eigenvectors of <span class="math inline">A = \left[\begin{array}{cc}
        1 &amp; 2 \\
        4 &amp; -1
    \end{array}\right]</span>.</p>
<div class="Solution">
<p></p><details class="Solution fbx-default closebutton"><summary><strong>Solution</strong></summary><div>The characteristic polynomial is<p></p>
<p><span class="math display">\begin{alignedat}{2}
        |A - \lambda I_{2}| &amp;= \left|\begin{array}{cc}
            1 - \lambda &amp; 2 \\
            4 &amp; -1-\lambda
        \end{array}\right| \\
        {} &amp;= (1-\lambda )(-1-\lambda ) - 8 \\
        {} &amp;= \lambda ^{2} - 9 \\
        {} &amp;= (\lambda + 3)(\lambda - 3). \\
    \end{alignedat}</span> Hence the eigenvalues of <span class="math inline">A</span> are the roots of <span class="math inline">(\lambda + 3)(\lambda - 3) = 0</span>; that is <span class="math inline">\lambda _{1} = -3</span> and <span class="math inline">\lambda _{2} = 3</span>.<br>
To find the eigenvectors corresponding to <span class="math inline">\lambda _{1} = -3</span>, we find the nullspace of <span class="math display">A - (-3)I_{2} = \left[\begin{array}{cc} 4 &amp; 2 \\ 4 &amp; 2 \end{array}\right]</span></p>
<p>Row reduction produces <span class="math display">\left[A+3I_{2} | \mathbf{0}\right] = \left[\begin{array}{cc|c} 4 &amp; 2 &amp; 0 \\ 4 &amp; 2 &amp; 0 \end{array}\right] \xrightarrow{R_{2} - R_{1}} \left[\begin{array}{cc|c} 4 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right]</span></p>
<p>Thus <span class="math inline">\mathbf{x} \in {\rm Null}(A+3I_{2})</span> if and only if <span class="math inline">4x_{1} + 2x_{2} = 0</span>.<br>
Setting the free variable <span class="math inline">x_{2} = t</span>, we see that <span class="math inline">x_{1} = -\frac{1}{2}t</span>. We take <span class="math inline">\left[\begin{array}{c} x_{1} \\ x_{2} \end{array}\right] = \left[\begin{array}{c} -1 \\ 2 \end{array}\right]</span> be our eigenvector; or indeed any nonzero multiple of <span class="math inline">\left[\begin{array}{c} -1 \\ 2 \end{array}\right]</span>.<br>
To find the eigenvectors corresponding to <span class="math inline">\lambda _{2} = 3</span>, we find the nullspace of <span class="math inline">A - 3I_{2}</span> by row reduction: <span class="math display">\left[A - 3I_{2} | \mathbf{0}\right] = \left[\begin{array}{cc|c} -2 &amp; 2 &amp; 0 \\ 4 &amp; -4 &amp; 0 \end{array}\right] \xrightarrow{R_{2} + 2R_{1}} \left[\begin{array}{cc|c} -2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right]</span></p>
<p>So <span class="math inline">\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \end{array}\right] \in {\rm Null}(A - 3I_{2})</span> if and only if <span class="math inline">-2x_{1} + 2x_{2} = 0</span>. Setting the free variable <span class="math inline">x_{2} = t</span>, we find <span class="math inline">x_{1} = t</span>. We take <span class="math inline">\left[\begin{array}{c} x_{1} \\ x_{2} \end{array}\right] = \left[\begin{array}{c} 1 \\ 1 \end{array}\right]</span> to be our eigenvector.</p>
</div></details>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.4</strong></span> Find the eigenvalues and the corresponding eigenvectors of <span class="math display">A = \left[\begin{array}{ccc}
            3 &amp; 2 &amp; 2 \\
            1 &amp; 4 &amp; 1 \\
            -2 &amp; -4 &amp; -1
        \end{array}\right]</span>.</p>
</div>
<div id="Solution*-5.3" class="Solution">
<p></p><details class="Solution fbx-default closebutton"><summary><strong>Solution</strong></summary><div>The characteristic equation is <span class="math display">\begin{alignedat}{2}
        0 = |A - \lambda I| &amp;= \left|\begin{array}{ccc}
            3-\lambda &amp; 2 &amp; 2 \\
            1 &amp; 4-\lambda &amp; 1 \\
            -2 &amp; -4 &amp; -1-\lambda
        \end{array}\right| \\
        {} &amp;= (3-\lambda ) \left|\begin{array}{cc} 4-\lambda &amp; 1 \\ -4 &amp; -1-\lambda \end{array}\right| - 2 \left|\begin{array}{cc} 1 &amp; 1 \\ -2 &amp; -1-\lambda \end{array}\right| + 2 \left|\begin{array}{cc} 1 &amp; 4-\lambda \\ -2 &amp; -4\end{array}\right| \\
        {} &amp;= (3-\lambda ) \{(4-\lambda )(-1-\lambda )+4\} - 2\{(-1-\lambda )+2\} + 2\{-4+2(4-\lambda )\} \\
        {} &amp;= -\lambda ^3 + 6\lambda ^2 - 11\lambda + 6 \\
        {} &amp;= (\lambda -1)(-\lambda ^2 + 5\lambda - 6) \\
        {} &amp;= (\lambda -1)\{-(\lambda ^2 - 5\lambda + 6)\} \\
        {} &amp;= (\lambda -1)\{-(\lambda - 2)(\lambda - 3)\} \\
    \end{alignedat}</span><p></p>
<p>Hence, the eigenvalues are <span class="math inline">\lambda _{1} = 1, \lambda _{2} = 2</span> and <span class="math inline">\lambda _{3} = 3</span>.<br>
For <span class="math inline">\lambda _{1}=1</span>, we compute <span class="math display">\left[A - I | \mathbf{0}\right] = \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{2}} &amp; 2 &amp; 2 &amp; 0 \\ 1 &amp; 3 &amp; 1 &amp; 0 \\ -2 &amp; -4 &amp; -2 &amp; 0 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_{2} - \frac{1}{2}R_{1} \\ R_{3} + R_{1} \end{subarray}} \left[\begin{array}{ccc | c} 2 &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{2}} &amp; 0 &amp; 0 \\ 0 &amp; -2 &amp; 0 &amp; 0 \end{array}\right] </span> <span class="math display">\xrightarrow{R_{3} + R_{2}} \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{2}} &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{2}} &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]</span> from which it follows that an eigenvector <span class="math display">\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right]</span> satisfies <span class="math inline">x_{2} = 0</span> and <span class="math inline">x_{3} = -x_{1}</span>. We take <span class="math inline">\left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right] = \left[\begin{array}{c} 1 \\ 0 \\ -1 \end{array}\right]</span> to be our eigenvector .</p>
<p>For <span class="math inline">\lambda _{2} = 2</span>, we compute <span class="math display">\left[A - 2I | \mathbf{0}\right] = \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{1}} &amp; 2 &amp; 2 &amp; 0 \\ 1 &amp; 2 &amp; 1 &amp; 0 \\ -2 &amp; -4 &amp; -3 &amp; 0 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_{2} - R_{1} \\ R_{3} + 2R_{1} \end{subarray}} \left[\begin{array}{ccc | c} 1 &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; \textcircled{\raisebox{-0.9pt}{-1}} &amp; 0 \\ 0 &amp; 0 &amp; 3 &amp; 0 \end{array}\right]</span> <span class="math display">\xrightarrow{R_{3} + 3R_{2}} \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{1}} &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; \textcircled{\raisebox{-0.9pt}{-1}} &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]</span> which gives an eigenvector <span class="math display">\mathbf{x} = \left[\begin{array}{c} -2 \\ 1 \\ 0 \end{array}\right]</span>.</p>
<p>For <span class="math inline">\lambda _{3} = 3</span>, we compute <span class="math display">\left[A - 3I | \mathbf{0}\right] = \left[\begin{array}{ccc | c} 0 &amp; 2 &amp; 2 &amp; 0 \\ 1 &amp; 1 &amp; 1 &amp; 0 \\ -2 &amp; -4 &amp; -4 &amp; 0 \end{array}\right] \xrightarrow{R_{2} \leftrightarrow R_{1}} \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{1}} &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 2 &amp; 2 &amp; 0 \\ -2 &amp; -4 &amp; -4 &amp; 0 \end{array}\right] </span> <span class="math display">\xrightarrow{R_{3} + 2R_{1}} \left[\begin{array}{ccc | c} 1 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{2}} &amp; 2 &amp; 0 \\ 0 &amp; -2 &amp; -2 &amp; 0 \end{array}\right] \xrightarrow{R_{3} + R_{2}} \left[\begin{array}{ccc | c} 1 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]</span> which gives an eigenvector <span class="math display">\mathbf{x} = \left[\begin{array}{c} 0 \\ -1 \\ 1 \end{array}\right]</span>.</p>
</div></details>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.5</strong></span> Find the eigenvalues and the corresponding eigenspaces of <span class="math display">A = \left[\begin{array}{ccc}
        0 &amp; 1 &amp; 0 \\
        0 &amp; 0 &amp; 1 \\
        2 &amp; -5 &amp; 4
    \end{array}\right]</span></p>
</div>
<div id="Solution*-5.4" class="Solution">
<p></p><details class="Solution fbx-default closebutton"><summary><strong>Solution</strong></summary><div>The characteristic equations is<p></p>
<p><span class="math display">\begin{alignedat}{2}
        0 = |A - \lambda I| &amp;= \left|\begin{array}{ccc}
            -\lambda &amp; 1 &amp; 0 \\
            0 &amp; -\lambda &amp; 1 \\
            2 &amp; -5 &amp; 4-\lambda
        \end{array}\right| \\
        {} &amp;= -\lambda \left|\begin{array}{cc} -\lambda &amp; 1 \\ -5 &amp; 4-\lambda \end{array}\right| -  \left|\begin{array}{cc} 0 &amp; 1 \\ 2 &amp; 4-\lambda \end{array}\right| \\
        {} &amp;= -\lambda (\lambda ^2-4\lambda +5)-(-2) \\
        {} &amp;= -\lambda ^3 + 4\lambda ^2 - 5\lambda + 2 \\
        {} &amp;= (\lambda -1)(-\lambda ^2 + 3\lambda - 2) \\
        {} &amp;= -(\lambda -1)^2(\lambda -2) \\
    \end{alignedat}</span></p>
<p>Hence, the eigenvalues are <span class="math inline">\lambda _{1} = \lambda _{2} = 1</span> and <span class="math inline">\lambda _{3} = 2</span>.<br>
To find the eigenvectors corresponding to <span class="math inline">\lambda _{1} = \lambda _{2} = 1</span>, we compute <span class="math display">\left[A - I | \mathbf{0}\right] = \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{-1}} &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 1 &amp; 0 \\ 2 &amp; -5 &amp; 3 &amp; 0 \end{array}\right] \xrightarrow{R_{3} + 2R_{1}} \left[\begin{array}{ccc | c} -1 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{-1}} &amp; 1 &amp; 0 \\ 0 &amp; -3 &amp; 3 &amp; 0 \end{array}\right] \xrightarrow{R_{3} - 3R_{2}} \left[\begin{array}{ccc | c} -1 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]</span></p>
<p>Thus, <span class="math display">\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right]</span> is in the eigenspace <span class="math inline">E_{1}</span> if and only if <span class="math inline">-x_{1} + x_{2} = 0</span> and <span class="math inline">-x_{2} + x_{3} = 0</span>. Setting the free variable <span class="math inline">x_{3} = t</span>, we see that <span class="math inline">x_{1} = t</span> and <span class="math inline">x_{2} = t</span>, from which it follows that <span class="math display">E_{1} = \left\{\left[\begin{array}{c} t \\ t \\ t \end{array}\right]\right\} = \left\{t \left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right]\right\} = {\rm span}\left(\left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right]\right)</span></p>
<p>To find the eigenvectors correspond to <span class="math inline">\lambda _{3} = 2</span>, we find the nullspace of <span class="math inline">A - 2I</span> by row reduction: <span class="math display">\left[A - 2I | \mathbf{0}\right] = \left[\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{-2}} &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -2 &amp; 1 &amp; 0 \\ 2 &amp; -5 &amp; 2 &amp; 0 \end{array}\right] \xrightarrow{R_{3} + R_{1}} \left[\begin{array}{ccc | c} -2 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{-2}} &amp; 1 &amp; 0 \\ 0 &amp; -4 &amp; 2 &amp; 0 \end{array}\right] </span> <span class="math display">\xrightarrow{R_{3} - 2R_{2}} \left[\begin{array}{ccc | c} -2 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -2 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right]</span></p>
<p>So <span class="math display">\mathbf{x} = \left[\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right]</span> is in the eigenspace <span class="math inline">E_{2}</span> if and only if <span class="math inline">-2x_{1} + x_{2} = 0</span> and <span class="math inline">-2x_{2} + x_{3} = 0</span>. Setting the free variable <span class="math inline">x_{3} = t</span>, we have <span class="math display">E_{2} = \left\{\left[\begin{array}{c} \frac{1}{4}t \\ \frac{1}{2}t \\ t \end{array}\right]\right\} = \left\{t \left[\begin{array}{c} \frac{1}{4} \\ \frac{1}{2} \\ 1 \end{array}\right]\right\} = {\rm span}\left(\left[\begin{array}{c} \frac{1}{4} \\ \frac{1}{2} \\ 1 \end{array}\right]\right) = {\rm span}\left(\left[\begin{array}{c} 1 \\ 2 \\ 4 \end{array}\right]\right)</span></p>
</div></details>
</div>
<div id="algebraic-and-geometric-multiplicity" class="Definition" title="Algebraic and Geometric Multiplicity">
<p></p><details class="Definition fbx-simplebox fbx-default" open=""><summary><strong>Definition 5.5: </strong>Algebraic and Geometric Multiplicity</summary><div>The algebraic multiplicity of an eigenvalue is its multiplicity as a root of the characteristic equation.<br>
The geometric multiplicity of an eigenvalue <span class="math inline">\lambda</span> is <span class="math inline">{\rm dim} (E_{\lambda })</span>, the dimension of its corresponding eigenspace.<p></p>
</div></details>
</div>
<p>In the above Example, <span class="math inline">\lambda = 1</span> has algebraic multiplicity <span class="math inline">2</span> and geometric multiplicity <span class="math inline">1</span>. <span class="math inline">\lambda = 2</span> has algebraic multiplicity <span class="math inline">1</span> and geometric multiplicity <span class="math inline">1</span>.</p>
<div id="eigenvaluestriangular" class="Corollary">
<p></p><details class="Corollary fbx-simplebox fbx-default" open=""><summary><strong>Corollary 5.6</strong></summary><div>The eigenvalues of a triangular matrix are the entries on its main diagonal.<p></p>
</div></details>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.6</strong></span> Let <span class="math display">A = \left[\begin{array}{cccc} 2 &amp; 0 &amp; 0 &amp; 0 \\ -1 &amp; 1 &amp; 0 &amp; 0 \\ 3 &amp; 0 &amp; 3 &amp; 0 \\ 5 &amp; 7 &amp; 4 &amp; -2 \end{array}\right]</span> The characteristic polynomial is: <span class="math display">\begin{alignedat}{2}
        |A - \lambda I| &amp;= \left|\begin{array}{cccc} 2-\lambda &amp; 0 &amp; 0 &amp; 0 \\ -1 &amp; 1-\lambda &amp; 0 &amp; 0 \\ 3 &amp; 0 &amp; 3-\lambda &amp; 0 \\ 5 &amp; 7 &amp; 4 &amp; -2-\lambda \end{array}\right| \\
        {} &amp;= (2-\lambda )\left|\begin{array}{ccc} 1-\lambda &amp; 0 &amp; 0 \\ 0 &amp; 3-\lambda &amp; 0 \\ 7 &amp; 4 &amp; -2-\lambda \end{array}\right| \\
        {} &amp;= (2-\lambda )(1-\lambda )\left|\begin{array}{cc} 3-\lambda &amp; 0 \\ 4 &amp; -2-\lambda \end{array}\right| \\
        {} &amp;= (2-\lambda )(1-\lambda )(3-\lambda )(-2-\lambda )
    \end{alignedat}</span></p>
<p>Hence, the eigenvalues are <span class="math inline">\lambda _{1}=2, \lambda _{2}=1, \lambda _{3}=3, \lambda _{4}=-2</span>.</p>
</div>
<p>Note that diagonal matrices are a special case of Corollary <a href="eigenvalues.html#eigenvaluestriangular">5.6</a>.</p>
<div id="eigenlinind" class="Theorem">
<p></p><details class="Theorem fbx-simplebox fbx-default" open=""><summary><strong>Theorem 5.7</strong></summary><div>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> matrix and let <span class="math inline">\lambda _{1}, \lambda _{2}, ..., \lambda _{m}</span> be distinct eigenvalues of <span class="math inline">A</span> with corresponding eigenvectors <span class="math inline">\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}</span>. Then <span class="math inline">\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}</span> are linearly independent.<p></p>
</div></details>
</div>
<div id="Proof*-5.5" class="Proof">
<p></p><details class="Proof fbx-default closebutton"><summary><strong>Proof</strong></summary><div>We prove this by contradiction.<p></p>
<p>Suppose <span class="math inline">\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}</span> are linearly dependent. Let <span class="math inline">\mathbf{v_{k+1}}</span> be the first of the vectors <span class="math inline">\mathbf{v_{i}}</span> that can be expressed as a linear combination of the previous ones. In other words, <span class="math inline">\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{k}}</span> are linearly independent, but there are <span class="math inline">c_{1}, c_{2}, ..., c_{k}</span> such that <span class="math display">\mathbf{v_{k+1}} = c_{1}\mathbf{v_{1}} + c_{2}\mathbf{v_{2}} + ... + c_{k}\mathbf{v_{k}} \qquad (1)</span></p>
<p>Multiplying both sides of Equation (1) by <span class="math inline">A</span> from left and using the fact that <span class="math inline">A\mathbf{v_{i}} = \lambda _{i}\mathbf{v_{i}}</span> for each <span class="math inline">i</span>, we have <span class="math display">\begin{alignedat}{2}
        \lambda _{k+1}\mathbf{v_{k+1}} = A\mathbf{v_{k+1}} &amp;= A(c_{1}\mathbf{v_{1}} + c_{2}\mathbf{v_{2}} + ... + c_{k}\mathbf{v_{k}}) \\
        {} &amp;= c_{1}A\mathbf{v_{1}} + c_{2}A\mathbf{v_{2}} + ... + c_{k}A\mathbf{v_{k}} \\
        {} &amp;= c_{1}\lambda _{1}\mathbf{v_{1}} + c_{2}\lambda _{2}\mathbf{v_{2}} + ... + c_{k}\lambda _{k}\mathbf{v_{k}} \qquad (2)
    \end{alignedat}</span></p>
<p>Now we multiply both sides of Equation (1) by <span class="math inline">\lambda _{k+1}</span> to obtain <span class="math display">\lambda _{k+1}\mathbf{v_{k+1}} = c_{1}\lambda _{k+1}\mathbf{v_{1}} + c_{2}\lambda _{k+1}\mathbf{v_{2}} + ... + c_{k}\lambda _{k+1}\mathbf{v_{k}} \qquad (3)</span></p>
<p>When we subtract Equation (3) from Equation (2), we obtain <span class="math display">\mathbf{0} = c_{1}(\lambda _{1}-\lambda _{k+1})\mathbf{v_{1}} + c_{2}(\lambda _{2}-\lambda _{k+1})\mathbf{v_{2}} + ... + c_{k}(\lambda _{k}-\lambda _{k+1})\mathbf{v_{k}}</span></p>
<p>The linear independence of <span class="math display">\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{k}}</span> implies that <span class="math display">c_{1}(\lambda _{1}-\lambda _{k+1}) = c_{2}(\lambda _{2}-\lambda _{k+1}) = ... = c_{k}(\lambda _{k}-\lambda _{k+1}) = 0</span></p>
<p>Since the eigenvalues <span class="math inline">\lambda _{i}</span> are all distinct, <span class="math inline">\lambda _{i} - \lambda _{k+1} \neq 0</span> for all <span class="math inline">i = 1, ..., k</span>. Hence <span class="math inline">c_{1} = c_{2} = ... = c_{k} = 0</span>. This implies that <span class="math display">\mathbf{v_{k+1}} = 0\mathbf{v_{1}} + 0\mathbf{v_{2}} + ... + 0\mathbf{v_{k}} = \mathbf{0}</span></p>
<p>which is impossible since the eigenvector <span class="math inline">\mathbf{v_{k+1}}</span> cannot be zero.</p>
<p>Thus, our assumption that <span class="math inline">\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}</span> are linearly dependent is false. It follows that <span class="math inline">\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}</span> must be linearly independent.&nbsp;â—»</p>
</div></details>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.7 (Coupled oscillators, normal modes)</strong></span> Consider a system of two coupled oscillators connected by springs as shown in the figure.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="coupledosc.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Coupled Oscillators</figcaption>
</figure>
</div>
<p>We can write down a system of equations for the dynamics of these two oscillators: <span class="math display"> m \ddot{x_1} = - k x_1 + k (x_2-x_1) </span> <span class="math display"> m \ddot{x_x} = - k (x_2 -x_1) - k x_2 </span> We can write this as a vector equation for the vector <span class="math display">\vec{x} =\begin{pmatrix} x_1 \\ x_2 \end{pmatrix} </span></p>
<p><span class="math display"> m \ddot{\vec{x}} = \begin{pmatrix} -2 k &amp; k \\ k &amp; -2k  \end{pmatrix} \vec{x}</span></p>
<p>If we are looking for the <strong>normal modes</strong> of the system, we are looking for harmonic oscillations with a single frequency. So we assume</p>
<p><span class="math display">\vec{x} = \begin{pmatrix} x_{10} \\ x_{20} \end{pmatrix} e^{i \omega t} \quad \ddot{\vec{x}} = - \omega^2 \begin{pmatrix} x_{10} \\ x_{20} \end{pmatrix} e^{i \omega t}</span> Substituting in the vector equation above leads to an eigenvalue problem <span class="math display"> \Rightarrow - m \omega^2 \vec{x_0} = \begin{pmatrix} -2 k &amp; k \\ k &amp; -2k  \end{pmatrix} \vec{x_0}</span> <span class="math display"> \Rightarrow  \begin{pmatrix} 2 k/m &amp; - k/m \\ - k/m &amp; 2k/m  \end{pmatrix} \vec{x_0} = \omega^2 \vec{x_0}  </span> for the frequency <span class="math inline">\omega</span>. The eigenvalues and eigenvectors of the matrix are<br>
<span class="math display"> \lambda_1 = 3 k/m \quad \vec{x_0} = \begin{pmatrix} -1\\ 1 \end{pmatrix}, </span> and <span class="math display"> \lambda_1 = k/m \quad \vec{x_0} = \begin{pmatrix} 1\\ 1 \end{pmatrix}, </span> corresponding to an oscillation with frequency <span class="math inline">\omega = \sqrt{3k/m}</span> where the two oscillators are 180 degree out of phase, and an oscillation with frequency <span class="math inline">\omega = \sqrt{k/m}</span> where the oscillators are in phase.</p>
<p>Other solutions for the system can now be obtained as a linear combination of these two modes. The modes are a basis of the solution space.</p>
</div>
</section>
<section id="similarity-and-diagonalisation" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="similarity-and-diagonalisation"><span class="header-section-number">5.2</span> Similarity and Diagonalisation</h2>
<section id="introduction" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">5.2.1</span> Introduction</h3>
<p>In many applications, matrices represent linear mappings of vectors in a physical space, as in the example given at the start of the Eigenvector section. The choice of a coordinate system (in particular, its orientation) in this space is arbitrary, and this choice determines what the matrix looks like. In this section, we show that under certain conditions, there exists a choice of a coordinate system in which the matrix becomes a diagonal matrix. In this case, the coordinate axes have the direction of the eigenvectors of the matrix, and the diagonal elements of the matrix are the eigenvalues.</p>
</section>
<section id="similar-matrices" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="similar-matrices"><span class="header-section-number">5.2.2</span> Similar Matrices</h3>
<div id="similar-matrices-1" class="Definition" title="Similar Matrices">
<p></p><details class="Definition fbx-simplebox fbx-default" open=""><summary><strong>Definition 5.8: </strong>Similar Matrices</summary><div>Let <span class="math inline">A</span> and <span class="math inline">B</span> be <span class="math inline">n \times n</span> matrices. We say that <span class="math inline">A</span> is similar to <span class="math inline">B</span> if there is an invertible <span class="math inline">n \times n</span> matrix <span class="math inline">P</span> such that <span class="math inline">P^{-1}AP = B</span>. If <span class="math inline">A</span> is similar to <span class="math inline">B</span>, we write <span class="math inline">A \sim B</span>.<p></p>
</div></details>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">A \sim B</span>, we can write, equivalently, that <span class="math inline">A = PBP^{-1}</span> or <span class="math inline">AP = PB</span>. The matrix <span class="math inline">P</span> depends on <span class="math inline">A</span> and <span class="math inline">B</span>. It is not unique for a given pair of similar matrices <span class="math inline">A</span> and <span class="math inline">B</span>.</p>
</div>
</div>
<div id="simmatrices" class="Theorem" title="Properties of Similar Matrices">
<p></p><details class="Theorem fbx-simplebox fbx-default" open=""><summary><strong>Theorem 5.9: </strong>Properties of Similar Matrices</summary><div>Let <span class="math inline">A</span> and <span class="math inline">B</span> be <span class="math inline">n \times n</span> matrices with <span class="math inline">A \sim B</span>. Then<br>
(a) <span class="math inline">{\rm det}(A) = {\rm det}(B)</span><br>
(b) <span class="math inline">A</span> and <span class="math inline">B</span> have the same rank.<br>
(c) <span class="math inline">A</span> and <span class="math inline">B</span> have the same characteristic polynomial.<br>
(d) <span class="math inline">A</span> and <span class="math inline">B</span> have the same eigenvalues.<p></p>
</div></details>
</div>
<div id="Proof*-5.6" class="Proof">
<p></p><details class="Proof fbx-default closebutton"><summary><strong>Proof</strong></summary><div>If <span class="math inline">A \sim B</span>, then <span class="math inline">{\rm P^{-1}AP = B}</span> for some invertible matrix <span class="math inline">P</span>.<p></p>
<p>(a)<br>
<span class="math display">\begin{alignedat}{2}
            \qquad {\rm det}(B) &amp;= {\rm det}(P^{-1}AP) \\
            {} &amp;= {\rm det(P^{-1}){\rm det}(A){\rm det}(P)} \\
            {} &amp;= \frac{1}{{\rm det}(P)}{\rm det(A){\rm det}(P)} \\
            {} &amp;= {\rm det}(A). \\
        \end{alignedat}</span></p>
<p>(b)<br>
We first show that nullity(A)=nullity(B). One can then use the rank theorem <a href="linearequations.html#ranktheorem">3.19</a> to show that this also implies that rank(A)=rank(B).</p>
<p>Let <span class="math inline">\{\vec{x_1},..., \vec{x_k}\}</span> be a basis of the nullspace of A. Hence <span class="math inline">\vec{y}_i= P^{-1}\vec{x_i}</span>, <span class="math inline">i=1..k</span>, is a linearly independent set (P is one-to-one) in the nullspace of B, as <span class="math display"> B \vec{y}_i = P^{-1}A P P^{-1}\vec{x_i} = P^{-1}A \vec{x_i} = P^{-1}\vec{0} = \vec{0}. </span> To show that the <span class="math inline">\vec{y}_i</span> indeed are a basis of the nullspace of B, we observe that for any <span class="math inline">\vec{y}</span> in the nullspace of B, <span class="math inline">P\vec{y} = \vec{x}</span> is in the nullspace of A. So <span class="math inline">\vec{y}_1, ... \vec{y}_k</span> are a basis of the nullspace of B and nullity(A)=nullity(B).</p>
<p>(c) The characteristic polynomial of <span class="math inline">B</span> is <span class="math display">\begin{alignedat}{2}
            {\rm det}(B - \lambda I) &amp;= {\rm det}(P^{-1}AP - \lambda I) \\
            {} &amp;= {\rm det}(P^{-1}AP - \lambda P^{-1}IP) \\
            {} &amp;= {\rm det}(P^{-1}AP - P^{-1}(\lambda I)P) \\
            {} &amp;= {\rm det}(P^{-1}(A - \lambda I)P) \\
            {} &amp;= {\rm det}(P^{-1}){\rm det}(A - \lambda I){\rm det}(P) \\
            {} &amp;= \frac{1}{{\rm det}(P)}{\rm det}(A - \lambda I){\rm det}(P) \\
            {} &amp;= {\rm det}(A - \lambda I)  
        \end{alignedat}</span> &nbsp;â—»</p>
</div></details>
</div>
<p>Theorem <a href="eigenvalues.html#simmatrices">5.9</a> is helpful in showing that two matrices are not similar, since <span class="math inline">A</span> and <span class="math inline">B</span> cannot be similar if any of the properties fail.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.8</strong></span> &nbsp;</p>
<ol type="a">
<li>The two matrices <span class="math display">A = \left[\begin{array}{cc} 1 &amp; 2 \\ 2 &amp; 1 \end{array}\right], \quad B = \left[\begin{array}{cc} 2 &amp; 1 \\ 1 &amp; 2 \end{array}\right], </span> are not similar since <span class="math inline">{\rm det}(A) = -3</span> but <span class="math inline">{\rm det}(B) = 3</span>.<br>
</li>
<li>The two matrices <span class="math display">A = \left[\begin{array}{cc} 1 &amp; 3 \\ 2 &amp; 2 \end{array}\right], \quad B = \left[\begin{array}{cc} 1 &amp; 1 \\ 3 &amp; -1 \end{array}\right]</span> are not similar, since <span class="math inline">|A - \lambda I| = \lambda ^{2} - 3\lambda - 4</span> while <span class="math inline">|B - \lambda I| = \lambda ^{2} - 4</span>. Note that <span class="math inline">A</span> and <span class="math inline">B</span> have the same determinant and rank, however.</li>
</ol>
</div>
</section>
<section id="diagonalisation" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="diagonalisation"><span class="header-section-number">5.2.3</span> Diagonalisation</h3>
<div id="diagonalisable-matrices" class="Definition" title="Diagonalisable Matrices">
<p></p><details class="Definition fbx-simplebox fbx-default" open=""><summary><strong>Definition 5.10: </strong>Diagonalisable Matrices</summary><div>An <span class="math inline">n \times n</span> matrix <span class="math inline">A</span> is diagonalisable if there is a diagonal matrix <span class="math inline">D</span> such that <span class="math inline">A</span> is similar to <span class="math inline">D</span> - that is, if there is an invertible <span class="math inline">n \times n</span> matrix <span class="math inline">P</span> such that <span class="math inline">P^{-1}AP = D</span>.<p></p>
</div></details>
</div>
<div id="diagifind" class="Theorem" title="Condition for Diagonalisability">
<p></p><details class="Theorem fbx-simplebox fbx-default" open=""><summary><strong>Theorem 5.11: </strong>Condition for Diagonalisability</summary><div>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> matrix. Then <span class="math inline">A</span> is diagonalisable if and only if <span class="math inline">A</span> has <span class="math inline">n</span> linearly independent eigenvectors.<br>
More precisely, there exists an invertible matrix <span class="math inline">P</span> and a diagonal matrix <span class="math inline">D</span> such that <span class="math inline">P^{-1}AP = D</span> if and only if the columns of <span class="math inline">P</span> are <span class="math inline">n</span> linearly independent eigenvectors of <span class="math inline">A</span> and the diagonal entries of <span class="math inline">D</span> are the eigenvalues of <span class="math inline">A</span> corresponding to the eigenvectors in <span class="math inline">P</span> in the same order.*<p></p>
</div></details>
</div>
<div id="Proof*-5.7" class="Proof">
<p></p><details class="Proof fbx-default closebutton"><summary><strong>Proof</strong></summary><div>Suppose first that <span class="math inline">A</span> is similar to the diagonal matrix <span class="math inline">D</span> by <span class="math inline">P^{-1}AP = D</span> or, equivalently, <span class="math inline">AP = PD</span>. Let the columns of <span class="math inline">P</span> be <span class="math inline">\mathbf{p_{1}}, \mathbf{p_{2}}, ..., \mathbf{p_{n}}</span> and let the diagonal entries of <span class="math inline">D</span> be <span class="math inline">\lambda _{1}, \lambda _{2}, ..., \lambda _{n}</span>. Then <span class="math display">A\left[\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots, \mathbf{p_{n}}\right] = \left[\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots, \mathbf{p_{n}}\right] \left[\begin{array}{cccc} \lambda _{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda _{2} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda _{n} \end{array}\right] \qquad (1)</span> or <span class="math display">\qquad \left[A\mathbf{p_{1}}, A\mathbf{p_{2}}, \cdots, A\mathbf{p_{n}}\right] = \left[\lambda _{1}\mathbf{p_{1}}, \lambda _{2}\mathbf{p_{2}}, \cdots, \lambda _{n}\mathbf{p_{n}}\right] \qquad (2)</span><p></p>
<p>Equating columns, we have <span class="math display">A\mathbf{p_{1}} = \lambda _{1}\mathbf{p_{1}}, A\mathbf{p_{2}} = \lambda _{2}\mathbf{p_{2}}, \cdots , A\mathbf{p_{n}} = \lambda _{n}\mathbf{p_{n}}</span> which proves that the column vectors of <span class="math inline">P</span> are eigenvectors of <span class="math inline">A</span> whose corresponding eigenvalues are the diagonal entries of <span class="math inline">D</span> in the same order. Since <span class="math inline">P</span> is invertible, its columns are linearly independent.</p>
<p>Conversely, if <span class="math inline">A</span> has <span class="math inline">n</span> linearly independent eigenvectors <span class="math inline">\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots , \mathbf{p_{n}}</span> with corresponding eigenvalues <span class="math inline">\lambda _{1}, \lambda _{2}, \cdots , \lambda _{n}</span>, respectively, then <span class="math display">A\mathbf{p_{1}} = \lambda _{1}\mathbf{p_{1}}, A\mathbf{p_{2}} = \lambda _{2}\mathbf{p_{2}}, \cdots , A\mathbf{p_{n}} = \lambda _{n}\mathbf{p_{n}}</span></p>
<p>This implies Eq. (2), which is equivalent to Eq. (1), that is <span class="math inline">AP = PD</span>. Since the columns <span class="math inline">\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots , \mathbf{p_{n}}</span> of <span class="math inline">P</span> are linearly independent, <span class="math inline">P</span> is invertible, so <span class="math inline">P^{-1}AP = D</span>, that is, <span class="math inline">A</span> is diagonalisable.&nbsp;â—»</p>
</div></details>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.9</strong></span> If possible, find a matrix <span class="math inline">P</span> that diagonalises <span class="math display">A = \left[\begin{array}{ccc}
            0 &amp; 1 &amp; 0 \\
            0 &amp; 0 &amp; 1 \\
            2 &amp; -5 &amp; 4
        \end{array}\right]</span></p>
</div>
<div id="Solution*-5.8" class="Solution">
<p></p><details class="Solution fbx-default closebutton"><summary><strong>Solution</strong></summary><div>We studied this matrix previously and found that it has eigenvalues <span class="math inline">\lambda _{1} = \lambda _{2} = 1</span> and <span class="math inline">\lambda _{3} = 2</span>. The eigenspaces have the following bases:<br>
For <span class="math inline">\lambda _{1} = \lambda _{2} = 1</span>, <span class="math inline">E_{1}</span> has basis <span class="math inline">\left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right]</span>.<br>
For <span class="math inline">\lambda _{3} = 2</span>, <span class="math inline">E_{2}</span> has basis <span class="math inline">\left[\begin{array}{c} 1 \\ 2 \\ 4 \end{array}\right]</span>.<br>
Since all other eigenvectors are just multiples of one of these two basis vectors, there cannot be three linearly independent eigenvectors. By Theorem 4.6, <span class="math inline">A</span> is not diagonalisable.<p></p>
</div></details>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.10</strong></span> If possible, find a matrix <span class="math inline">P</span> that diagonalises <span class="math display">A = \left[\begin{array}{ccc}
            2 &amp; 2 &amp; 0 \\
            0 &amp; 1 &amp; 0 \\
            -4 &amp; -8 &amp; 1
        \end{array}\right]</span></p>
</div>
<div id="Solution*-5.9" class="Solution">
<p></p><details class="Solution fbx-default closebutton"><summary><strong>Solution</strong></summary><div>This is the matrix of Question 3, Worksheet 6. There we found that the eigenvalues of <span class="math inline">A</span> are <span class="math inline">\lambda _{1} = 2</span> and <span class="math inline">\lambda _{2} = \lambda _{3} = 1</span>, with the following bases for the eigenspaces:<br>
For <span class="math inline">\lambda _{1} = 2</span>, <span class="math inline">E_{2}</span> has basis <span class="math inline">\mathbf{p_{1}} = \left[\begin{array}{c} 1 \\ 0 \\ -4 \end{array}\right]</span>.<br>
For <span class="math inline">\lambda _{2} = \lambda _{3} = 1</span>, <span class="math inline">E_{1}</span> has basis <span class="math inline">\mathbf{p_{2}} = \left[\begin{array}{c} -2 \\ 1 \\ 0 \end{array}\right]</span> and <span class="math inline">\mathbf{p_{3}} = \left[\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right]</span>.<br>
Now we check whether <span class="math inline">\left\{\mathbf{p_{1}}, \mathbf{p_{2}}, \mathbf{p_{3}}\right\}</span> is linearly independent. <span class="math display">\left[\begin{array}{ccc} \textcircled{\raisebox{-0.9pt}{1}} &amp; -2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -4 &amp; 0 &amp; 1 \end{array}\right] \xrightarrow{R_{3} + 4R_{1}} \left[\begin{array}{ccc} 1 &amp; -2 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{1}} &amp; 0 \\ 0 &amp; -8 &amp; 1 \end{array}\right] \xrightarrow{R_{3} + 8R_{2}} \left[\begin{array}{ccc} 1 &amp; -2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right]</span><p></p>
<p>Since <span class="math inline">rank = 3</span>, <span class="math inline">\left\{\mathbf{p_{1}}, \mathbf{p_{2}}, \mathbf{p_{3}}\right\}</span> is linearly independent. Thus, if we take <span class="math display">P = \left[\mathbf{p_{1}}, \mathbf{p_{2}}, \mathbf{p_{3}}\right] = \left[\begin{array}{ccc} 1 &amp; -2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -4 &amp; 0 &amp; 1 \end{array}\right]</span> then <span class="math inline">P</span> is invertible. Furthermore, <span class="math display">P^{-1}AP = \left[\begin{array}{ccc} 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right] = D</span></p>
<p>(Note: It is much easier to check the equivalent equation <span class="math inline">AP = PD</span>).</p>
</div></details>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>Eigenvectors can be placed into the columns of <span class="math inline">P</span> in any order. However, the eigenvalues will come up on the diagonal of <span class="math inline">D</span> in the same order as their corresponding eigenvectors in <span class="math inline">P</span>. For example, if we had chosen <span class="math display">P = \left[\mathbf{p_{2}}, \mathbf{p_{3}}, \mathbf{p_{1}}\right] = \left[\begin{array}{ccc} -2 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; -4 \end{array}\right]</span> Then we would have found <span class="math display">P^{-1}AP = \left[\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{array}\right]</span></p>
<p>We checked that the eigenvectors <span class="math inline">\mathbf{p_{1}}, \mathbf{p_{2}}</span> and <span class="math inline">\mathbf{p_{3}}</span> were linearly independent. However, the following Theorem guarantees that linear independence is preserved when the bases of different subspaces are combined.</p>
</div>
</div>
<div id="section" class="Theorem" title="">
<p></p><details class="Theorem fbx-simplebox fbx-default" open=""><summary><strong>Theorem 5.12</strong></summary><div>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> matrix and let <span class="math inline">\lambda _{1}, \lambda _{2}, \cdots, \lambda _{k}</span> be distinct eigenvalues of <span class="math inline">A</span>. If <span class="math inline">B_{i}</span> is a basis for the eigenspace <span class="math inline">E_{i}</span>, then <span class="math inline">B = B_{1} \cup B_{2} \cup \cdots \cup B_{k}</span> (i.e.&nbsp;the total collection of basis vectors for all of the eigenspaces) is linearly independent.<p></p>
</div></details>
</div>
<div id="eigendiag" class="Theorem">
<p></p><details class="Theorem fbx-simplebox fbx-default" open=""><summary><strong>Theorem 5.13</strong></summary><div>If <span class="math inline">A</span> is an <span class="math inline">n \times n</span> matrix with <span class="math inline">n</span> distinct eigenvalues, then <span class="math inline">A</span> is diagonalisable.<p></p>
</div></details>
</div>
<div id="Proof*-5.10" class="Proof">
<p></p><details class="Proof fbx-default closebutton"><summary><strong>Proof</strong></summary><div>Let <span class="math inline">\mathbf{v_{1}}, \mathbf{v_{2}}, \cdots, \mathbf{v_{n}}</span> be eigenvectors corresponding to the <span class="math inline">n</span> distinct eigenvalues of <span class="math inline">A</span>. By theorem <a href="eigenvalues.html#eigenlinind">5.7</a>, <span class="math inline">\mathbf{v_{1}}, \mathbf{v_{2}}, \cdots, \mathbf{v_{n}}</span> are linearly independent, so, by Theorem <a href="eigenvalues.html#diagifind">5.11</a>, <span class="math inline">A</span> is diagonalisable.&nbsp;â—»<p></p>
</div></details>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.11</strong></span> The matrix <span class="math display">A = \left[\begin{array}{cccc}
            2 &amp; 0 &amp; 0 &amp; 0 \\
            -1 &amp; 1 &amp; 0 &amp; 0 \\
            3 &amp; 0 &amp; 3 &amp; 0 \\
            5 &amp; 7 &amp; 4 &amp; -2
        \end{array}\right]</span> has eigenvalues <span class="math inline">\lambda _{1} = 2, \lambda _{2} = 1, \lambda _{3} = 3</span> and <span class="math inline">\lambda _{4} = -2</span>, by Corollary <a href="eigenvalues.html#eigenvaluestriangular">5.6</a>. Since these are four distinct eigenvalues for a <span class="math inline">4 \times 4</span> matrix, <span class="math inline">A</span> is diagonalisable, by Theorem <a href="eigenvalues.html#eigendiag">5.13</a>.</p>
</div>
<div id="Corollary-5.14" class="Corollary">
<p></p><details class="Corollary fbx-simplebox fbx-default" open=""><summary><strong>Corollary 5.14</strong></summary><div>If <span class="math inline">A</span> is an <span class="math inline">n \times n</span> matrix, then the geometric multiplicity of each eigenvalue is less than or equal to its algebraic multiplicity.<p></p>
</div></details>
</div>
<div id="diagonalisation-theorem" class="Theorem" title="Diagonalisation Theorem">
<details class="Theorem fbx-simplebox fbx-default" open=""><summary><strong>Theorem 5.15: </strong>Diagonalisation Theorem</summary><div>
<p>Let <span class="math inline">A</span> be an <span class="math inline">n \times n</span> matrix whose distinct eigenvalues are <span class="math inline">\lambda _{1}, \lambda _{2}, \cdots, \lambda _{k}</span>, where <span class="math inline">1\leq k\leq n</span>. The following statements are equivalent:</p>
<ol type="a">
<li><p><span class="math inline">A</span> is diagonalisable.</p></li>
<li><p>The union <span class="math inline">B</span> of the bases of the eigenspaces of <span class="math inline">A</span> contains <span class="math inline">n</span> vectors.</p></li>
<li><p>The algebraic multiplicity of each eigenvalue equals its geometric multiplicity.</p></li>
</ol>
</div></details>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.12</strong></span> &nbsp;</p>
<ol type="a">
<li>The matrix <span class="math display">A = \left[\begin{array}{ccc}
    0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 2 &amp; -5 &amp; 4 \end{array}\right]</span> has two distinct eigenvalues <span class="math inline">\lambda _{1} = \lambda _{2} = 1</span> and <span class="math inline">\lambda _{3} = 2</span>. Since the eigenvalue <span class="math inline">\lambda _{1} = \lambda _{2} = 1</span> with <span class="math inline">E_{1} = {\rm span}( (1,1,1)^T)</span> has algebraic multiplicity 2 but geometric multiplicity 1, <span class="math inline">A</span> is not diagonalisable, by the Diagonalisation Theorem.<br>
</li>
<li>The matrix <span class="math display">A = \left[\begin{array}{ccc}
    2 &amp; 2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -4 &amp; -8 &amp; 1 \end{array}\right]</span> has two distinct eigenvalues <span class="math inline">\lambda _{1} = 2</span> and <span class="math inline">\lambda _{2} = \lambda _{3} = 1</span>. We found: for <span class="math inline">\lambda _{1} = 2</span>, <span class="math inline">E_{1}</span> has basis <span class="math inline">\mathbf{p_{1}} = (1, 0, -4)^T</span>, and for <span class="math inline">\lambda _{2} = \lambda _{3} = 1</span>, <span class="math inline">E_{2}</span> has basis <span class="math inline">\mathbf{p_{2}} = (-2,1, 0)^T</span> and <span class="math inline">\mathbf{p_{3}} = (0,0,1)^T</span>. Thus, the eigenvalue 2 has algebraic and geometric multiplicity 1, and the eigenvalue 1 has algebraic and geometric multiplicity 2. Thus, <span class="math inline">A</span> is diagonalisable, by the Diagonalisation Theorem.</li>
</ol>
</div>
<div id="joint-eigenbasis" class="Theorem" title="Joint eigenbasis">
<p></p><details class="Theorem fbx-simplebox fbx-default" open=""><summary><strong>Theorem 5.16: </strong>Joint eigenbasis</summary><div>If two matrices commute and one of them is diagonalisable then they share a basis of eigenvectors.<p></p>
</div></details>
</div>
<div id="Proof*-5.11" class="Proof">
<p></p><details class="Proof fbx-default closebutton"><summary><strong>Proof</strong></summary><div>Let <span class="math inline">{\rm A}</span> be diagonalisable. Then <span class="math inline">{\rm A}</span> has an eigenbasis <span class="math inline">\vec{x}_i, i=1..n</span>. For simplicity we first assume that there are <span class="math inline">n</span> distinct eigenvalues, but one also prove the theorem for repeated eigenvalues.<p></p>
<p>Let <span class="math inline">{\rm B}</span> be a matrix that commutes with <span class="math inline">{\rm A}</span>:<br>
<span class="math display"> {\rm A B = B A} \quad \Rightarrow {\rm A B}\vec{x}_i ={\rm B A}\vec{x}_i \quad \Rightarrow {\rm A}({\rm  B}\vec{x}_i) = \lambda_i ({\rm B} \vec{x}_i)  </span> That means that <span class="math inline">{\rm B} \vec{x}_i</span> is also an eigenvector of <span class="math inline">{\rm A}</span> to the eigenvalue <span class="math inline">\lambda_i</span> and since the eigenspace to <span class="math inline">\lambda_i</span> is 1-dimensional and spanned by <span class="math inline">\vec{x}_i</span> we have <span class="math inline">{\rm B} \vec{x}_i = \mu_i \vec{x}_i</span>, for some real number <span class="math inline">\mu_i</span>. This means that <span class="math inline">\vec{x}_i</span> is also an eigenvector of <span class="math inline">{\rm B}</span>. A similar argumentation works for repeated eigenvalues.</p>
</div></details>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>Joint eigenbases play an important role in quantum mechanics, where the eigenstates of commuting operators can be expressed w.r.t.~a common basis. It also means that measurements of these operator can be carried out simultaneously with (theoretically) infinite precision, while non-commuting operators have to satisfy a Heisenberg Uncertainty Principle.</p>
</div>
</div>
</section>
</section>
<section id="adjoint-operator" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="adjoint-operator"><span class="header-section-number">5.3</span> Adjoint Operator</h2>
<p>If our vector space is equipped with an inner product <span class="math inline">\langle . , . \rangle</span>, and we are given a linear operator (matrix) <span class="math inline">{\rm A}</span> then there exists a further linear operator closely related to <span class="math inline">{\rm A}</span>, the Hermitian adjoint.</p>
<div id="hermitian-adjoint-operator" class="Definition" title="(Hermitian) Adjoint Operator">
<p></p><details class="Definition fbx-simplebox fbx-default" open=""><summary><strong>Definition 5.17: </strong>(Hermitian) Adjoint Operator</summary><div>Given a linear operator <span class="math inline">{\rm A}</span> the hermitian adjoint of <span class="math inline">{\rm A}</span> is defined by <span class="math display">\langle {\rm A}\vec{x},\vec{y} \rangle = \langle \vec{x},{\rm A}^\dag \vec{y} \rangle. </span> The operator <span class="math inline">{\rm A}</span> is called self-adjoint if <span class="math inline">{\rm A} = {\rm A}^\dag</span>.<p></p>
</div></details>
</div>
<p>For the simplest case of a real, finite-dimensional vector space with an inner product given by the standard scalar product the hermitian adjoint of <span class="math inline">{\rm A}</span> is just the transpose of <span class="math inline">{\rm A}</span>. This is because the scalar product <span class="math inline">\vec{x}\cdot\vec{y}</span> can be written also as a matrix product <span class="math inline">\vec{x}^T \vec{y}</span> and hence <span class="math display">\langle {\rm A}\vec{x},\vec{y} \rangle = ({\rm A}\vec{x})^T\vec{y} = \vec{x}^T{\rm A}^T\vec{y} = \langle \vec{x},{\rm A}^T\vec{y} \rangle. </span></p>
<div id="properties-of-the-hermitian-adjoint" class="Corollary" title="Properties of the hermitian adjoint">
<details class="Corollary fbx-simplebox fbx-default" open=""><summary><strong>Corollary 5.18: </strong>Properties of the hermitian adjoint</summary><div>
<ul>
<li><span class="math inline">({\rm A}^\dag)^\dag = {\rm A}</span></li>
<li><span class="math inline">{\rm A}^\dag + {\rm B}^\dag = ({\rm A} +{\rm B})^\dag</span></li>
<li><span class="math inline">({\rm A}{\rm B})^\dag = {\rm B}^\dag {\rm A}^\dag</span></li>
</ul>
</div></details>
</div>
<div id="orthogonal-eigenbasis" class="Corollary" title="orthogonal eigenbasis">
<p></p><details class="Corollary fbx-simplebox fbx-default" open=""><summary><strong>Corollary 5.19: </strong>orthogonal eigenbasis</summary><div>For a self-adjoint operator eigenvectors to different eigenvalues are orthogonal.<p></p>
</div></details>
</div>
<div id="Proof*-5.12" class="Proof">
<p></p><details class="Proof fbx-default closebutton"><summary><strong>Proof</strong></summary><div><span class="math display"> \langle {\rm A}\vec{x},\vec{y} \rangle = \langle \vec{x},{\rm A}^\dag \vec{y} \rangle  = \langle \vec{x},{\rm A} \vec{y} \rangle</span> <span class="math display"> \Rightarrow \langle \lambda_x \vec{x},\vec{y} \rangle = \langle \vec{x},\lambda_y \vec{y} \rangle  \quad \Rightarrow \underbrace{(\lambda_x -\lambda_y)}_{\neq 0} \langle \vec{x},\vec{y} \rangle = 0  \quad \Rightarrow \langle \vec{x},\vec{y} \rangle = 0 </span><p></p>
</div></details>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/dundeemath\.github\.io\/MA21001");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./determinants.html" class="pagination-link" aria-label="Determinants">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Determinants</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./appendix.html" class="pagination-link" aria-label="Appendix">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Appendix</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Eigenvalues and Eigenvectors</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction to Eigenvalues and Eigenvectors</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>Consider an elastic body, e.g., a rubber ball. If we compress the ball</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>in one direction, it will be elongated in the plane perpendicular to</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>that direction. The figure below shows, in a cross-section, how</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>individual points on the surface move under the compression.</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>::: {#fig-deformation layout-ncol=2}</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="al">![](eigenv1.png)</span>{width=50%}</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="al">![](eigenv2.png)</span>{width=50%}</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>Effect of a deformation on the points (vectors) in an</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>elastic sphere.</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>There are some vectors in these plots that don't change direction, but</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>only their length under the deformation. We can find those by</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>superimposing the two plots:</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>::: {#fig-deformation2 layout-ncol=2}</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="al">![](eigenv3.png)</span>{width=50%}</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="al">![](eigenv4.png)</span>{width=50%}</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>There are directions in which the vectors only change their</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>length.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>These directions are characteristic of the deformation, as is the factor</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>of elongation/compression along these directions. These directions can</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>be found mathematically as the eigenvectors of a matrix, the deformation</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>matrix, which maps the original vectors to the vectors of the deformed</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>ball. The eigenvalues are the factors of compression/elongation along</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>these directions. The word \"eigen\" is German for \"own\" and was</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>likely introduced by David Hilbert (also known for Hilbert spaces).</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>Eigenvalues and eigenvectors play an essential role in mathematics,</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>physics and engineering. The stability of an equilibrium of a dynamical</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>system is determined by the eigenvalues of a matrix that describes the</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>linearised system at the equilibrium point. The values of a measurement</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>in quantum mechanics are the eigenvalues of an operator. The principal</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>axes of a rigid body are the eigenvectors of the moment of inertia</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>tensor.</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>::: {.Definition}</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="fu">## Eigenvalue and Eigenvector</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a> Let $A$ be an $n \times n$ matrix. A scalar $\lambda$ is called an eigenvalue of $A$ if there is a nonzero vector</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>$\mathbf{x}$ such that $A\mathbf{x} = \lambda \mathbf{x}$. Such a vector $\mathbf{x}$ is called an</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>eigenvector of $A$ corresponding to $\lambda$.</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>::: {#exm-    }</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>Suppose</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>$A\left<span class="co">[</span><span class="ot">\begin{array}{c} x \\ y \end{array}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{c} 2x \\ 0 \end{array}\right</span><span class="co">]</span>$.</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>Then</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>$$A\left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 0 \end{array}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{c} 2 \\ 0 \end{array}\right</span><span class="co">]</span> = 2\left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 0 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>so $2$ is an eigenvalue and</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>$\left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 0 \end{array}\right</span><span class="co">]</span>$ a corresponding</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>eigenvector. Also,</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>$$A\left<span class="co">[</span><span class="ot">\begin{array}{c} 0 \\ 1 \end{array}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{c} 0 \\ 0 \end{array}\right</span><span class="co">]</span> = 0\left<span class="co">[</span><span class="ot">\begin{array}{c} 0 \\ 1 \end{array}\right</span><span class="co">]</span>,$$</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>so $0$ is an eigenvalue and</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>$\left<span class="co">[</span><span class="ot">\begin{array}{c} 0 \\ 1 \end{array}\right</span><span class="co">]</span>$ a corresponding</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>eigenvector.</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>Notice that $\left<span class="co">[</span><span class="ot">\begin{array}{c} \alpha \\ 0 \end{array}\right</span><span class="co">]</span>$ and $\left<span class="co">[</span><span class="ot">\begin{array}{c} 0 \\ \alpha \end{array}\right</span><span class="co">]</span>$ are eigenvectors for any $\alpha \neq 0$.\</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>In general, if $\mathbf{v}$ is an eigenvector of $A$, then so is $\alpha \mathbf{v}$ for any nonzero scalar $\alpha$.</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>::: {#exm-    }</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>Show that $5$ is an eigenvalue of</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>$A = \left<span class="co">[</span><span class="ot">\begin{array}{cc} 1 &amp; 2 \\ 4 &amp; 3 \end{array}\right</span><span class="co">]</span>$ and</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>determine all eigenvectors corresponding to this eigenvalue.</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>::: {.Solution}</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>We must show that there is a nonzero vector $\mathbf{x}$ such that</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>$A\mathbf{x} = 5\mathbf{x}$, which is equivalent to the equation</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>$(A - 5I)\mathbf{x} = \mathbf{0}$. We compute the nullspace by:</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>$$\left<span class="co">[</span><span class="ot">A - 5I | \mathbf{0}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{cc|c} -4 &amp; 2 &amp; 0 \\ 4 &amp; -2 &amp; 0 \end{array}\right</span><span class="co">]</span> \xrightarrow{R_{2} + R_{1}} \left<span class="co">[</span><span class="ot">\begin{array}{cc|c} -4 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>Thus</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>$\mathbf{x} = \left<span class="co">[</span><span class="ot">\begin{array}{c} x_{1} \\ x_{2} \end{array}\right</span><span class="co">]</span> \in {\rm Null}(A-5I)$</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>satisfies $-4x_{1} + 2x_{2} = 0$, or $x_{2} = 2x_{1}$.\</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>Thus, $A\mathbf{x} = 5\mathbf{x}$ has a nontrivial solution of the form</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>$\mathbf{x} = \left<span class="co">[</span><span class="ot">\begin{array}{c} x_{1} \\ x_{2} \end{array}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{c} x_{1} \\ 2x_{1} \end{array}\right</span><span class="co">]</span> = x_{1}\left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 2 \end{array}\right</span><span class="co">]</span>$,</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>so $5$ is an eigenvalue of $A$ and the corresponding eigenvectors are</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>the nonzero multiples of</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>$\left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 2 \end{array}\right</span><span class="co">]</span>$.</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>::: {.Definition}</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="fu">## Eigenspace</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>Let $A$ be an $n \times n$ matrix and let $\lambda$ be an eigenvalue of $A$. The collection of all eigenvectors</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>corresponding to $\lambda$, together with the zero vector, is called the eigenspace of</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>$\lambda$ and is denoted by $E_{\lambda }$.</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>Therefore, in the above Example,</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>$E_{5} = \left<span class="sc">\{</span>t\left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 2 \end{array}\right</span><span class="co">]</span>\right<span class="sc">\}</span>$,</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>where $t \in \mathbb{R}$.</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>::: {.Theorem}</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="fu">## Characteristic Equation</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>Let $A$ be an $n \times n$ matrix. Then $\lambda$ is an eigenvalue of $A$ if and only if $|A-\lambda I_{n}| = 0$ (or ${\rm det}(A - \lambda I_{n}) = 0$).</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>::: {.Proof}</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>Suppose that $\lambda$ is an eigenvalue of $A$. Then $A\mathbf{v} = \lambda \mathbf{v}$ for some nonzero $\mathbf{v} \in \mathbb{R}^{n}$. This</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>is equivalent to $A\mathbf{v} = \lambda I_{n} \mathbf{v}$ or $(A - \lambda I_{n})\mathbf{v} = \mathbf{0}$. But this means that $\mathbf{v}$</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>is a nonzero solution to the homogeneous system of equations defined by</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>the matrix $A - \lambda I_{n}$. This means $A - \lambda I_{n}$ is singular, and so $|A - \lambda I_{n}| = 0$.\</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>Conversely, if $|A - \lambda I_{n}| = 0$ then $A - \lambda I_{n}$ is singular, and so the system of equations defined by</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>$A - \lambda I_{n}$ has nonzero solutions. Hence there exists a nonzero</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>$\mathbf{v} \in \mathbb{R}^{n}$ with $(A - \lambda I_{n})\mathbf{v} = \mathbf{0}$, which is equivalent to</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>$A \mathbf{v} = \lambda \mathbf{v}$, and so $\lambda$ is an eigenvalue of $A$.&nbsp;â—»</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>::: {.Definition}</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a><span class="fu">## Characteristic Equation/Polynomial</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>For an $n \times n$ matrix $A$, the equation $|A - \lambda I_{n}| = 0$ is called the characteristic equation of $A$,</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>and $|A - \lambda I_{n}|$ is called the characteristic polynomial of $A$.</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>:::: {#exm-    }</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>Find the eigenvalues and the corresponding eigenvectors</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>of $A = \left[\begin{array}{cc}</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>        1 &amp; 2 <span class="sc">\\</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>        4 &amp; -1</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>    \end{array}\right]$.</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>::: {.Solution}</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>The characteristic polynomial is</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>$$\begin{alignedat}{2}</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>        <span class="pp">|</span>A - \lambda I_{2}<span class="pp">|</span> &amp;= \left<span class="pp">|</span>\begin{array}{cc}</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>            1 - \lambda &amp; 2 <span class="sc">\\</span></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>            4 &amp; -1-\lambda </span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>        \end{array}\right| <span class="sc">\\</span></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (1-\lambda )(-1-\lambda ) - 8 <span class="sc">\\</span></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>        {} &amp;= \lambda ^{2} - 9 <span class="sc">\\</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (\lambda + 3)(\lambda - 3). <span class="sc">\\</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>    \end{alignedat}$$</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>Hence the eigenvalues of $A$ are the roots of $(\lambda + 3)(\lambda - 3) = 0$; that is $\lambda _{1} = -3$ and $\lambda _{2} = 3$.\</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>To find the eigenvectors corresponding to $\lambda _{1} = -3$, we find the nullspace of</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>$$A - (-3)I_{2} = \left<span class="co">[</span><span class="ot">\begin{array}{cc} 4 &amp; 2 \\ 4 &amp; 2 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>Row reduction produces</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>$$\left<span class="co">[</span><span class="ot">A+3I_{2} | \mathbf{0}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{cc|c} 4 &amp; 2 &amp; 0 \\ 4 &amp; 2 &amp; 0 \end{array}\right</span><span class="co">]</span> \xrightarrow{R_{2} - R_{1}} \left<span class="co">[</span><span class="ot">\begin{array}{cc|c} 4 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>Thus $\mathbf{x} \in {\rm Null}(A+3I_{2})$ if and only if</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>$4x_{1} + 2x_{2} = 0$.\</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>Setting the free variable $x_{2} = t$, we see that</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>$x_{1} = -\frac{1}{2}t$. We take</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>$\left<span class="co">[</span><span class="ot">\begin{array}{c} x_{1} \\ x_{2} \end{array}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{c} -1 \\ 2 \end{array}\right</span><span class="co">]</span>$</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>be our eigenvector; or indeed any nonzero multiple of</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>$\left<span class="co">[</span><span class="ot">\begin{array}{c} -1 \\ 2 \end{array}\right</span><span class="co">]</span>$.\</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>To find the eigenvectors corresponding to $\lambda _{2} = 3$, we find the nullspace of $A - 3I_{2}$ by row reduction:</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>$$\left<span class="co">[</span><span class="ot">A - 3I_{2} | \mathbf{0}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{cc|c} -2 &amp; 2 &amp; 0 \\ 4 &amp; -4 &amp; 0 \end{array}\right</span><span class="co">]</span> \xrightarrow{R_{2} + 2R_{1}} \left<span class="co">[</span><span class="ot">\begin{array}{cc|c} -2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>$\mathbf{x} = \left<span class="co">[</span><span class="ot">\begin{array}{c} x_{1} \\ x_{2} \end{array}\right</span><span class="co">]</span> \in {\rm Null}(A - 3I_{2})$</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>if and only if $-2x_{1} + 2x_{2} = 0$. Setting the free variable</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>$x_{2} = t$, we find $x_{1} = t$. We take</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>$\left<span class="co">[</span><span class="ot">\begin{array}{c} x_{1} \\ x_{2} \end{array}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 1 \end{array}\right</span><span class="co">]</span>$</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>to be our eigenvector.</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>::: {#exm-    } </span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>Find the eigenvalues and the corresponding eigenvectors of</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>$$A = \left[\begin{array}{ccc}</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>            3 &amp; 2 &amp; 2 <span class="sc">\\</span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>            1 &amp; 4 &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>            -2 &amp; -4 &amp; -1</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>        \end{array}\right]$$.</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>::: {.Solution}</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>The characteristic equation is</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>$$\begin{alignedat}{2}</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>        0 = |A - \lambda I| &amp;= \left|\begin{array}{ccc}</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>            3-\lambda &amp; 2 &amp; 2 <span class="sc">\\</span></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>            1 &amp; 4-\lambda &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>            -2 &amp; -4 &amp; -1-\lambda </span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>        \end{array}\right| <span class="sc">\\</span></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (3-\lambda ) \left|\begin{array}{cc} 4-\lambda &amp; 1 <span class="sc">\\</span> -4 &amp; -1-\lambda \end{array}\right| - 2 \left|\begin{array}{cc} 1 &amp; 1 <span class="sc">\\</span> -2 &amp; -1-\lambda \end{array}\right| + 2 \left|\begin{array}{cc} 1 &amp; 4-\lambda <span class="sc">\\</span> -2 &amp; -4\end{array}\right| <span class="sc">\\</span></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (3-\lambda ) <span class="sc">\{</span>(4-\lambda )(-1-\lambda )+4<span class="sc">\}</span> - 2<span class="sc">\{</span>(-1-\lambda )+2<span class="sc">\}</span> + 2<span class="sc">\{</span>-4+2(4-\lambda )<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>        {} &amp;= -\lambda ^3 + 6\lambda ^2 - 11\lambda + 6 <span class="sc">\\</span></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (\lambda -1)(-\lambda ^2 + 5\lambda - 6) <span class="sc">\\</span></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (\lambda -1)<span class="sc">\{</span>-(\lambda ^2 - 5\lambda + 6)<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (\lambda -1)<span class="sc">\{</span>-(\lambda - 2)(\lambda - 3)<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>    \end{alignedat}$$</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>Hence, the eigenvalues are $\lambda _{1} = 1, \lambda _{2} = 2$ and $\lambda _{3} = 3$.\</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>For $\lambda _{1}=1$, we compute</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>$$\left<span class="co">[</span><span class="ot">A - I | \mathbf{0}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{2}} &amp; 2 &amp; 2 &amp; 0 \\ 1 &amp; 3 &amp; 1 &amp; 0 \\ -2 &amp; -4 &amp; -2 &amp; 0 \end{array}\right</span><span class="co">]</span> \xrightarrow{\begin{subarray}{c} R_{2} - \frac{1}{2}R_{1} <span class="sc">\\</span> R_{3} + R_{1} \end{subarray}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} 2 &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{2}} &amp; 0 &amp; 0 \\ 0 &amp; -2 &amp; 0 &amp; 0 \end{array}\right</span><span class="co">]</span> $$</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>$$\xrightarrow{R_{3} + R_{2}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{2}} &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{2}} &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>from which it follows that an eigenvector</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>$$\mathbf{x} = \left<span class="co">[</span><span class="ot">\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>satisfies $x_{2} = 0$ and $x_{3} = -x_{1}$. We take</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>$\left<span class="co">[</span><span class="ot">\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 0 \\ -1 \end{array}\right</span><span class="co">]</span>$</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>to be our eigenvector .</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>For $\lambda _{2} = 2$, we compute</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>$$\left<span class="co">[</span><span class="ot">A - 2I | \mathbf{0}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{1}} &amp; 2 &amp; 2 &amp; 0 \\ 1 &amp; 2 &amp; 1 &amp; 0 \\ -2 &amp; -4 &amp; -3 &amp; 0 \end{array}\right</span><span class="co">]</span> \xrightarrow{\begin{subarray}{c} R_{2} - R_{1} <span class="sc">\\</span> R_{3} + 2R_{1} \end{subarray}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} 1 &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; \textcircled{\raisebox{-0.9pt}{-1}} &amp; 0 \\ 0 &amp; 0 &amp; 3 &amp; 0 \end{array}\right</span><span class="co">]</span>$$ </span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>$$\xrightarrow{R_{3} + 3R_{2}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{1}} &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; \textcircled{\raisebox{-0.9pt}{-1}} &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>which gives an eigenvector</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>$$\mathbf{x} = \left<span class="co">[</span><span class="ot">\begin{array}{c} -2 \\ 1 \\ 0 \end{array}\right</span><span class="co">]</span>$$.</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>For $\lambda _{3} = 3$, we compute</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>$$\left<span class="co">[</span><span class="ot">A - 3I | \mathbf{0}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} 0 &amp; 2 &amp; 2 &amp; 0 \\ 1 &amp; 1 &amp; 1 &amp; 0 \\ -2 &amp; -4 &amp; -4 &amp; 0 \end{array}\right</span><span class="co">]</span> \xrightarrow{R_{2} \leftrightarrow R_{1}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{1}} &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 2 &amp; 2 &amp; 0 \\ -2 &amp; -4 &amp; -4 &amp; 0 \end{array}\right</span><span class="co">]</span> $$ </span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>$$\xrightarrow{R_{3} + 2R_{1}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} 1 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{2}} &amp; 2 &amp; 0 \\ 0 &amp; -2 &amp; -2 &amp; 0 \end{array}\right</span><span class="co">]</span> \xrightarrow{R_{3} + R_{2}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} 1 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>which gives an eigenvector</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>$$\mathbf{x} = \left<span class="co">[</span><span class="ot">\begin{array}{c} 0 \\ -1 \\ 1 \end{array}\right</span><span class="co">]</span>$$.</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>::: {#exm-    }</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>Find the eigenvalues and the corresponding eigenspaces of</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>$$A = \left[\begin{array}{ccc}</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>        0 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>        0 &amp; 0 &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>        2 &amp; -5 &amp; 4</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>    \end{array}\right]$$</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>:::    </span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>::: {.Solution}</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>The characteristic equations is</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>$$\begin{alignedat}{2}</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>        0 = |A - \lambda I| &amp;= \left|\begin{array}{ccc}</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>            -\lambda &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>            0 &amp; -\lambda &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>            2 &amp; -5 &amp; 4-\lambda </span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>        \end{array}\right| <span class="sc">\\</span></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>        {} &amp;= -\lambda \left|\begin{array}{cc} -\lambda &amp; 1 <span class="sc">\\</span> -5 &amp; 4-\lambda \end{array}\right| -  \left|\begin{array}{cc} 0 &amp; 1 <span class="sc">\\</span> 2 &amp; 4-\lambda \end{array}\right| <span class="sc">\\</span></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>        {} &amp;= -\lambda (\lambda ^2-4\lambda +5)-(-2) <span class="sc">\\</span></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>        {} &amp;= -\lambda ^3 + 4\lambda ^2 - 5\lambda + 2 <span class="sc">\\</span></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (\lambda -1)(-\lambda ^2 + 3\lambda - 2) <span class="sc">\\</span></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>        {} &amp;= -(\lambda -1)^2(\lambda -2) <span class="sc">\\</span></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>    \end{alignedat}$$</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>Hence, the eigenvalues are $\lambda _{1} = \lambda _{2} = 1$ and $\lambda _{3} = 2$.\</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>To find the eigenvectors corresponding to $\lambda _{1} = \lambda _{2} = 1$, we compute</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>$$\left<span class="co">[</span><span class="ot">A - I | \mathbf{0}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{-1}} &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 1 &amp; 0 \\ 2 &amp; -5 &amp; 3 &amp; 0 \end{array}\right</span><span class="co">]</span> \xrightarrow{R_{3} + 2R_{1}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} -1 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{-1}} &amp; 1 &amp; 0 \\ 0 &amp; -3 &amp; 3 &amp; 0 \end{array}\right</span><span class="co">]</span> \xrightarrow{R_{3} - 3R_{2}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} -1 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>Thus,</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>$$\mathbf{x} = \left<span class="co">[</span><span class="ot">\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>is in the eigenspace $E_{1}$ if and only if $-x_{1} + x_{2} = 0$ and</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>$-x_{2} + x_{3} = 0$. Setting the free variable $x_{3} = t$, we see that</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>$x_{1} = t$ and $x_{2} = t$, from which it follows that</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>$$E_{1} = \left<span class="sc">\{</span>\left<span class="co">[</span><span class="ot">\begin{array}{c} t \\ t \\ t \end{array}\right</span><span class="co">]</span>\right<span class="sc">\}</span> = \left<span class="sc">\{</span>t \left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right</span><span class="co">]</span>\right<span class="sc">\}</span> = {\rm span}\left(\left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right</span><span class="co">]</span>\right)$$</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>To find the eigenvectors correspond to $\lambda _{3} = 2$, we find the nullspace of $A - 2I$ by row reduction:</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>$$\left<span class="co">[</span><span class="ot">A - 2I | \mathbf{0}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} \textcircled{\raisebox{-0.9pt}{-2}} &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -2 &amp; 1 &amp; 0 \\ 2 &amp; -5 &amp; 2 &amp; 0 \end{array}\right</span><span class="co">]</span> \xrightarrow{R_{3} + R_{1}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} -2 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{-2}} &amp; 1 &amp; 0 \\ 0 &amp; -4 &amp; 2 &amp; 0 \end{array}\right</span><span class="co">]</span> $$ </span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>$$\xrightarrow{R_{3} - 2R_{2}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc | c} -2 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -2 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>$$\mathbf{x} = \left<span class="co">[</span><span class="ot">\begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>is in the eigenspace $E_{2}$ if and only if $-2x_{1} + x_{2} = 0$ and</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>$-2x_{2} + x_{3} = 0$. Setting the free variable $x_{3} = t$, we have</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>$$E_{2} = \left<span class="sc">\{</span>\left<span class="co">[</span><span class="ot">\begin{array}{c} \frac{1}{4}t \\ \frac{1}{2}t \\ t \end{array}\right</span><span class="co">]</span>\right<span class="sc">\}</span> = \left<span class="sc">\{</span>t \left<span class="co">[</span><span class="ot">\begin{array}{c} \frac{1}{4} \\ \frac{1}{2} \\ 1 \end{array}\right</span><span class="co">]</span>\right<span class="sc">\}</span> = {\rm span}\left(\left<span class="co">[</span><span class="ot">\begin{array}{c} \frac{1}{4} \\ \frac{1}{2} \\ 1 \end{array}\right</span><span class="co">]</span>\right) = {\rm span}\left(\left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 2 \\ 4 \end{array}\right</span><span class="co">]</span>\right)$$</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>::: {.Definition}</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a><span class="fu">## Algebraic and Geometric Multiplicity</span></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>The algebraic multiplicity of an eigenvalue is</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>its multiplicity as a root of the characteristic equation.\</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>The geometric multiplicity of an eigenvalue $\lambda$ is ${\rm dim} (E_{\lambda })$, the dimension of its corresponding eigenspace.</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>In the above Example, $\lambda = 1$ has algebraic multiplicity $2$ and geometric multiplicity</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>$1$. $\lambda = 2$ has algebraic multiplicity $1$ and geometric multiplicity</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>$1$.</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>::: {.Corollary #eigenvaluestriangular}</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>The eigenvalues of a triangular matrix are the entries on its main diagonal.</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>::: {#exm-    }</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>Let</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>$$A = \left<span class="co">[</span><span class="ot">\begin{array}{cccc} 2 &amp; 0 &amp; 0 &amp; 0 \\ -1 &amp; 1 &amp; 0 &amp; 0 \\ 3 &amp; 0 &amp; 3 &amp; 0 \\ 5 &amp; 7 &amp; 4 &amp; -2 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>The characteristic polynomial is:</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>$$\begin{alignedat}{2}</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>        <span class="pp">|</span>A - \lambda I<span class="pp">|</span> &amp;= \left<span class="pp">|</span>\begin{array}{cccc} 2-\lambda &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span> -1 &amp; 1-\lambda &amp; 0 &amp; 0 <span class="sc">\\</span> 3 &amp; 0 &amp; 3-\lambda &amp; 0 <span class="sc">\\</span> 5 &amp; 7 &amp; 4 &amp; -2-\lambda \end{array}\right<span class="pp">|</span> <span class="sc">\\</span></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (2-\lambda )\left|\begin{array}{ccc} 1-\lambda &amp; 0 &amp; 0 <span class="sc">\\</span> 0 &amp; 3-\lambda &amp; 0 <span class="sc">\\</span> 7 &amp; 4 &amp; -2-\lambda \end{array}\right| <span class="sc">\\</span></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (2-\lambda )(1-\lambda )\left|\begin{array}{cc} 3-\lambda &amp; 0 <span class="sc">\\</span> 4 &amp; -2-\lambda \end{array}\right| <span class="sc">\\</span></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>        {} &amp;= (2-\lambda )(1-\lambda )(3-\lambda )(-2-\lambda ) </span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>    \end{alignedat}$$</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>Hence, the eigenvalues are $\lambda _{1}=2, \lambda _{2}=1, \lambda _{3}=3, \lambda _{4}=-2$.</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>Note that diagonal matrices are a special case of Corollary \ref{eigenvaluestriangular}.</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>::: {.Theorem #eigenlinind}</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>Let $A$ be an $n \times n$ matrix and let $\lambda _{1}, \lambda _{2}, ..., \lambda _{m}$ be distinct eigenvalues of $A$ with corresponding</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>eigenvectors $\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}$. Then</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>$\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}$ are linearly independent.</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>::: {.Proof}</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>We prove this by contradiction.</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}$ are</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>linearly dependent. Let $\mathbf{v_{k+1}}$ be the first of the vectors</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>$\mathbf{v_{i}}$ that can be expressed as a linear combination of the</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>previous ones. In other words,</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>$\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{k}}$ are linearly</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>independent, but there are $c_{1}, c_{2}, ..., c_{k}$ such that</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>$$\mathbf{v_{k+1}} = c_{1}\mathbf{v_{1}} + c_{2}\mathbf{v_{2}} + ... + c_{k}\mathbf{v_{k}} \qquad (1)$$</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>Multiplying both sides of Equation (1) by $A$ from left and using the</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>fact that $A\mathbf{v_{i}} = \lambda _{i}\mathbf{v_{i}}$ for each $i$, we have</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>$$\begin{alignedat}{2}</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>        \lambda _{k+1}\mathbf{v_{k+1}} = A\mathbf{v_{k+1}} &amp;= A(c_{1}\mathbf{v_{1}} + c_{2}\mathbf{v_{2}} + ... + c_{k}\mathbf{v_{k}}) <span class="sc">\\</span></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>        {} &amp;= c_{1}A\mathbf{v_{1}} + c_{2}A\mathbf{v_{2}} + ... + c_{k}A\mathbf{v_{k}} <span class="sc">\\</span></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>        {} &amp;= c_{1}\lambda _{1}\mathbf{v_{1}} + c_{2}\lambda _{2}\mathbf{v_{2}} + ... + c_{k}\lambda _{k}\mathbf{v_{k}} \qquad (2) </span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>    \end{alignedat}$$</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>Now we multiply both sides of Equation (1) by $\lambda _{k+1}$ to obtain</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>$$\lambda _{k+1}\mathbf{v_{k+1}} = c_{1}\lambda _{k+1}\mathbf{v_{1}} + c_{2}\lambda _{k+1}\mathbf{v_{2}} + ... + c_{k}\lambda _{k+1}\mathbf{v_{k}} \qquad (3)$$</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>When we subtract Equation (3) from Equation (2), we obtain</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>$$\mathbf{0} = c_{1}(\lambda _{1}-\lambda _{k+1})\mathbf{v_{1}} + c_{2}(\lambda _{2}-\lambda _{k+1})\mathbf{v_{2}} + ... + c_{k}(\lambda _{k}-\lambda _{k+1})\mathbf{v_{k}}$$</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>The linear independence of</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>$$\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{k}}$$ implies that</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>$$c_{1}(\lambda _{1}-\lambda _{k+1}) = c_{2}(\lambda _{2}-\lambda _{k+1}) = ... = c_{k}(\lambda _{k}-\lambda _{k+1}) = 0$$</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a>Since the eigenvalues $\lambda _{i}$ are all distinct, $\lambda _{i} - \lambda _{k+1} \neq 0$ for all $i = 1, ..., k$. Hence</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>$c_{1} = c_{2} = ... = c_{k} = 0$. This implies that</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a>$$\mathbf{v_{k+1}} = 0\mathbf{v_{1}} + 0\mathbf{v_{2}} + ... + 0\mathbf{v_{k}} = \mathbf{0}$$</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>which is impossible since the eigenvector $\mathbf{v_{k+1}}$ cannot be</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>zero.</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>Thus, our assumption that</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>$\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}$ are linearly dependent is false. It follows that $\mathbf{v_{1}}, \mathbf{v_{2}}, ..., \mathbf{v_{m}}$ must be linearly</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a>independent.&nbsp;â—»</span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>::: {#exm- }</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a><span class="fu">## Coupled oscillators, normal modes</span></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>Consider a system of two coupled oscillators connected by springs as shown in the figure. </span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a><span class="al">![Coupled Oscillators](coupledosc.png)</span>{width=70%}</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>We can write down a system of equations for the dynamics of these two oscillators:</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a>$$ m \ddot{x_1} = - k x_1 + k (x_2-x_1) $$</span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>$$ m \ddot{x_x} = - k (x_2 -x_1) - k x_2 $$</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a>We can write this as a vector equation for the vector $$\vec{x} =\begin{pmatrix} x_1 <span class="sc">\\</span> x_2 \end{pmatrix} $$</span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a>$$ m \ddot{\vec{x}} = \begin{pmatrix} -2 k &amp; k <span class="sc">\\</span> k &amp; -2k  \end{pmatrix} \vec{x}$$</span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a>If we are looking for the **normal modes** of the system, we are looking for harmonic oscillations with a single frequency. So we assume</span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a>$$\vec{x} = \begin{pmatrix} x_{10} <span class="sc">\\</span> x_{20} \end{pmatrix} e^{i \omega t} \quad \ddot{\vec{x}} = - \omega^2 \begin{pmatrix} x_{10} <span class="sc">\\</span> x_{20} \end{pmatrix} e^{i \omega t}$$</span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a>Substituting in the vector equation above leads to an eigenvalue problem </span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>$$ \Rightarrow - m \omega^2 \vec{x_0} = \begin{pmatrix} -2 k &amp; k <span class="sc">\\</span> k &amp; -2k  \end{pmatrix} \vec{x_0}$$</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a>$$ \Rightarrow  \begin{pmatrix} 2 k/m &amp; - k/m <span class="sc">\\</span> - k/m &amp; 2k/m  \end{pmatrix} \vec{x_0} = \omega^2 \vec{x_0}  $$</span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>for the frequency $\omega$.</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>The eigenvalues and eigenvectors of the matrix are  </span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>$$ \lambda_1 = 3 k/m \quad \vec{x_0} = \begin{pmatrix} -1<span class="sc">\\</span> 1 \end{pmatrix}, $$</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a>$$ \lambda_1 = k/m \quad \vec{x_0} = \begin{pmatrix} 1<span class="sc">\\</span> 1 \end{pmatrix}, $$</span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>corresponding to an oscillation with frequency $\omega = \sqrt{3k/m}$ where the two oscillators are 180 degree out of phase, and an oscillation with frequency $\omega = \sqrt{k/m}$ where the oscillators are in phase. </span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>Other solutions for the system can now be obtained as a linear combination of these two modes. The modes are a basis of the solution space. </span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a><span class="fu">## Similarity and Diagonalisation</span></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a><span class="fu">### Introduction</span></span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>In many applications, matrices represent linear mappings of vectors in a</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a>physical space, as in the example given at the start of the Eigenvector</span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>section. The choice of a coordinate system (in particular, its</span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a>orientation) in this space is arbitrary, and this choice determines what</span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>the matrix looks like. In this section, we show that under certain</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a>conditions, there exists a choice of a coordinate system in which the</span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>matrix becomes a diagonal matrix. In this case, the coordinate axes have</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>the direction of the eigenvectors of the matrix, and the diagonal</span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a>elements of the matrix are the eigenvalues.</span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a><span class="fu">### Similar Matrices</span></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a>::: {.Definition}</span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a><span class="fu">## Similar Matrices </span></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a>Let $A$ and $B$ be $n \times n$ matrices. We say</span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>that $A$ is similar to $B$ if there is an invertible $n \times n$ matrix</span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a>$P$ such that $P^{-1}AP = B$. If $A$ is similar to $B$, we write</span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a>$A \sim B$.</span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a><span class="fu">## Remark</span></span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a>If $A \sim B$, we can write, equivalently, that $A = PBP^{-1}$ or $AP = PB$. The matrix $P$ depends on $A$ and $B$. It is not unique for a given pair of similar matrices $A$ and $B$.</span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>::: {.Theorem #simmatrices} </span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of Similar Matrices</span></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>Let $A$ and $B$ be $n \times n$ matrices with</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a>$A \sim B$. Then\</span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>(a) ${\rm det}(A) = {\rm det}(B)$\</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a>(b) $A$ and $B$ have the same rank.\</span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>(c) $A$ and $B$ have the same characteristic polynomial.\</span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a>(d) $A$ and $B$ have the same eigenvalues.</span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>::: {.Proof}</span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a>If $A \sim B$, then ${\rm P^{-1}AP = B}$ for some invertible matrix $P$.</span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>(a)\</span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a>$$\begin{alignedat}{2}</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>            \qquad {\rm det}(B) &amp;= {\rm det}(P^{-1}AP) <span class="sc">\\</span></span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a>            {} &amp;= {\rm det(P^{-1}){\rm det}(A){\rm det}(P)} <span class="sc">\\</span></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a>            {} &amp;= \frac{1}{{\rm det}(P)}{\rm det(A){\rm det}(P)} <span class="sc">\\</span></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>            {} &amp;= {\rm det}(A). <span class="sc">\\</span></span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a>        \end{alignedat}$$</span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>(b)\</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a>We first show that nullity(A)=nullity(B). One can then use the rank theorem \ref{ranktheorem} to show that this also implies that rank(A)=rank(B). </span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a>Let $<span class="sc">\{</span>\vec{x_1},..., \vec{x_k}<span class="sc">\}</span>$ be a basis of the nullspace of A. Hence $\vec{y}_i= P^{-1}\vec{x_i}$, $i=1..k$, is a linearly independent set (P is one-to-one) in the nullspace of B, as</span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a>$$ B \vec{y}_i = P^{-1}A P P^{-1}\vec{x_i} = P^{-1}A \vec{x_i} = P^{-1}\vec{0} = \vec{0}. $$</span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a>To show that the $\vec{y}_i$ indeed are a basis of the nullspace of B, we observe that for any $\vec{y}$ in the nullspace of B, $P\vec{y} = \vec{x}$ is in the nullspace of A. So $\vec{y}_1, ... \vec{y}_k$ are a basis of the nullspace of B and nullity(A)=nullity(B).</span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>c<span class="sc">\)</span> The characteristic polynomial of $B$ is</span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>$$\begin{alignedat}{2}</span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a>            {\rm det}(B - \lambda I) &amp;= {\rm det}(P^{-1}AP - \lambda I) <span class="sc">\\</span></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a>            {} &amp;= {\rm det}(P^{-1}AP - \lambda P^{-1}IP) <span class="sc">\\</span></span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a>            {} &amp;= {\rm det}(P^{-1}AP - P^{-1}(\lambda I)P) <span class="sc">\\</span></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a>            {} &amp;= {\rm det}(P^{-1}(A - \lambda I)P) <span class="sc">\\</span></span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a>            {} &amp;= {\rm det}(P^{-1}){\rm det}(A - \lambda I){\rm det}(P) <span class="sc">\\</span></span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a>            {} &amp;= \frac{1}{{\rm det}(P)}{\rm det}(A - \lambda I){\rm det}(P) <span class="sc">\\</span></span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a>            {} &amp;= {\rm det}(A - \lambda I)  </span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>        \end{alignedat}$$</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a>&nbsp;â—»</span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>Theorem \ref{simmatrices} is helpful in showing that two matrices are not similar,</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a>since $A$ and $B$ cannot be similar if any of the properties fail.</span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a>::: {#exm-    } </span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a>(a) The two matrices</span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a>$$A = \left<span class="co">[</span><span class="ot">\begin{array}{cc} 1 &amp; 2 \\ 2 &amp; 1 \end{array}\right</span><span class="co">]</span>, \quad B = \left<span class="co">[</span><span class="ot">\begin{array}{cc} 2 &amp; 1 \\ 1 &amp; 2 \end{array}\right</span><span class="co">]</span>, $$</span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a>are not similar since ${\rm det}(A) = -3$ but ${\rm det}(B) = 3$.\</span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a>(b) The two matrices</span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>$$A = \left<span class="co">[</span><span class="ot">\begin{array}{cc} 1 &amp; 3 \\ 2 &amp; 2 \end{array}\right</span><span class="co">]</span>, \quad B = \left<span class="co">[</span><span class="ot">\begin{array}{cc} 1 &amp; 1 \\ 3 &amp; -1 \end{array}\right</span><span class="co">]</span>$$ are not</span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a>similar, since $|A - \lambda I| = \lambda ^{2} - 3\lambda - 4$ while $|B - \lambda I| = \lambda ^{2} - 4$. Note that $A$ and $B$ have the same determinant and</span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a>rank, however.</span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a><span class="fu">### Diagonalisation</span></span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a>::: {.Definition}</span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a><span class="fu">## Diagonalisable Matrices</span></span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a>An $n \times n$ matrix $A$ is diagonalisable if there is a diagonal matrix $D$ such that $A$ is similar to $D$ - that</span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a>is, if there is an invertible $n \times n$ matrix $P$ such that</span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a>$P^{-1}AP = D$.</span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a>::: {.Theorem #diagifind}</span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a><span class="fu">## Condition for Diagonalisability</span></span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a>Let $A$ be an $n \times n$ matrix. Then $A$ is</span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a>diagonalisable if and only if $A$ has $n$ linearly independent</span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a>eigenvectors.\</span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a>More precisely, there exists an invertible matrix $P$ and a diagonal</span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a>matrix $D$ such that $P^{-1}AP = D$ if and only if the columns of $P$</span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a>are $n$ linearly independent eigenvectors of $A$ and the diagonal</span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a>entries of $D$ are the eigenvalues of $A$ corresponding to the</span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a>eigenvectors in $P$ in the same order.*</span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a>:::{.Proof}</span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a>Suppose first that $A$ is similar to the diagonal matrix $D$ by</span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a>$P^{-1}AP = D$ or, equivalently, $AP = PD$. Let the columns of $P$ be</span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a>$\mathbf{p_{1}}, \mathbf{p_{2}}, ..., \mathbf{p_{n}}$ and let the</span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a>diagonal entries of $D$ be $\lambda _{1}, \lambda _{2}, ..., \lambda _{n}$. Then</span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a>$$A\left<span class="co">[</span><span class="ot">\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots, \mathbf{p_{n}}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots, \mathbf{p_{n}}\right</span><span class="co">]</span> \left<span class="co">[</span><span class="ot">\begin{array}{cccc} \lambda _{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda _{2} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda _{n} \end{array}\right</span><span class="co">]</span> \qquad (1)$$</span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a>or</span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a>$$\qquad \left<span class="co">[</span><span class="ot">A\mathbf{p_{1}}, A\mathbf{p_{2}}, \cdots, A\mathbf{p_{n}}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\lambda _{1}\mathbf{p_{1}}, \lambda _{2}\mathbf{p_{2}}, \cdots, \lambda _{n}\mathbf{p_{n}}\right</span><span class="co">]</span> \qquad (2)$$</span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a>Equating columns, we have</span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a>$$A\mathbf{p_{1}} = \lambda _{1}\mathbf{p_{1}}, A\mathbf{p_{2}} = \lambda _{2}\mathbf{p_{2}}, \cdots , A\mathbf{p_{n}} = \lambda _{n}\mathbf{p_{n}}$$</span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a>which proves that the column vectors of $P$ are eigenvectors of $A$</span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a>whose corresponding eigenvalues are the diagonal entries of $D$ in the</span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a>same order. Since $P$ is invertible, its columns are linearly</span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a>independent.</span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a>Conversely, if $A$ has $n$ linearly independent eigenvectors</span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a>$\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots , \mathbf{p_{n}}$ with</span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a>corresponding eigenvalues $\lambda _{1}, \lambda _{2}, \cdots , \lambda _{n}$, respectively, then</span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a>$$A\mathbf{p_{1}} = \lambda _{1}\mathbf{p_{1}}, A\mathbf{p_{2}} = \lambda _{2}\mathbf{p_{2}}, \cdots , A\mathbf{p_{n}} = \lambda _{n}\mathbf{p_{n}}$$</span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a>This implies Eq. (2), which is equivalent to Eq. (1), that is $AP = PD$.</span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a>Since the columns</span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a>$\mathbf{p_{1}}, \mathbf{p_{2}}, \cdots , \mathbf{p_{n}}$ of $P$ are</span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a>linearly independent, $P$ is invertible, so $P^{-1}AP = D$, that is, $A$</span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a>is diagonalisable.&nbsp;â—»</span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a>::: {#exm-    }</span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a>If possible, find a matrix $P$ that diagonalises</span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a>$$A = \left[\begin{array}{ccc}</span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a>            0 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a>            0 &amp; 0 &amp; 1 <span class="sc">\\</span></span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a>            2 &amp; -5 &amp; 4 </span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a>        \end{array}\right]$$</span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a>:::        </span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a>::: {.Solution}</span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a>We studied this matrix previously and found that it has eigenvalues $\lambda _{1} = \lambda _{2} = 1$ and $\lambda _{3} = 2$. The eigenspaces have the following bases:\</span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a>For $\lambda _{1} = \lambda _{2} = 1$, $E_{1}$ has basis</span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a>$\left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right</span><span class="co">]</span>$.\</span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a>For $\lambda _{3} = 2$, $E_{2}$ has basis</span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a>$\left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 2 \\ 4 \end{array}\right</span><span class="co">]</span>$.\</span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a>Since all other eigenvectors are just multiples of one of these two</span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a>basis vectors, there cannot be three linearly independent eigenvectors.</span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a>By Theorem 4.6, $A$ is not diagonalisable.</span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a>::: {#exm-    }</span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a>If possible, find a matrix $P$ that diagonalises</span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a>$$A = \left[\begin{array}{ccc}</span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a>            2 &amp; 2 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a>            0 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a>            -4 &amp; -8 &amp; 1 </span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a>        \end{array}\right]$$</span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a>:::        </span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a>::: {.Solution}</span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a>This is the matrix of Question 3, Worksheet 6. There we found that the</span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a>eigenvalues of $A$ are $\lambda _{1} = 2$ and $\lambda _{2} = \lambda _{3} = 1$, with the following bases for the eigenspaces:\</span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a>For $\lambda _{1} = 2$, $E_{2}$ has basis</span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a>$\mathbf{p_{1}} = \left<span class="co">[</span><span class="ot">\begin{array}{c} 1 \\ 0 \\ -4 \end{array}\right</span><span class="co">]</span>$.\</span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a>For $\lambda _{2} = \lambda _{3} = 1$, $E_{1}$ has basis</span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a>$\mathbf{p_{2}} = \left<span class="co">[</span><span class="ot">\begin{array}{c} -2 \\ 1 \\ 0 \end{array}\right</span><span class="co">]</span>$</span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a>$\mathbf{p_{3}} = \left<span class="co">[</span><span class="ot">\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right</span><span class="co">]</span>$.\</span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a>Now we check whether</span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a>$\left<span class="sc">\{</span>\mathbf{p_{1}}, \mathbf{p_{2}}, \mathbf{p_{3}}\right<span class="sc">\}</span>$ is</span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a>linearly independent.</span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a>$$\left<span class="co">[</span><span class="ot">\begin{array}{ccc} \textcircled{\raisebox{-0.9pt}{1}} &amp; -2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -4 &amp; 0 &amp; 1 \end{array}\right</span><span class="co">]</span> \xrightarrow{R_{3} + 4R_{1}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc} 1 &amp; -2 &amp; 0 \\ 0 &amp; \textcircled{\raisebox{-0.9pt}{1}} &amp; 0 \\ 0 &amp; -8 &amp; 1 \end{array}\right</span><span class="co">]</span> \xrightarrow{R_{3} + 8R_{2}} \left<span class="co">[</span><span class="ot">\begin{array}{ccc} 1 &amp; -2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a>Since $rank = 3$,</span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a>$\left<span class="sc">\{</span>\mathbf{p_{1}}, \mathbf{p_{2}}, \mathbf{p_{3}}\right<span class="sc">\}</span>$ is</span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a>linearly independent. Thus, if we take</span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a>$$P = \left<span class="co">[</span><span class="ot">\mathbf{p_{1}}, \mathbf{p_{2}}, \mathbf{p_{3}}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{ccc} 1 &amp; -2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -4 &amp; 0 &amp; 1 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a>then $P$ is invertible. Furthermore,</span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a>$$P^{-1}AP = \left<span class="co">[</span><span class="ot">\begin{array}{ccc} 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right</span><span class="co">]</span> = D$$</span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a>(Note: It is much easier to check the equivalent equation $AP = PD$).</span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a><span class="fu">## Remark</span></span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a>Eigenvectors can be placed into the columns of $P$ in any</span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a>order. However, the eigenvalues will come up on the diagonal of $D$ in</span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a>the same order as their corresponding eigenvectors in $P$. For example,</span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a>if we had chosen</span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a>$$P = \left<span class="co">[</span><span class="ot">\mathbf{p_{2}}, \mathbf{p_{3}}, \mathbf{p_{1}}\right</span><span class="co">]</span> = \left<span class="co">[</span><span class="ot">\begin{array}{ccc} -2 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; -4 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a>Then we would have found</span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a>$$P^{-1}AP = \left<span class="co">[</span><span class="ot">\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{array}\right</span><span class="co">]</span>$$</span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a>We checked that the eigenvectors $\mathbf{p_{1}}, \mathbf{p_{2}}$ and</span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a>$\mathbf{p_{3}}$ were linearly independent. However, the following</span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a>Theorem guarantees that linear independence is preserved when the bases</span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a>of different subspaces are combined.</span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a>::: {.Theorem}</span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a>##</span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a>Let $A$ be an $n \times n$ matrix and let $\lambda _{1}, \lambda _{2}, \cdots, \lambda _{k}$ be distinct eigenvalues of $A$. If $B_{i}$ is a basis for</span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a>the eigenspace $E_{i}$, then</span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a>$B = B_{1} \cup B_{2} \cup \cdots \cup B_{k}$ (i.e. the total collection</span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a>of basis vectors for all of the eigenspaces) is linearly independent.</span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a>::: {.Theorem #eigendiag}</span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a>If $A$ is an $n \times n$ matrix with $n$ distinct</span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a>eigenvalues, then $A$ is diagonalisable.</span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a>::: {.Proof}</span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a>Let $\mathbf{v_{1}}, \mathbf{v_{2}}, \cdots, \mathbf{v_{n}}$ be</span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a>eigenvectors corresponding to the $n$ distinct eigenvalues of $A$. By</span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a>theorem \ref{eigenlinind}, $\mathbf{v_{1}}, \mathbf{v_{2}}, \cdots, \mathbf{v_{n}}$</span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a>are linearly independent, so, by Theorem \ref{diagifind}, $A$ is diagonalisable.&nbsp;â—»</span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a>::: {#exm-    }</span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a>The matrix</span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a>$$A = \left[\begin{array}{cccc}</span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a>            2 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a>            -1 &amp; 1 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a>            3 &amp; 0 &amp; 3 &amp; 0 <span class="sc">\\</span></span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a>            5 &amp; 7 &amp; 4 &amp; -2</span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a>        \end{array}\right]$$</span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a>has eigenvalues $\lambda _{1} = 2, \lambda _{2} = 1, \lambda _{3} = 3$ and $\lambda _{4} = -2$, by Corollary \ref{eigenvaluestriangular}. Since these are four distinct</span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a>eigenvalues for a $4 \times 4$ matrix, $A$ is diagonalisable, by Theorem \ref{eigendiag}.</span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a>::: {.Corollary}</span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a>If $A$ is an $n \times n$ matrix, then the geometric</span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a>multiplicity of each eigenvalue is less than or equal to its algebraic</span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a>multiplicity.</span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a>::: {.Theorem}</span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a><span class="fu">## Diagonalisation Theorem</span></span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a>Let $A$ be an $n \times n$ matrix whose distinct eigenvalues are $\lambda _{1}, \lambda _{2}, \cdots, \lambda _{k}$, where $1\leq k\leq n$. The following statements are</span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a>equivalent:</span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a>(a) $A$ is diagonalisable.</span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a>(b) The union $B$ of the bases of the eigenspaces of $A$ contains $n$</span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a>vectors.</span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a>(c) The algebraic multiplicity of each eigenvalue equals its geometric</span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a>multiplicity.</span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a>::: {#exm-    }</span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a>(a) The matrix </span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a>$$A = \left[\begin{array}{ccc}</span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a>        0 &amp; 1 &amp; 0 <span class="sc">\\</span> 0 &amp; 0 &amp; 1 <span class="sc">\\</span> 2 &amp; -5 &amp; 4 \end{array}\right]$$</span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a>has two distinct eigenvalues $\lambda _{1} = \lambda _{2} = 1$ and $\lambda _{3} = 2$. Since the eigenvalue $\lambda _{1} = \lambda _{2} = 1$ with</span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a>$E_{1} = {\rm span}( (1,1,1)^T)$ has algebraic multiplicity 2 but geometric multiplicity 1, $A$ is</span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a>not diagonalisable, by the Diagonalisation Theorem.\</span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a>(b) The matrix </span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a>$$A = \left[\begin{array}{ccc}</span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a>        2 &amp; 2 &amp; 0 <span class="sc">\\</span> 0 &amp; 1 &amp; 0 <span class="sc">\\</span> -4 &amp; -8 &amp; 1 \end{array}\right]$$</span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a>has two distinct eigenvalues $\lambda _{1} = 2$ and $\lambda _{2} = \lambda _{3} = 1$. We found:</span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a>for $\lambda _{1} = 2$, $E_{1}$ has basis</span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a>$\mathbf{p_{1}} = (1, 0, -4)^T$, and for $\lambda _{2} = \lambda _{3} = 1$, $E_{2}$ has basis</span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a>$\mathbf{p_{2}} = (-2,1, 0)^T$ and $\mathbf{p_{3}} = (0,0,1)^T$.</span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a>Thus, the eigenvalue 2 has algebraic and geometric multiplicity 1,</span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a>and the eigenvalue 1 has algebraic and geometric multiplicity 2.</span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a>Thus, $A$ is diagonalisable, by the Diagonalisation Theorem.</span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a>::: {.Theorem}</span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a><span class="fu">## Joint eigenbasis</span></span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a>If two matrices commute and one of them is diagonalisable then they share a basis of eigenvectors. </span>
<span id="cb1-660"><a href="#cb1-660" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-661"><a href="#cb1-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-662"><a href="#cb1-662" aria-hidden="true" tabindex="-1"></a>::: {.Proof}</span>
<span id="cb1-663"><a href="#cb1-663" aria-hidden="true" tabindex="-1"></a>Let ${\rm A}$ be diagonalisable. Then ${\rm A}$ has an eigenbasis $\vec{x}_i, i=1..n$. For simplicity we first assume that there are $n$ distinct eigenvalues, but one also prove the theorem for repeated eigenvalues.  </span>
<span id="cb1-664"><a href="#cb1-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-665"><a href="#cb1-665" aria-hidden="true" tabindex="-1"></a>Let ${\rm B}$ be a matrix that commutes with ${\rm A}$:   </span>
<span id="cb1-666"><a href="#cb1-666" aria-hidden="true" tabindex="-1"></a>$$ {\rm A B = B A} \quad \Rightarrow {\rm A B}\vec{x}_i ={\rm B A}\vec{x}_i \quad \Rightarrow {\rm A}({\rm  B}\vec{x}_i) = \lambda_i ({\rm B} \vec{x}_i)  $$ </span>
<span id="cb1-667"><a href="#cb1-667" aria-hidden="true" tabindex="-1"></a>That means that ${\rm B} \vec{x}_i$ is also an eigenvector of ${\rm A}$ to the eigenvalue $\lambda_i$ and since the eigenspace to $\lambda_i$ is 1-dimensional and spanned by $\vec{x}_i$ we have ${\rm B} \vec{x}_i = \mu_i \vec{x}_i$, for some real number $\mu_i$. This means that $\vec{x}_i$ is also an eigenvector of ${\rm B}$.  A similar argumentation works for repeated eigenvalues. </span>
<span id="cb1-668"><a href="#cb1-668" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-669"><a href="#cb1-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-670"><a href="#cb1-670" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-671"><a href="#cb1-671" aria-hidden="true" tabindex="-1"></a><span class="fu">## Remark</span></span>
<span id="cb1-672"><a href="#cb1-672" aria-hidden="true" tabindex="-1"></a>Joint eigenbases play an important role in quantum mechanics, where the eigenstates of commuting operators can be expressed w.r.t.~a common basis. It also means that measurements of these operator can be carried out simultaneously with (theoretically) infinite precision, while non-commuting operators have to satisfy a Heisenberg Uncertainty Principle. </span>
<span id="cb1-673"><a href="#cb1-673" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-674"><a href="#cb1-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-675"><a href="#cb1-675" aria-hidden="true" tabindex="-1"></a><span class="fu">## Adjoint Operator</span></span>
<span id="cb1-676"><a href="#cb1-676" aria-hidden="true" tabindex="-1"></a>If our vector space is equipped with an inner product $\langle . , . \rangle$, and we are given a linear operator (matrix) ${\rm A}$ then there exists a further linear operator closely related to ${\rm A}$, the Hermitian adjoint. </span>
<span id="cb1-677"><a href="#cb1-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-678"><a href="#cb1-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-679"><a href="#cb1-679" aria-hidden="true" tabindex="-1"></a>::: {.Definition}</span>
<span id="cb1-680"><a href="#cb1-680" aria-hidden="true" tabindex="-1"></a><span class="fu">## (Hermitian) Adjoint Operator</span></span>
<span id="cb1-681"><a href="#cb1-681" aria-hidden="true" tabindex="-1"></a>Given a linear operator ${\rm A}$ the hermitian adjoint of ${\rm A}$ is defined by </span>
<span id="cb1-682"><a href="#cb1-682" aria-hidden="true" tabindex="-1"></a>$$\langle {\rm A}\vec{x},\vec{y} \rangle = \langle \vec{x},{\rm A}^\dag \vec{y} \rangle. $$</span>
<span id="cb1-683"><a href="#cb1-683" aria-hidden="true" tabindex="-1"></a>The operator ${\rm A}$ is called self-adjoint if ${\rm A} = {\rm A}^\dag$.</span>
<span id="cb1-684"><a href="#cb1-684" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-685"><a href="#cb1-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-686"><a href="#cb1-686" aria-hidden="true" tabindex="-1"></a>For the simplest case of a real, finite-dimensional vector space with an inner product given by the standard scalar product the hermitian adjoint of ${\rm A}$ is just the transpose of ${\rm A}$. This is because the scalar product $\vec{x}\cdot\vec{y}$ can be written also as a matrix product $\vec{x}^T \vec{y}$ and hence </span>
<span id="cb1-687"><a href="#cb1-687" aria-hidden="true" tabindex="-1"></a>$$\langle {\rm A}\vec{x},\vec{y} \rangle = ({\rm A}\vec{x})^T\vec{y} = \vec{x}^T{\rm A}^T\vec{y} = \langle \vec{x},{\rm A}^T\vec{y} \rangle. $$</span>
<span id="cb1-688"><a href="#cb1-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-689"><a href="#cb1-689" aria-hidden="true" tabindex="-1"></a>::: {.Corollary}</span>
<span id="cb1-690"><a href="#cb1-690" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of the hermitian adjoint</span></span>
<span id="cb1-691"><a href="#cb1-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-692"><a href="#cb1-692" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$({\rm A}^\dag)^\dag = {\rm A}$</span>
<span id="cb1-693"><a href="#cb1-693" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\rm A}^\dag + {\rm B}^\dag = ({\rm A} +{\rm B})^\dag$</span>
<span id="cb1-694"><a href="#cb1-694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$({\rm A}{\rm B})^\dag = {\rm B}^\dag {\rm A}^\dag$</span>
<span id="cb1-695"><a href="#cb1-695" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-696"><a href="#cb1-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-697"><a href="#cb1-697" aria-hidden="true" tabindex="-1"></a>::: {.Corollary}</span>
<span id="cb1-698"><a href="#cb1-698" aria-hidden="true" tabindex="-1"></a><span class="fu">## orthogonal eigenbasis</span></span>
<span id="cb1-699"><a href="#cb1-699" aria-hidden="true" tabindex="-1"></a>For a self-adjoint operator eigenvectors to different eigenvalues are orthogonal. </span>
<span id="cb1-700"><a href="#cb1-700" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-701"><a href="#cb1-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-702"><a href="#cb1-702" aria-hidden="true" tabindex="-1"></a>::: {.Proof}</span>
<span id="cb1-703"><a href="#cb1-703" aria-hidden="true" tabindex="-1"></a>$$ \langle {\rm A}\vec{x},\vec{y} \rangle = \langle \vec{x},{\rm A}^\dag \vec{y} \rangle  = \langle \vec{x},{\rm A} \vec{y} \rangle$$</span>
<span id="cb1-704"><a href="#cb1-704" aria-hidden="true" tabindex="-1"></a>$$ \Rightarrow \langle \lambda_x \vec{x},\vec{y} \rangle = \langle \vec{x},\lambda_y \vec{y} \rangle  \quad \Rightarrow \underbrace{(\lambda_x -\lambda_y)}_{\neq 0} \langle \vec{x},\vec{y} \rangle = 0  \quad \Rightarrow \langle \vec{x},\vec{y} \rangle = 0 $$</span>
<span id="cb1-705"><a href="#cb1-705" aria-hidden="true" tabindex="-1"></a>:::</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Gunnar Hornig (University of Dundee)</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/dundeemath/MA21001/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>