# Linear equations

## Matrices

::: {.Definition}
## Matrix
An $m\times n$ (read "m by n") matrix is an
array of numbers or expressions with $m$ rows and $n$ columns. The set
of all $m\times n$ matrices is denoted by $M_{m\times n}$.
:::

::: {#exm-}
A $2\times3$ matrix: 
$${\rm A} =\left(
\begin{array}{rrr}
 1 &  2 & 5  \\
  3 & 4  &   -7
  \end{array}
\right) \in M_{2\times3} ,  \quad A_{1,2} =2 .$$
:::

::: {.callout-note} 
## Notation
Matrices are usually named by upper case roman letters, e.g. ${\rm A}$.
Individual entries are referred to by $A_{i,j}$, i.e. the entry in row
$i$ and column $j$ of the array. Entries are also sometimes denoted by
the corresponding lower-case letter, e.g. $a_{i,j}$. A matrix can be
symbolically represented by its general entry:
${\rm A}=\left[ a_{i,j}\right]$. 
:::

Analogous to the rules of addition and scalar multiplication of vectors,
we have the following rules for matrices:

::: {.Definition}
## Matrix sum and scalar multiplication
Addition of two matrices of the same type (${\rm A} ,{\rm B} \in M_{m \times n}$) is defined by 
$${\rm A} + {\rm B}=[a_{i,j}] + [b_{i,j}]= [ a_{i,j}+b_{i,j}], $$
and the scalar multiplication by
$$c {\rm A} = c [a_{i,j}] = [ c a_{i,j}] .$$
:::

Example: 
$$\left(
\begin{array}{rrr}
 1 &  2 & 5  \\
  3 & 4  &   -7
  \end{array}
\right) + \left(
\begin{array}{rrr}
 2 &  1 & 0  \\
  -1 & 2  &   3
  \end{array}
\right) = \left(
\begin{array}{rrr}
 3 &  3 & 5  \\
  2 & 6  &   -4
  \end{array}
\right)$$ $$3 \left(
\begin{array}{rrr}
 1 &  2 & 5  \\
  3 & 4  &   -7
  \end{array}  \right)  =  \left(
\begin{array}{rrr}
 3 &  6 & 15  \\
  9 & 12  &   -21
  \end{array}
\right)$$ The neutral element of the addition is the *zero matrix*,
${\rm O}$, where all entries are zero. The inverse element of the
addition is the negative matrix $-{\rm A} = [-a_{i,j}]$. The two
operations lead to the following properties: 

::: {.Corollary}
## Basic Matrix Operations
$$\begin{aligned}
 {\rm A} + {\rm B} & =  {\rm B} + {\rm A} \quad \text{(addition is ccommutative)} \\
  ({\rm A} + {\rm B}) + {\rm C} &= {\rm A} + ({\rm B}  + {\rm C}) \quad \text{(addition is associative)} \\
  {\rm A} + {\rm O} & =  {\rm A} \quad \text{(Existence of a neutral element for add.)} \\
  {\rm A} + {\rm -A} & =  {\rm O} \quad \text{(Existence of an inverse element for add.)} \\
  a ({\rm A} + {\rm B})  &= a {\rm A} + a {\rm B} \quad \text{(Distributive law 1)} \\
  (a+b)  {\rm A}  &= a {\rm A} + b {\rm A}   \quad \text{(Distributive law 2)} \\
  a(b {\rm A} )&= (ab) {\rm A}  \quad \text{(scalar mult. is associative)} 
\end{aligned}$$
:::

In addition to the rules above, we have a matrix multiplication:

::: {.Definition}
## Matrix product
 The matrix product of the
$m \! \times \! r$ matrix $G$ and the $r \! \times \! n$ matrix $H$ is
the $m \! \times \! n$ matrix $P$, where 
$$p_{i,j}  = g_{i,1}h_{1,j}+g_{i,2}h_{2,j}+\dots+g_{i,r}h_{r,j}$$ 
that is, the
$i,j$-th entry of the product is the dot product of the $i$-th row and
the $j$-th column. 
$$GH=
    \begin{pmatrix}
              &\vdots                     \\
      g_{i,1} &g_{i,2} &\ldots  &g_{i,r}  \\
              &\vdots
    \end{pmatrix}
    \begin{pmatrix}
              &h_{1,j}           \\
      \ldots  &h_{2,j} &\ldots   \\
              &\vdots            \\
              &h_{r,j}
    \end{pmatrix}
  =
    \begin{pmatrix}
              &\vdots            \\
      \ldots  &p_{i,j}  &\ldots  \\
              &\vdots
    \end{pmatrix}$$
:::
::: {#exm-}
$$\begin{pmatrix}
     2  &0  \\
     4  &6  \\
     8  &2
  \end{pmatrix}
  \begin{pmatrix}
    1  &3  \\
    5  &7
  \end{pmatrix}
  =
  \begin{pmatrix}
   2\cdot 1+0\cdot 5  &2\cdot 3+0\cdot 7  \\
   4\cdot 1+6\cdot 5  &4\cdot 3+6\cdot 7  \\
   8\cdot 1+2\cdot 5  &8\cdot 3+2\cdot 7
  \end{pmatrix}
  =
  \begin{pmatrix}
    2  &6  \\
   34  &54 \\
   18  &38
  \end{pmatrix}$$ 
:::

::: {.Corollary}
## Matrix product rules  
The matrix multiplication is distributive with respect
to the addition and scalar multiplication, and associative but not
commutative. 

$$
\begin{aligned}
 ({\rm A} + {\rm B} ) {\rm C} & =  {\rm A}  {\rm C} + {\rm B}  {\rm C}  , \\
{\rm C}  ({\rm A} + {\rm B} )  & =  {\rm C} {\rm A}  + {\rm C}  {\rm B} , \\
(a {\rm A})(b {\rm B}) &= (ab) {\rm A}  {\rm B} , \\
({\rm A} {\rm B} ){\rm C}  & = {\rm A} ({\rm B} {\rm C}) 
\end{aligned}
$$
:::

::: {.callout-warning}
Matrix multiplication is not commutative in general: ${\rm A}  {\rm B} \neq  {\rm B}  {\rm A}$. This is clear already for dimensional reasons. If
${\rm A} \in M_{m\times r}$ and ${\rm B} \in M_{r\times n}$ then
${\rm AB}\in M_{m\times n}$, but the product ${\rm BA}$ is not even
defined, unless $m=n$. However, even if we have quadratic matrices
$(m=r=n)$, the product is not necessarily commutative as the following example
shows: 
$$\begin{pmatrix}
 0 & 0\\
 1 &0
  \end{pmatrix} 
\begin{pmatrix}
 2 & 0\\
 0 &0
  \end{pmatrix} 
 = \begin{pmatrix}
 0 & 0\\
 2 &0
  \end{pmatrix}  \quad \text{but} \quad 
  \begin{pmatrix}
 2 & 0\\
 0 &0
  \end{pmatrix} 
  \begin{pmatrix}
 0 & 0\\
 1 &0
  \end{pmatrix} 
 = \begin{pmatrix}
 0 & 0\\
 0 &0
  \end{pmatrix}.$$
:::

::: {#exm-    }
Let $$A= \begin{pmatrix}
 2  & 1  & {-1}  \\
 0  & 2  & 1  \\
 1  & 0  & 2  \\
\end{pmatrix} ,\quad B= \begin{pmatrix}
 1  & 0 & 1  \\
 2  & 1 & 2 \\
\end{pmatrix} ,\quad C= \begin{pmatrix}
 1  \\
 2  \\
 1  \\
\end{pmatrix} .$$ *AB*, *CA*, *CB* are not defined, but the other product are
$$BA= \begin{pmatrix}
 3  & 1 & 1 \\
 6  & 4 & 3 \\
\end{pmatrix} ,\quad BC= \begin{pmatrix}
 2  \\
 6  \\
\end{pmatrix} ,\quad 
AC= \begin{pmatrix}
 3  \\
 5  \\
 3  \\
\end{pmatrix} ,\quad 
A^2= \begin{pmatrix}
 3  & 4  & {-3}  \\
 1  & 4  & 4  \\
 4  & 1  & 3  \\
\end{pmatrix}$$.
:::

The matrix $C$ in the above example shows that vectors
$\in \mathbb{R}^{n}$ can be considered as $n\times1$ matrices.
Similarly, a $1\times n$ matrix is often called a "row-vector". We can
use the operation of transposition to convert a vector to a "row vector"
and vice versa. This operation is defined for arbitrary matrices.


::: {.Definition}
## Transpose of a Matrix
If $A=[a_{i\,j} ]$ is an
$m\times n$ matrix, then the transpose of $A$, denoted by $A^T$, is
defined as $$A^T=[a_{j,i}]  \in M_{n\times m}  .$$
:::

This means that, for example, the first row of $A^T$ is the first column
of $A$ and so on. Note that unless we have a square matrix ($m=n$), the
transpose of a matrix is of a different type than the original matrix.
In particular, the transpose of a column matrix (i.e. a $n\times 1$
matrix ) is a row matrix, and vice versa.

::: {#exm-    } 
*Example* Check that ${\rm A}{\rm A}^{-1}= {\rm I}$ and ${\rm A}^{-1}{\rm A} = {\rm I}$.
$$\left( \begin{array}{rrr}
 2  & 0  & 1  \\
 1  & 0  & 3  
\end{array}  \right)^T=\left( \begin{array}{rrr}
 2  & 1  \\
 0 & 0  \\
 1 & 3 \\
\end{array}  \right)    \qquad  (2, 0 , 1)^{T} = \scriptstyle \begin{pmatrix}   2\\ 0 \\ 1  \end{pmatrix} \textstyle$$
:::

::: {.Corollary}
## Properties of the Transpose
$$\begin{aligned}
&(i)  \quad ({\rm A}^T)^T  =  {\rm A}   \\ 
&(ii) \quad ({\rm A}+{\rm B})^T  =  {\rm A}^T+{\rm B}^T \\
&(iii)\quad ({\rm A}{\rm B})^T  =  {\rm B}^T{\rm A}^T 
\end{aligned}$$
:::

::: {.Proof}
(i) follows directly from the definition. 
(ii)
$$\left( ({\rm A} + {\rm B})^{T}\right)_{ij} =  ({\rm A} + {\rm B})_{ji} = {\rm A}_{ji} + {\rm B}_{ji}  = ({\rm A}^{T})_{ij} + ({\rm B}^{T})_{ij}$$
(iii) This requires, of course, that the product is defined, that is
${\rm A}\in M_{m \times r}$ and ${\rm B}\in M_{r \times n}$.
$$\begin{aligned}
 ({\rm A} {\rm B})_{ij} & = \sum_{s=1}^{r} A_{is}B_{sj} \\
\left( ({\rm A} {\rm B})^{T}\right)_{ij} & = \sum_{s=1}^{r} A_{js}B_{si} \\
& = \sum_{s=1}^{r} ({\rm A}^{T})_{sj}({\rm B}^{T})_{is}  \\
& =\sum_{s=1}^{r} ({\rm B}^{T})_{is} ({\rm A}^{T})_{sj} = {\rm B}^{T} {\rm A}^{T}
\end{aligned}.$$ ◻
:::

## Symmetric and Skew-symmetric Matrices {#subsec:symmetric}

::: {.Definition}
## Symmetric and skew-symmetric matrices
Let ${\rm A}$ be a square matrix. If ${\rm A}={\rm A}^T$ then ${\rm A}$ is said to be
symmetric. If ${\rm A}= - {\rm A}^T$ then ${\rm A}$ is said to be skew-symmetric. 
:::

Some matrices are neither symmetric nor skew-symmetric. However, every
square matrix can be written uniquely as the sum of a symmetric and
skew-symmetric matrix because of the identity
$${\rm A} = \frac{{\rm A}^{T}+{\rm A}}{2} + \frac{{\rm A}- {\rm A}^{T}}{2} ,$$
noting that $(A^{T}+A)/2$ is symmetric, and $({\rm A}- {\rm A}^{T} )/2$
is skew-symmetric.

::: {#exm-} 
$$\left( \begin{array}{rrr}
 2  & 0  & {-4}  \\
 {-2}  & 2  & 0  \\
 2  & 4  & 6  
\end{array}  \right) =\left(\begin{array}{rrr}
 2  & {-1}  & {-1}  \\
 {-1}  & 2  & 2  \\
 {-1}  & 2  & 6  
\end{array}  \right)+\left( \begin{array}{rrr}
 0  & 1  & {-3}  \\
 {-1}  & 0  & {-2}  \\
 3  & 2  & 0  
\end{array}  \right).$$
:::

## Inverse Matrix

There exists a neutral element ${\rm I} \in M_{n\times n}$ called the
identity matrix for the multiplication, which consists of 1's down the
main diagonal and 0's everywhere else: $${\rm I} =  \begin{pmatrix}
          1 &0  &\ldots   &0  \\
          0 &1  &   &\vdots   \\
           \vdots  &   & \ddots  &0 \\
          0   &  \ldots &  0&1
        \end{pmatrix}$$

The existence of a neutral element with respect to matrix multiplication
means: $$\begin{aligned}
{\rm A} {\rm I} & = {\rm A}  \quad \text{and} \quad  {\rm I}  {\rm A} = {\rm A}  \quad \text{for all} \ {\rm A} \in M_ {m\times r}.
\end{aligned}$$ Note that if ${\rm A} \in M_{m\times r}$ then in the
first equation ${\rm I}\in M_{r\times r}$ while in the second equation
${\rm I}\in M_{m\times m}$.

::: {.Definition} 
# Inverse of a matrix
Given two square matrices ${\rm A}$ and ${\rm B}$ which
satisfy
$${\rm A} {\rm B} = {\rm B}  {\rm A} = {\rm I}$$
then ${\rm B} = {\rm A}^{-1}$ is called the inverse of ${\rm A}$. If
${\rm A}$ has an inverse, it is said to be non-singular. Any matrix
which does not have an inverse is said to be singular.
:::

::: {.callout-note}
## Remark
+ Note that we have have to require both ${\rm A} {\rm B} ={\rm I}$ and ${\rm B} {\rm A} ={\rm I}$ since the matrix product is not commutative. 
+ Non-square matrices can never have an inverse (see remark -@nte-nonsquare).
:::

::: {.callout-warning}
A division of matrices is **not** defined. 
:::
That we don't have a division of matrices is due to the fact that many matrices don't have an inverse contrary to the multiplication of real numbers where only 0 doesn't have an inverse. And even if they do have an inverse it matters whether we multiply from the left or the right, as the multiplication is not commutative. For instance the matrix equation (${\rm A,B,C}$  are $n \times n$ matrices)
$$ {\rm A}{\rm B} = {\rm C}$$ can be solved for ${\rm B}$ using the inverse of ${\rm A}$, provided it exists. We multiply from the left by ${\rm A}^{-1}$:
$$ {\rm A}^{-1}{\rm A}{\rm B} = {\rm A}^{-1}{\rm C}$$ 
$$ \Rightarrow {\rm I}{\rm B} = {\rm B} = {\rm A}^{-1}{\rm C}$$ 
However, we can't write ${\rm C}/{\rm A}$ as that would not indicate whether we have to multiply with the inverse from the left or right.


::: {#exm-}
$$A=\left[ \begin{array}{ccc}
 2 & 0 & 1 \\
 1 & {-2} & 1 \\
 3 & 1 & 1 
\end{array}  \right], \quad 
A^{-1}=\left[ \begin{array}{ccc}
 {-3} & 1 & 2 \\
 2 & {-1} & {-1} \\
 7 & {-2} & {-4} \\
\end{array}  \right]$$
:::

::: {.Corollary}
## Inverse of Tranpose and Product
If ${\rm A}$ and ${\rm B}$ are non-singular
$n\times n$ matrices then $$({\rm A}^{-1})^T  =  ({\rm A}^T)^{-1}$$ and
$$({\rm A}{\rm B})^{-1}={\rm B}^{-1}{\rm A}^{-1}$$
:::

::: {.Proof}
 $$ {\rm I}  =   {\rm I}^T =  ({\rm A}^{-1}{\rm A})^T  =  {\rm A}^T\left({\rm A}^{-1}\right)^{T} $$ 
and
$$ {\rm I}  =   {\rm I}^T =  ({\rm A} {\rm A}^{-1})^T  = \left( {\rm A}^{-1}\right)^{T} {\rm A}^T $$
imply that $\left({\rm A}^{-1}\right)^T  = \left({\rm A}^T\right)^{-1}$.

The second identity is proved by
$$(AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=AI_n A^{-1}=AA^{-1}=I_n .$$ and
the corresponding reverse sequence:
$$(B^{-1}A^{-1}(AB))=B^{-1}A^{-1}AB=I_n .$$ ◻
:::

## Gaussian Elimination

In the previous sections, we considered individual equations of the type
$$a_{1} x_{1} + a_{2} x_{2}  + ... + a_{n} x_{n} = d.$$ In the
following, we will consider systems of such equations and use a method
called Gaussian elimination to determine solutions.

We start with a simple example. Consider the system 
$$\begin{aligned}
2x + 3y  & = 1,  \\
x+y & = 2 .  
\end{aligned}$$ 
Usually we would solve the system by solving e.g. the
second equation for $x$ and substituting the result in the first
equation: 
\begin{align*}
 2(2-y) + 3y & = 1   \\ 
 \Leftrightarrow   (3-2\cdot 1)y & = 1- 2\cdot 2 \\
 \Rightarrow y & =-3 
\end{align*} 
The result $y=-3$ is then substituted back in either of
the two equations, and we find $x=5$. When it comes to systems with
large numbers of variables and equations, this approach becomes
increasingly hard to follow, and we need a more systematic approach.
Note that second line above can be understood as the result of subtracting twice
the second equation from the first equation in the original system. 
That is, we converted the system 
\begin{align*}
2x + 3y  &=  1 ,\\
2x+ 2y &= 4 \ .
\end{align*}
into the equivalent system 
\begin{align*}
2x + 3y  &=  1 ,\\
      - y &= 3 \ , 
\end{align*}
by subtracting equations. \"Equivalent\" here means that
both systems have the same solutions. This system is now trivial to
solve. Operations which lead to equivalent systems are, e.g.,
multiplying an equation by a (non-zero) number and, as we have seen
above, adding multiples of other equations.

::: {#exm-ge1}
Let us try this method with a more complicated system. We indicate on
the right-hand side of the system the operation (multiplication and
subtraction) which leads to the new system. 
$$\begin{aligned}
 x+\phantom{2}y-\phantom{2}z+2t &= 12,\\
 2x-\phantom{2}y+\phantom{2}z-\phantom{2}t &=-5,    \qquad Eq. 2- 2 Eq. 1\\
 x-2y+3z+4t &= 10, \qquad Eq. 3-  Eq. 1\\ 
 3x+3y+\phantom{2}z+\phantom{2}t &= 12. \qquad Eq. 4- 3 Eq. 1
\end{aligned}$$ 
$$\begin{aligned}
 x+\phantom{2}y-\phantom{2}z+2t &= 12,\\
\Leftrightarrow \qquad  0 - 3 y +3 z- 5 t &=-29,    \qquad  \\
 0-3y+4z+2t &= -2, \qquad Eq. 3-  Eq. 2\\ 
 0+\phantom{2}0 +4 z -5 t &= -24. 
\end{aligned}$$ 
$$\begin{aligned}
 x+\phantom{2}y-\phantom{2}z+2t &= 12,\\
\Leftrightarrow \qquad  0 - 3 y +3 z- 5 t &=-29,    \qquad  \\
 0 + \phantom{2}0 + \phantom{1} z+7t &= 27,\\ 
 0+\phantom{2}0 +4 z -5 t &= -24.  \qquad  Eq.4- 4 Eq. 3
\end{aligned}$$ 
$$\begin{aligned}
 x+\phantom{2}y-\phantom{2}z+2t &= 12,\\
\Leftrightarrow \qquad   0 - 3 y +3 z- 5 t &=-29,    \qquad  \phantom{Eq.4- 4 Eq. 3} \\
 0 + \phantom{2}0 + \phantom{1} z+7t &= 27,\\ 
 0+\phantom{2}0 + \phantom{2}0  -33 t &= -132.  
\end{aligned}$$ 
Now the system can be solved recursively: The last
equation implies $t=4$, which can be used in the third equation to find
$z=-1$. This, in turn, leads to $y=2$ in the second equation, and
eventually, we get $x=1$ from the first equation.
:::

Note that it was now possible to easily solve the system because the
non-zero entries on the left-hand side form an upper triangle, so that
we can successively solve for all the variables (back-substitution).

To make the notation more compact, we can suppress the variables and
write the system as a matrix, called *augmented matrix*. Instead of refering to equations we refer now to rows, and $2 R_2 - R_1$ for instance means take twice row 2 and subtract row 1. For the example
from above, we have e.g. 

$$\begin{array}{rrrr|rl}
 1 &   1& -1 & 2& 12 & \\
 2 &  -1 & 1& -1 &-5 & (R_{2}-2R_{1})\\
1  &   -2& 3 & 4  & 10 & (R_{3}-R_{1})\\
3 & 3 &1 & 1 & 12  &   (R_{4}-3R_{1})
\end{array}  \quad \Rightarrow \quad 
 \begin{array}{rrrr|rl}
 1 &   1& -1 & 2& 12  &\\
 0&  -3 & 3& -5 &-29 & \\
0  &   -3& 4 & 2  & -2 & (R_{3}-R_{2}) \\
0 & 0 &4 & -5 & -24 &    
\end{array}$$ 
$$\Rightarrow \begin{array}{rrrr|rl}
 1 &   1& -1 & 2& 12 & \\
 0&  -3 & 3& -5 &-29 &\\
0  &   0& 1 & 7  & 27 &\\
0 & 0 &4 & -5 & -24   & (R_{4}-4R_{3}) 
\end{array}
\quad \Rightarrow \begin{array}{rrrr|r}
 1 &   1& -1 & 2& 12  \\
 0&  -3 & 3& -5 &-29 \\
0  &   0& 1 & 7  & 27 \\
0 & 0 &0 & -33 & -132    
\end{array}$$

This method is called Gauss elimination, named after Carl Friedrich
Gauss (see Appendix [\[Gauss\]](#Gauss){reference-type="ref"
reference="Gauss"}). To bring the system into upper triangular form, we
can use three rules which do not change the solutions of the system of
equations:

1.  multiply an equation by an arbitrary non-zero number,

2.  adding (or subtracting) multiples of equations to other equations
    (not itself!),

3.  change the sequence of equations.

There are two cases where the system can fail to give a unique solution.
The first case is that the system is *underdetermined* (or has
infinitely many solutions), that is, we either have fewer equations than
variables or the equations are not linearly independent. In this case,
we are left with one (or more) free variables, and the best we can do is
express all the other variables in terms of this free variable.

::: {#exm-ge2}
$$\begin{aligned}
& &\begin{array}{rrrr|r}
 1 &   1& -1 & 2& 12  \\
 2 &  -1 & 1& -1 &-5 \\
1  &   -2& 3 & 4  & 10 \\
2 & 2 & -3 & -3 & -3    
\end{array}  \quad \Rightarrow \quad 
 \begin{array}{rrrr|r}
 1 &   1& -1 & 2& 12  \\
0&   -3 & 3& -5 &-29 \\
 0&  -3& 4 & 2  & -2 \\
  0&  0 &-1 & -7 & -27    
\end{array} \\
&\Rightarrow& \begin{array}{rrrr|r}
 1 &   1& -1 & 2& 12  \\
0&   -3 & 3& -5 &-29 \\
0& 0 & 1 & 7  & 27 \\
0& 0 & -1 & -7 & -27    
\end{array} \Rightarrow \begin{array}{rrrr|r}
1 &   1& -1 & 2& 12  \\
0&   -3 & 3& -5 &-29 \\
0& 0 & 1 & 7  & 27 \\
0 & 0 & 0 &  0 & 0    
\end{array} 
\end{aligned}$$ Here, the last equation does not determine the variable
$t$. We can, however, express all the other variables in terms of $t$.
The third step yields $z=27-7 t$, the second step: $y=110/3-26/3t$ and
eventually we find $x= 7/3-t/3$. Note that we can write this as the
parametric form of a line
$$\vec{r} =  \begin{pmatrix}   7/3 \\ 110/3 \\ 27 \\ 0  \end{pmatrix}  + t  \begin{pmatrix}   -1/3 \\ -26/3 \\ -7 \\ 1  \end{pmatrix}. $$
Indeed, we would have the same situation if we had ignored the last
equation in the first place. The equation is superfluous; unfortunately,
it is often not trivial to recognise which equation can be omitted if a
system is underdetermined.
:::

The other case where the method can fail to produce a unique solution is
when the system is *overdetermined* or *inconsistent*.

::: {#exm-ge3}
$$\begin{array}{rrr|r}
  1 & 0& 1 &1 \\
   0& 1 & 1  & 2 \\
    1 & 2 & 3 & -3    
\end{array}
\Rightarrow
\begin{array}{rrr|r}
  1 & 0& 1 &1 \\
   0& 1 & 1  & 2 \\
    0 & 2 & 2 & -4    
\end{array}
\Rightarrow
\begin{array}{rrr|r}
  1 & 0& 1 &1 \\
   0& 1 & 1  & 2 \\
    0 & 0 & 0 & -8    
\end{array}$$ In this case, the last equation states: $0\cdot z=-8$,
which is impossible to satisfy for any $z$. This case often occurs if
there are more equations than variables.
:::

All three cases ---the case of a unique solution, the overdetermined
case, and the underdetermined case --- can be understood in geometric
terms. We recall that the solution to each equation is a hyperplane in
$\mathbb{R}^{n}$. Hence, the solution to the whole system is the
intersection of all these hyperplanes. Two basic situations can be
distinguished.

1.  The set of normal vectors of the planes is linearly independent. The
    dimension of the space of solutions is $m=n-k$, where $n$ is the
    number of unknowns and $k$ is the number of equations, that is, all
    variables can be expressed in terms of $m$ free variables or
    parameters. For the case $k=n$, there are no free parameters, and we
    obtain a unique solution.

2.  The set of normal vectors of the hyperplanes is linearly dependent.
    This is always the case if $k>n$.

    1.  One (or more) equations are linear combinations of the others.
        These equations can be removed from the system, until we are
        left with a set of linearly independent equations. If the normal
        vectors are linearly independent, case 1 applies; otherwise, we
        have case 2b).

    2.  The equations are linearly independent and the system has no
        solution.

In @exm-ge1 we had the situation of case 1 with $k=n$ and hence a
unique solution.\
In @exm-ge2, we had the situation that one equation is a linear
combination of the others: $(Eq.~4) = (Eq.~1)+(Eq.~2) - (Eq.~3)$. This
is case 2a) in the above scheme. If we remove this equation, we have
situation 1 with $n=4$, $k=3$ and therefore $m=1$, which corresponds to
a line as a solution.

In @exm-ge3, we had case 2b). The normal vectors of the three planes
are
$$\vec{n}_{1} = \scriptscriptstyle \begin{pmatrix}   1 \\ 0 \\ 1  \end{pmatrix} \textstyle, \quad \vec{n}_{2} = \scriptscriptstyle \begin{pmatrix}   0 \\ 1 \\ 1  \end{pmatrix} \textstyle, \quad \vec{n}_{1} = \scriptscriptstyle \begin{pmatrix}   1 \\ 2 \\ 3  \end{pmatrix} \textstyle .$$
They are not linearly independent
$\vec{n}_{3} = \vec{n}_{1} +2 \vec{n}_{2}$. But the equations are
linearly independent, i.e. $Eq. 3 \neq Eq_{1} + 2 Eq. 2$.

<figure>
<p><img src="hyperpl-r3.jpg" style="width:6cm" alt="image" /> <img
src="hyperpl-r3b.jpg" style="width:6cm" alt="image" /></p>
<figcaption>The left figure is the situation in Example 3. The three
hyperplanes intersect in three parallel lines. The right figure shows
the situation of three linearly independent hyperplanes in <span
class="math inline">ℝ<sup>3</sup></span> which have one point in common.
This is situation 1.</figcaption>
</figure>

## Elementary Matrices

Each of the three transformations we performed on the augmented matrix
can be achieved by multiplying the matrix on the left by an *elementary
matrix*. The corresponding elementary matrix can be found by applying
one of the three elementary row transformations to the identity matrix.

::: {.Definition}
An elementary matrix is an $n\times n$ matrix which
can be obtained from the identity matrix $I_{n}$ by performing on
$I_{n}$ a single elementary row transformation.
:::

::: {#exm-    }
$$\left(\begin{array}{ccc}1 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 1\end{array}\right)$$
is an elementary matrix. It can be obtained by multiplying row $2$ of
the identity matrix by $3$. In other words, we are performing the row
operation $3R_{2}\rightarrow R_{2}$.
:::

::: {#exm-    }
$$\left(\begin{array}{ccc}1 & 0 & 0 \\ 0 & 1 & 0 \\ -2 & 0 & 1\end{array}\right)$$
is an elementary matrix. It can be obtained by replacing row $3$ of the
identity matrix by row $3$ plus $-2$ times row $1$. In other words, we
are performing the row operation $R_{3}-2 R_{1}\rightarrow R_{1}$.
:::

::: {#exm-    }
$$\left(\begin{array}{ccc}0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1\end{array}\right)$$
is an elementary matrix. It can be obtained by switching rows $1$ and
$2$ of the identity matrix. In other words, we are performing on the row
operation $R_{1}\leftrightarrow R_{2}$.
:::

Suppose we want to perform an elementary row operation on a matrix $A$.
In that case, it is equivalent to multiplying the matrix $A$ on the left
by the elementary matrix obtained from the identity matrix by the same
transformation.


Interchanging Rows: $R_{i}\leftrightarrow R_{j}$

:   To interchange rows $i$ and $j$ of matrix $A$
    ($R_{i}\leftrightarrow R_{j}$), we multiply $A$ on the left by the
    elementary matrix obtained from the identity matrix in which rows
    $i$ and $j$ have been interchanged.

Multiplying a Row by a Constant: $aR_{i}\rightarrow R_{i}$

:   To multiply row $i$ of matrix $A$ by a number $a$
    ($aR_{i}\rightarrow R_{i}$), we multiply $A$ on the left by the
    elementary matrix obtained from the identity matrix in which row $i$
    has been multiplied by $a$.

Replacing a Row by Itself Plus a Multiple of Another: $R_{i}+aR_{j}\rightarrow R_{i}$

:   To replace a row $i$ by itself plus a multiple of another row $j$
    ($R_{i}+aR_{j}\rightarrow R_{i}$), we multiply $A$ on the left by
    the elementary matrix obtained from the identity matrix in which row
    $i$ has been replaced by itself plus row $j$ multiplied by $a$.


::: {#exm-    }
$R_{1}\leftrightarrow R_{3}$:
$$\left(\begin{array}{ccc}0 & 0 & 1\\ 0 & 1 & 0 \\ 1 & 0 & 0\end{array}\right)\left(\begin{array}{ccc}1 & 2 & 3\\ 4 & 5 & 6 \\ 7 & 8 & 9\end{array}\right)
=\left(\begin{array}{ccc}7 & 8 & 9\\ 4 & 5 & 6 \\ 1 & 2 & 3\end{array}\right).$$
:::

::: {#exm-    }
$3R_{1}\rightarrow R_{1}$:
$$\left(\begin{array}{ccc}3 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array}\right)\left(\begin{array}{ccc}1 & 2 & 3\\ 4 & 5 & 6 \\ 7 & 8 & 9\end{array}\right)
=\left(\begin{array}{ccc}3 & 6 & 9\\ 4 & 5 & 6 \\ 7 & 8 & 9\end{array}\right).$$
:::

::: {#exm-    }
$R_{2}-R_{1}\rightarrow R_{2}$:
$$\left(\begin{array}{ccc}1 & 0 & 0\\ -1 & 1 & 0 \\ 0 & 0 & 1\end{array}\right)\left(\begin{array}{ccc}1 & 2 & 3\\ 4 & 5 & 6 \\ 7 & 8 & 9\end{array}\right)
=\left(\begin{array}{ccc}1 & 2 & 3\\ 3 & 3 & 3 \\ 7 & 8 & 9\end{array}\right).$$
:::

::: {.Corollary} 
## Inverse of elementary matrices
The elementary matrices are nonsingular. Furthermore,
their inverse is also an elementary matrix. 
:::

- The inverse of the elementary matrix which interchanges two rows is
  itself. For example
  $$\left(\begin{array}{ccc}0 & 1 & 0\\ 1 & 0 & 0 \\ 0 & 0 & 1\end{array}\right)^{-1} =\left(\begin{array}{ccc}0 & 1 & 0\\ 1 & 0 & 0 \\ 0 & 0 & 1\end{array}\right).$$

- The inverse of the elementary matrix which multiply a row $i$ by a
  constant $a$, i.e. $aR_{i}\rightarrow R_{i}$ is the elementary matrix
  which multiply a row $i$ by $\frac{1}{a}$, i.e. $\frac{1}{a}R_{i}\rightarrow R_{i}$. For example, 
  $$\left(\begin{array}{ccc}1 & 0 & 0\\ 0 & a & 0 \\ 0 & 0 & 1\end{array}\right)^{-1} =\left(\begin{array}{ccc}1 & 0 & 0\\ 0 & \frac{1}{a} & 0 \\ 0 & 0 & 1\end{array}\right).$$

- The inverse of the elementary matrix which replaces a row $i$ by
  itself plus a multiple of a row $j$, i.e.
  $R_{i}+aR_{j}\rightarrow R_{i}$ is the elementary matrix which
  replaces a row $i$ by itself minus a multiple of a row $j$. For
  example, 
  $$\left(\begin{array}{ccc}1 & 0 & 0\\ 0 & 1 & 0 \\ a & 0 & 1\end{array}\right)^{-1}=\left(\begin{array}{ccc}1 & 0 & 0\\ 0 & 1 & 0 \\ -a & 0 & 1\end{array}\right).$$

## Calculating the inverse

Now we can prove that a matrix is invertible if we can convert it to the
identity matrix with elementary row operations. Assume that there exists a sequence of row operations, with corresponding elementary matrices ${\rm E}_i$, that converts a matrix ${\rm A}$ into an identity matrix.
$$ {\rm I} = {\rm E}_{k} ... {\rm E}_{2} {\rm E}_{1} {\rm A} $$ {#eq-matrixinv1}
If we call the product of these elementary matrice ${\bf B}$ then
$${\rm I} = {\rm B}  {\rm A}  \quad \text{with} \quad  {\rm B} = {\rm E}_{k} ... {\rm E}_{2} {\rm E}_{1} {\rm I} $$
 To show that ${\rm B}$ is the inverse, we have to show
in addition ${\rm A}{\rm B} = {\rm I}$. From Eq. -@eq-matrixinv1 it follows by multiplying successively with the
inverse of the elementary matrices ${\rm E}_{j}$ from the left:
$$\begin{aligned}
{\rm E}_{k}^{-1}{\rm I} &=\underbrace{{\rm E}_{k}^{-1}{\rm E}_{k}}_{={\rm I}} {\rm E}_{k-1} ... {\rm E}_{2} {\rm E}_{1} {\rm A} \\
& \quad \vdots  \\
 {\rm E}_{1}^{-1}... {\rm E}_{k}^{-1}{\rm I} &=  {\rm A} 
 \end{aligned}$$ 
Ηence,
$$\begin{aligned}
 {\rm A}  {\rm B} & ={\rm E}_{1}^{-1}... {\rm E}_{k}^{-1}{\rm I}  {\rm E}_{k} ... {\rm E}_{2} {\rm E}_{1} {\rm I}   \\
 & ={\rm E}_{1}^{-1}... \underbrace{{\rm E}_{k}^{-1}{\rm E}_{k}}_{={\rm I}} ... {\rm E}_{2} {\rm E}_{1} \\
 & \quad \vdots \\
 &= {\rm I}
\end{aligned}$$

Hence, we can obtain the inverse of a matrix ${\rm A}$ (if it exists) by
applying the same row operations which convert ${\rm A}$ into an
identity matrix. The method is usually applied to the augmented matrix
${\rm A} \vert  {\rm  I}$ where any row operations are executed
simultaneously on both sides, to reduce it to
${\rm I} \vert  {\rm A}^{-1}$.

::: {#exm-inv}
To calculate ${\rm A}^{-1}$ when $${\rm A} =\left( \begin{array}{rrr}
 1  & 3  & 3  \\
 1  & 4  & 3  \\
 1  & 3  & 4  
\end{array} \right),$$ we reduce the augmented matrix to an identity
matrix using elementary row operations. 
$$\begin{aligned}
& \left. \begin{array}{rrr}
 1  & 3  & 3  \\
 1  & 4  & 3  \\
 1  & 3 & 4  
\end{array}  \right| \begin{array}{rrrr}
 1  & 0  & 0  & \qquad  \\
 0  & 1  & 0  & \qquad R_{2} - R_{1}\\
 0  & 0  & 1  & \qquad R_{3} - R_{1}
\end{array} \\
\Rightarrow \quad  &\left. \begin{array}{rrr}
 1  & 3  & 3  \\
 0  & 1  & 0  \\
 0 & 0 & 1 
\end{array}  \right| \begin{array}{rrrr}
 1  & 0  & 0  & \qquad R_{1} - 3 R_{2}- 3R_{3}\\
 -1  & 1  & 0  & \qquad \\
 -1  & 0  & 1  & \qquad 
\end{array}\\
\Rightarrow \quad  &\left. \begin{array}{rrr}
 1  & 0  & 0  \\
 0  & 1  & 0  \\
 0 & 0 & 1 
\end{array}  \right| \begin{array}{rrrr}
 7  & -3  & -3  & \qquad  \\
 -1  & 1  & 0  & \qquad \\
 -1  & 0  & 1  & \qquad 
\end{array} 
\end{aligned}$$
:::

::: {#exm-sytemofeqs}
The solution of the equations $$\left( \begin{array}{rrr}
 2  & 0  & 1  \\
 1  & {-2}  & 1  \\
 3  & 1  & 1 
\end{array} \right)\left(\begin{array}{r}
 x  \\
 y  \\
 z  
\end{array}  \right)=\left( \begin{array}{r}
 2  \\
 0  \\
 2  
\end{array}  \right)$$ is given by $$\left( \begin{array}{r}
 x  \\
 y  \\
 z  
\end{array} \right)=\left( \begin{array}{rrr}
 {-3}  & 1  & 2  \\
 2  & {-1}  & {-1}  \\
 7  & {-2}  & {-4}  
\end{array}  \right) \left( \begin{array}{r}
 2  \\
 0  \\
 2  
\end{array} \right)=\left( \begin{array}{r}
-2 \\
 2  \\
 6  
\end{array}  \right)$$
:::


## Factorising Matrices: The LU factorisation

::: {.Definition}
# Upper/Lower Triangular Matrix 
A square matrix is upper (lower) triangular if all its
entries below (above)the main diagonal are zero.
:::

So far, we know two methods to solve a system of linear equations
$$A\vec{r} =  \vec{q},  \quad \vec{r}, \vec{q}\in \mathbb{R}^{n}, {\rm A}\in M_{n\times n},$$
Gaussian elimination and the inversion of the matrix $A$. The former
uses the augmented matrix $A|\vec{q}$ and elementary row operations to
convert the system into a form where the left-hand side is an upper
triangular matrix. In many applications, e.g., in algorithms to solve
partial differential equations, ${\rm A}$ is a large matrix
($1000 \ times 1000$ is not unusual), and the system has to be solved
repeatedly for various right-hand sides $\ vec {q}$. Here, the Gaussian
elimination is very inefficient, as for every new $\vec{q}$, the system
has to be solved again. Inverting the matrix ${\rm A}$ seems to be much
more efficient since we invert the matrix only once and then only apply
${\rm A}^{-1}$ to every new $\vec{q}$.
$${\rm A} \vec{r} = \vec{q} \quad \Rightarrow \vec{r} =  {\rm A}^{-1}  \vec{q}.$$
However, the matrix inversion of such large matrices itself is often
numerically difficult, "numerically unstable" that is, it tends to
produce very large (or very small) numbers, which in turn produce
significant numerical errors.

Here, another method, the so-called LU decomposition, has proven to be
very efficient. The name is derived from the representation of the
matrix $A$ as a product
$${\rm A} = {\rm L}{\rm U} ;  \quad  {\rm A},  {\rm L}, {\rm U} \in M_{n\times n}$$
of two triangular matrices ${\rm L}$ and ${\rm U}$, where ${\rm L}$ is a
lower triangular matrix and ${\rm U}$ is an upper triangular matrix.

The advantage of this method is that determining ${\rm L}$ and ${\rm U}$
is faster (i.e., it uses fewer steps) than inverting ${\rm A}$, and we
can still solve the system comparatively easily. The methods consist of
three steps:

::: description
First step: Determine ${\rm L}$ and ${\rm U}$.

Second step: Solve
${\rm L}\underbrace{({\rm U} \vec{r})}_{ \vec{s} }= \vec{q}$ for
$\vec{s}$.

Third step: Solve ${\rm U} \vec{r} = \vec{s}$.
:::

Step 1: We know already (see Gauss elimination) that we can use
elementary row operations to convert a matrix ${\rm A}$ to an upper
triangular matrix. We have also seen that each of these row operations
can be expressed by a matrix ${\rm E}_{j}$, so that
$${\rm U} =  {\rm E}_{k} \ldots {\rm E}_{2} {\rm E}_{1} {\rm A} ,$$
where $k$ is the number of row operations we need to bring ${\rm A}$ in
an upper triangular form. For the following, we assume that we do not
need to exchange rows in this process. (The most general case with the
exchange of rows requires a more general representation of ${\rm A}$ as
${\rm A}= {\rm PLU}$ where ${\rm P}$ includes all permutations of rows.)
Then each of the elementary matrices ${\rm E}_{j}$ can be chosen as a
lower triangular matrix and has an inverse ${\rm E}_{j}^{-1}$ which is
again a lower triangular matrix, hence in
$$\underbrace{{\rm E}_{1}^{-1}  {\rm E}_{2}^{-1}\ldots {\rm E}_{k}^{-1}}_{:={\rm L}} {\rm U} =   {\rm A}$$
the product ${\rm E}_{1}^{-1}  {\rm E}_{2}^{- 1}\ldots {\rm E}_{k}^{-1}$
is also a lower triangular matrix.

::: {#exm-lugeneral} 
$$\begin{aligned}
A = &\left(\begin{array}{rrr}
a & b & c\\
d & e & f\\
g & h & k
\end{array}\right)
\begin{array}{c}
 \\
 R_2-\lambda R_1\rightarrow R_2 \\
 R_3-\mu R_1 \rightarrow R_3
 \end{array} \\
 \rightarrow&
 \left(\begin{array}{rrr}
 a & b & c \\
 0 & e' & f' \\
 0 & h' & k'
 \end{array}\right)
 \begin{array}{c}
 \\
 \\
 R_3-\omega R_1\rightarrow R_3
 \end{array} \\
 \rightarrow&
 \left(\begin{array}{rrr}
 a & b & c \\
 0 & e' & f' \\
 0 & 0 & k''
 \end{array}\right)={\rm U}.
\end{aligned}$$ 
where $\lambda =d/a$, $\mu =g/a$, $\omega =h'/e'$. Note that this can be accomplished by multiplying
${\rm A}$ from the left by the elementary matrice 
$${\rm E}_1 =\left( 
{{\begin{array}{rrr}
 1  & 0  & 0  \\
 {-\lambda }  & 1  & 0  \\
 0  & 0  & 1  \\
\end{array} }} \right), \quad  {\rm E}_2 =\left( 
{{\begin{array}{rrr}
 1  & 0  & 0  \\
 0  & 1  & 0  \\
 {-\mu }  & 0  & 1  \\
\end{array} }} \right),  \quad   {\rm E}_3 =\left(
\begin{array}{rrr}
 1  & 0  & 0  \\
 0  & 1  & 0  \\
 0  & - \omega  & 1  \\
\end{array} \right),$$ which means:\

$R_2-\lambda R_1 \rightarrow R_2$: $$\begin{aligned}
\left(\begin{array}{rrr}
1 & 0 & 0 \\
-\lambda & 1 & 0 \\
0 & 0 & 1
\end{array}\right)
\left(\begin{array}{rrr}
a & b & c\\
d & e & f \\
g & h & k
\end{array}\right) &=
\left(\begin{array}{rrr}
a & b & c \\
d-\lambda a & e-\lambda b & f-\lambda c \\
g & h & k
\end{array}\right) =
\left(\begin{array}{rrr}
a & b & c \\
0 & e' & f' \\
g & h & k
\end{array}\right),
\end{aligned}$$ $R_3-\mu R_1 \rightarrow R_3$: $$\begin{aligned}
\left(\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
-\mu & 0 & 1
\end{array}\right)
\left(\begin{array}{rrr}
a & b & c\\
0 & e' & f' \\
g & h & k
\end{array}\right) &=
\left(\begin{array}{rrr}
a & b & c \\
0 & e' & f' \\
g-\mu a & h-\mu b & k-\mu c
\end{array}\right) =
\left(\begin{array}{rrr}
a & b & c \\
0 & e' & f' \\
0 & h' & k'
\end{array}\right),
\end{aligned}$$ $R_3-\omega R_1\rightarrow R_3$: $$\begin{aligned}
\left(\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & -\omega & 1
\end{array}\right)
\left(\begin{array}{rrr}
a & b & c\\
0 & e' & f' \\
0 & h' & k'
\end{array}\right) &=
\left(\begin{array}{rrr}
a & b & c \\
0 & e' & f' \\
0 & h'-\omega e' & k'-\omega f'
\end{array}\right) =
\left(\begin{array}{rrr}
a & b & c \\
0 & e' & f' \\
0 & 0 & k''
\end{array}\right).
\end{aligned}$$

All three elementary matrices are of the lower triangular type and have
an inverse $${\rm E}_1^{-1} =\left(
{{\begin{array}{rrr}
 1  & 0  & 0  \\
 {\lambda }  & 1  & 0  \\
 0  & 0  & 1  \\
\end{array} }} \right), \quad  {\rm E}_2^{-1} =\left(
{{\begin{array}{rrr}
 1  & 0  & 0  \\
 0  & 1  & 0  \\
 {\mu }  & 0  & 1  \\
\end{array} }} \right), \quad   {\rm E}_3^{-1} =\left( 
\begin{array}{rrr}
 1  & 0  & 0  \\
 0  & 1  & 0  \\
 0  &  \omega  & 1  \\
\end{array} \right)  .$$

When we perform the matrix multiplication to obtain ${\rm L}$:
$${\rm L} ={\rm E}_{1}^{-1}  {\rm E}_{2}^{- 1} {\rm E}_{3}^{-1}  =\left(
\begin{array}{rrr}
 1  & 0  & 0  \\
 \lambda & 1  & 0  \\
 \mu &  \omega  & 1  \\
\end{array} \right),$$ We notice that this matrix multiplication can be
performed by just putting the nonzero off-diagonal entries of the
inverses of the elementary matrices into the appropriate positions in
the matrix ${\ rm L}$. This means that the entries of ${\rm L}$, which
are the multiplying factors in the Gaussian elimination process, can be
easily stored during the process of Gaussian elimination.

Thus, the matrix $A$ was factorised into the product of the lower
triangular matrix ${\rm L}$ and the upper triangular matrix ${\rm U}$ as
follows $$\left(\begin{array}{rrr}
a & b & c\\
d & e & f\\
g & h & k
\end{array}\right) =
\left(\begin{array}{rrr}
 1 & 0  & 0 \ \\
 \lambda & 1 & 0  \\
 \mu &  \omega & 1  \\
\end{array} \right)
 \left(\begin{array}{rrr}
 a & b & c \\
 0 & e' & f' \\
 0 & 0 & k''
 \end{array}\right)$$
:::

We illustrate the method by a simple example.

::: {#exm-lufactorization}
Let $${\rm A} =\left(\begin{array}{rrr}
 1  & 2  & 1  \\
 2  & {-1}  & 3  \\
 {-1}  & 2  & 2  
\end{array} \right).$$ We reduce $A$ to upper triangular form in the
usual way, but record the multiplying factors in a lower triangular
matrix $L$: 
$$\begin{aligned}
& \left(\begin{array}{rrr}
1 & 2 & 1 \\
2 & -1 & 3 \\
-1 & 2 & 2
\end{array}\right)
\begin{array}{r}
\\
R_2-2R_1 \\
R_3+R_1
\end{array} \\
\rightarrow &
\left(\begin{array}{rrr}
1 & 2 & 1 \\
0 & -5 & 1 \\
0 & 4 & 3
\end{array}\right)
\begin{array}{c}
\\
\\
R_3+\frac{4}{5}R_2
\end{array} \\
\rightarrow &
\left(\begin{array}{rrr}
1 & 2 & 1\\
0 & -5 & 1\\
0 & 0 & 19/5
\end{array}\right)
\end{aligned}$$


so $L$ becomes 
$${\rm L} = \left(\begin{array}{rrr}
 1  & 0  & 0  \\
 2  & 1  & 0  \\
 {-1}  & {-4/5}  & 1  
\end{array} \right).$$ Now check that 
$$\left(\begin{array}{rrr}
 1  & 0  & 0  \\
 2  & 1  & 0  \\
 {-1}  & {-4/5}  & 1  \\
\end{array} \right)\left(\begin{array}{rrr}
 1  & 2  & 1  \\
 0  & {-5}  & 1  \\
 0  & 0  & {19/5}  
\end{array} \right)=\left(\begin{array}{rrr}
 1  & 2  & 1  \\
 2  & {-1}  & 3  \\
 {-1}  & 2  & 2  
\end{array} \right).$$
:::

Once we have this factorisation, we can make use of it to solve
${\rm A}\vec{r}=\vec{q}$ as follows:
$${\rm A}\vec{r}={\rm L}{\rm U}\vec{r}=\vec{q}$$ is equivalent to
solving
$${\rm L}\vec{s}=\vec{q}\quad\textrm{for}\quad\vec{s}={\rm U}\vec{r},$$
using forward substitution, followed by $${\rm U}\vec{r}=\vec{s}$$ using
backwards substitution.

::: {#exm-LUfactor2} 
*Example 3.9*. To solve
${\rm A}\vec{r}= \scriptscriptstyle \begin{pmatrix}    2\\ 9 \\ 0  \end{pmatrix} \textstyle$,
first we solve ${\rm L}\vec{s}=\vec{q}$ for
$\vec{s}=(s_{x},s_{y},s_{z})^{T}$: 
$$\begin{aligned}
 & \left( \begin{array}{rrr}
 1  & 0  & 0  \\
 2  & 1  & 0  \\
 -1 & -4/5 & 1 
\end{array} \right) \scriptstyle \begin{pmatrix}   s_{x}\\ s_{y} \\ s_{z}  \end{pmatrix} \textstyle = \scriptstyle \begin{pmatrix}    2\\ 9 \\ 0  \end{pmatrix} \textstyle \\
\Rightarrow & \left\{ \begin{array}{l}
s_{x} =2, \\
s_{y} = 9-2s_{x} = 5,\\
s_{z} = s_{x}+\frac{4}{5}s_{y}=6,
\end{array} \right. \\
\Rightarrow &
 \vec{s}= \scriptstyle \begin{pmatrix}    2\\ 5 \\ 6  \end{pmatrix} \textstyle. 
\end{aligned}$$ 
Then we solve ${\rm U}\vec{r}=\vec{s}$ for
$\vec{r}=(x,y,z)^{T}$: 
$$\begin{aligned}
& \left( \begin{array}{rrr}
1  & 2  & 1  \\
0  & -5 & 1  \\
0  & 0  & 19/5
\end{array} \right) \scriptstyle \begin{pmatrix}   x\\ y \\ z  \end{pmatrix} \textstyle = \scriptstyle \begin{pmatrix}    2\\ 5 \\ 6  \end{pmatrix} \textstyle  \\
\Rightarrow &
\left\{\begin{array}{l}
z = 30/19,\\
y=-\frac{1}{5}(5-z)=-13/19,\\
x=2-2y-z=34/19,
\end{array} \right. \\
\Rightarrow &
 \vec{r}=\scriptstyle \begin{pmatrix}    34/19\\ -13/19 \\ 30/19  \end{pmatrix} \textstyle.
\end{aligned}$$
:::

## Range and Nullspace of a Matrix

Consider a system of linear equations, $A\mathbf{x} = \mathbf{y}$ where
$A$ is an $m \times n$ matrix. We can consider the matrix $A$ as a
mapping from the space of all $\mathbf{x}$ vectors in $\mathbb{R}^{n}$
onto the space of all possible $\mathbf{y}$ vectors, which is a subset
of $\mathbb{R}^{m}$.
$$A: \mathbf{x} \in \mathbb{R}^{n} \longrightarrow \mathbf{y} = A \mathbf{x} \in \mathbb{R}^{m}$$.

:::: {.Corollary}
## Range and Rank
Let $A$ be an $m \times n$ matrix. The image of
$\mathbb{R}^{n}$ under $A$ is a subspace of $\mathbb{R}^{m}$ called the range (or column space) of $A$, denoted by ${\rm range}(A)$, and
$$ {\rm range}(A) = {\rm span}({\rm columns \, of } A).$$
The dimension of the column space is called the rank and is denoted by ${\rm rank}(A)$.
::::

::: {#exm-  }
Calculate the range and rank of 
$$ {\rm A} = \left[\begin{array}{ccc}
            1 & 2 & 3 \\
            2 & 3 & 4 
        \end{array}\right].$$ 
:::

::: {.Solution}
$${\rm range(A)}= {\rm span} \left( \begin{pmatrix} 1 \\ 2  \end{pmatrix} , \begin{pmatrix} 2 \\ 3  \end{pmatrix},\begin{pmatrix} 3 \\ 4  \end{pmatrix} \right)$$
However, the set of vectors (columns of ${\rm A}$) is not linearly independent. A test for linear independence yields:
$$ c_1   \begin{pmatrix} 1 \\ 2  \end{pmatrix}+ c_2 \begin{pmatrix} 2 \\ 3  \end{pmatrix} +c_3 \begin{pmatrix} 3 \\ 4  \end{pmatrix} = \begin{pmatrix} 0 \\ 0  \end{pmatrix} $$ 
leads to 
$$ \left\{\begin{array}{l}
 c_1+2 c_2+3 c_3=0 \\
 2 c_1+ 3 c_2+ 4 c_3=0 
        \end{array}  \right.    \Leftrightarrow  \left\{ \begin{array}{l}
 c_1 = -2 c_2 -3c_3 \\
 2(-2 c_2 -3c_3) + 3c_2+ 4c_3=0 
        \end{array}  \right. $$
$$ \Leftrightarrow  \left\{ \begin{array}{l}
 c_1 = -2 c_2 -3c_3 \\
 - c_2+ 2c_3=0 
        \end{array}  \right.   \Leftrightarrow  \left\{ \begin{array}{l}
 c_1 = c_3 \\
c_2 =- 2 c_3 
        \end{array}  \right. $$
That means we have infinitely many solutions. For every choice of $c_3$ there is a $c_2$ and $c_1$, e.g. $(c_1,c_2,c_3)=(1,-2,1)$ is a solution. We can express for instance the third vector as 
$$ \begin{pmatrix} 3 \\ 4 \end{pmatrix} =  2 \begin{pmatrix} 2 \\ 3 \end{pmatrix} - \begin{pmatrix} 1 \\ 2 \end{pmatrix} $$
so 
$$ {\rm span} \left( \begin{pmatrix} 1 \\ 2  \end{pmatrix} , \begin{pmatrix} 2 \\ 3  \end{pmatrix},\begin{pmatrix} 3 \\ 4  \end{pmatrix} \right) ={\rm span} \left( \begin{pmatrix} 1 \\ 2  \end{pmatrix} , \begin{pmatrix} 2 \\ 3  \end{pmatrix} \right). $$
This means the two vectors form a basis of the column space (range) and the rank is the dimension of the column space, i.e. the number of elements of the basis: 2. 
::: 


In order for  $A\mathbf{x} = \mathbf{b}$ to have a solution, $\mathbf{b}$ has to be in the range of ${\rm A}$:   $\mathbf{b} \in {\rm range}(A)$.

::::: {#exm-    }
Let
$$A = \left[\begin{array}{ccc}
            1 & 0 & 1 \\
            1 & 1 & 3 \\
            0 & 1 & 2
        \end{array}\right]$$

Check whether the system $A\mathbf{x} = \mathbf{b}$ consistent when

(i) $\mathbf{b} = \left[1,1,1\right]^T$,
(ii) $\mathbf{b} = \left[1,2,1\right]^T$.

Under what general conditions on
$\mathbf{b} = \left[\mathbf{b_1},\mathbf{b_2},\mathbf{b_3}\right]^T$ is
the system consistent?

::: {.Solution}
We reduce $A$ to a row echelon form:
$$\left[\begin{array}{ccc} 1 & 0 & 1 \\ 1 & 1 & 3 \\ 0 & 1 & 2 \end{array}\right] \xrightarrow{R_2 - R_1} \left[\begin{array}{ccc} 1 & 0 & 1 \\ 0 & 1 & 2 \\ 0 & 1 & 2 \end{array}\right] \xrightarrow{R_3 - R_2} \left[\begin{array}{ccc} 1 & 0 & 1 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \end{array}\right]$$

Hence, for example, the first two columns span ${\rm range}(A)$ and hence
$\mathbf{b} = \lambda \left[1,1,0\right]^T + \mu \left[0,1,1\right]^T$ $= \left[\mathbf{b_1}, \mathbf{b_1} + \mathbf{b_3}, \mathbf{b_3}\right]^T$
is necessary for the system to be consistent. This is satisfied for
$\mathbf{b} = \left[1,2,1\right]^T$, but not for $\mathbf{b} = \left[1,1,1\right]^T$. Note that the first and the third columns of $A$ also span ${\rm range}(A)$.
Even the last two columns can be used.
:::
:::::


::: {.Definition}
## Nullspace and Nullity
Let $A$ be an $m \times n$ matrix. The nullspace  of $A$, 
is the set of vectors ${\bf x}$ such that $A\mathbf{x} = \mathbf{0}$. The nullspace is also called kernel and denoted by ${\rm Null}(A)$. 

The dimension of the nullspace is called the nullity and is denoted by ${\rm nullity}(A)$.
:::

::: {.Corollary} 
## Nullspace is a subspace
Let $A$ be an $m \times n$ matrix. The nullspace of
$A$ is a subspace of $\mathbb{R}^{n}$.
:::

::: {.Proof}
We have to check the three condition in the definition for a subspace (\ref{subspace}):

(i) Since $A\mathbf{0_n} = \mathbf{0_m},  \mathbf{0_n} \in {\rm Null}(A)$.

(ii) Let $\mathbf{u}$ and $\mathbf{v}$ be in ${\rm Null}(A)$. Therefore
$A\mathbf{u} = \mathbf{0}$ and $A\mathbf{v} = \mathbf{0}$. It follows
that $A(\mathbf{u} + \mathbf{v}) = A\mathbf{u} + A\mathbf{v}  = \mathbf{0} + \mathbf{0} = \mathbf{0}$ Hence, $\mathbf{u} + \mathbf{v} \in {\rm Null}(A)$.

(iii) Finally, for any scalar $\lambda$,
$A(\lambda \mathbf{u}) = \lambda (A\mathbf{u}) = \lambda \mathbf{0} = \mathbf{0}$ and therefore $\lambda \mathbf{u} \in {\rm Null}(A)$. It follows that ${\rm Null}(A)$ is
a subspace of $\mathbb{R}^{n}$.
 ◻
:::

::: {#exm-  }
Calculate the nullspace and nullity of 
$$ {\rm A} = \left(\begin{array}{ccc}
            1 & 2 & 3 \\
            2 & 3 & 4 
        \end{array}\right).$$ 
:::

::: {.Solution}
$$ {\rm A} \mathbf{x} = \left(\begin{array}{ccc}
            1 & 2 & 3 \\
            2 & 3 & 4 
        \end{array}\right)\begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} =\begin{pmatrix} x_1 + 2 x_2+ 3 x_3 \\ 2 x_1 + 3 x_2 +4 x_3\end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
$$ 
The vector equation is equivalent to two scalar equations for the components:
$$ \left\{\begin{array}{l}
 x_1 + 2 x_2 + 3 x_3=0 \\
 2 x_1+ 3 x_2+ 4 x_3=0 
        \end{array}  \right.    \Leftrightarrow  \left\{ \begin{array}{l}
 x_1 = -2 x_2 -3 x_3 \\
 2(-2 x_2 -3 x_3) + 3 x_2+ 4 x_3=0 
        \end{array}  \right. $$
$$ \Leftrightarrow  \left\{ \begin{array}{l}
 x_1 = -2 x_2 -3 x_3 \\
 - x_2+ 2 x_3=0 
        \end{array}  \right.   \Leftrightarrow  \left\{ \begin{array}{l}
 x_1 = x_3 \\
x_2 =- 2 x_3 
        \end{array}  \right. $$
This indicates that there are infinitely many solutions. We can choose one of the variables, e.g.~$x_3$ as a free parameter.
$$ \mathbb{x} = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} x_3 \\ -2 x_3 \\ x_3 \end{pmatrix} = x_3 \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix} = \lambda \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}, \quad \lambda \in \mathbb{R} $$
In the last step we replaced $x_3$ by $\lambda$, as it is convention to use Greek letters for parameters. Thus
$$ {\rm Null(A)} = {\rm span}\left(\begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix} \right)$$
and the nullity of ${\rm A}$ is 1. 
:::

::: {.Definition}
## Row and column space
The span of the columns of a $m \times n$ matrix ${\rm A}$ is called the column space. It is a subspace of $\mathbb{R}^n$. The span of the rows of ${\rm A}$ is called the row space and it is a subspace of $\mathbb{R}^m$.
:::



::: {.Theorem #rowcolumnspace} 
## Dimension of row and column spaces
The row and column spaces of a matrix $A$ have the
same dimension.
:::

::: {.Proof}
Let $R$ be a row echelon form of $A$,
$row(A) = row(R)$, as we used only row operations to convert $A$ to $R$.
$$\begin{aligned}
 {\rm dim}({\rm row}(A)) &=  {\rm dim}({\rm row}(R)) \\
             & = \text{number of nonzero rows of}\ R \\
             & =  \text{number of pivots of}\  R 
\end{aligned} $$

Let this number be called $\gamma$.\
Now ${rm col(A)} \neq {\rm col(R)}$, but the columns of ${\rm A}$ and ${\rm R}$ have the same
dependence relationships. Therefore, ${\rm dim(col(A))} = {\rm dim(col(R))}$. Since
there are $\gamma$ pivots, ${\rm R}$ has $\gamma$ columns that are linearly independent, and the remaining columns
of ${\rm R}$ are linear combinations of them. Thus, ${\rm dim(col(R))} = \gamma$. It follows that ${\rm dim(row(A))} = \gamma = {\rm dim(col(A))}$, as we wished to prove. ◻
:::


::: {.Theorem} 
## Rank Theorem
If $A$ is an $m \times n$ matrix, then ${\rm rank}(A) + {\rm nullity}(A) = n$, where $n$ is the number of columns of $A$.
:::

::: {.Proof}
Let $R$ be a row echelon form of $A$, and suppose that
$rank(A) = \gamma$. Then $R$ has $\gamma$ pivots, so there are $\gamma$ variables corresponding to the leading entries and $n-\gamma$ free variables in the solution to $A\mathbf{x} = \mathbf{0}$.
Since ${\rm dim}({\rm Null}(A)) = n - \gamma$, we have ${\rm rank}(A) + {\rm nullity}(A) = \gamma + (n- \gamma ) = n$. 
:::

:::: {#exm-    }
Find the rank and nullity, and then verify the rank theorem for the matrix:
$$ {\rm A} = \left(\begin{array}{ccc}
            1 & 1 & 0 \\
            1 & 0 & -1 \\
            0 & 2 & 2
        \end{array}\right).$$ 
::::

::: {.Solution} 
In order to find the rank we have to check whether the columns are linearly independent. For this we solve
$$c_1 \begin{pmatrix} 1 \\ 1 \\ 0  \end{pmatrix}+ c_2 \begin{pmatrix} 1 \\ 0 \\ 2  \end{pmatrix} + c_3 \begin{pmatrix} 0 \\ -1 \\ 2  \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0  \end{pmatrix} $$ 
  This is a system of three equations for the three unkowns $c_1,c_2,c_3$ and we can either try to directly solve the equations:
$$ \Leftrightarrow \left\{\begin{array}{l}
 c_1  = - c_2 \\
 c_1 - c_3=0 \\
 2 c_2 + 2 c_3 =0
        \end{array}  \right.    \Leftrightarrow  \left\{ \begin{array}{l}
 c_1 = - c_2  \\
 c_1 = c_3 
        \end{array}  \right. $$
or use Gaussian elimination to solve this:
$$\left[\begin{array}{ccc|c} 1 & 1 & 0 & 0\\ 1 & 0 & -1 & 0 \\ 0 & 2 & 2 & 0\end{array}\right] \xrightarrow{R_2 - R_1} \left[\begin{array}{ccc|c} 1 & 1 & 0 & 0\\ 0 & -1 & -1 & 0\\ 0 & 2 & 2 & 0 \end{array}\right] \xrightarrow{R_3 + 2R_2} \left[\begin{array}{ccc|c} 1 & 1 & 0 & 0 \\ 0 & -1 & -1 & 0\\ 0 & 0 & 0 & 0 \end{array}\right]$$
In either case we see that there are non-trivial solutions, e.g.~$(c_1,c_2,c_3)=(1,-1,1)$. So the system is linearly dependent. We can remove , e.g.~ the third column vector to obtain a linearly independent set (the first two columns are obviously linearly independent). Hence these can form a basis of the column space and the rank is 2. 

The nullity, that is the dimension of the nullspace, is found by an almost identical calculation:
$$ {\rm A} \mathbf{x} = \left(\begin{array}{ccc}
             1 & 1 & 0 \\
            1 & 0 & -1 \\
            0 & 2 & 2
        \end{array}\right)\begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} =\begin{pmatrix} x_1 + x_2\\ x_1 - x_3 \\ 2 x_2 + 2 x_3\end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix} $$
as this is the same system that we solved for the linear independence only the $c_i$ are now $x_i$:    
$$ \Leftrightarrow \left\{\begin{array}{l}
 x_1  = - x_2 \\
 x_1 - x_3=0 \\
 2 x_2 + 2 x_3 =0
        \end{array}  \right. \Rightarrow \left\{ \begin{array}{l}
 x_1 = - x_2  \\
 x_1 = x_3   \end{array} \right. $$
$$ \Rightarrow  \mathbb{x} = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} x_1 \\ - x_1 \\ x_1 \end{pmatrix} = x_1 \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix} = \lambda \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}, \quad \lambda \in \mathbb{R} $$
Hence,
$$ {\rm Null(A)} = {\rm span}\left(\begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} \right),$$
and the nullity of ${\rm A}$ is 1. 
:::

:::: {#exm-    }
Find the nullity of each of the following matrices:

$$M = \left[\begin{array}{cc} 2 & 3 \\ 1 & 5 \\ 4 & 7 \\ 3 & 6 \end{array}\right]; \qquad N = \left[\begin{array}{cccc} 2 & 1 & -2 & -1 \\ 4 & 4 & -3 & 1 \\ 2 & 7 & 1 & 8 \end{array}\right]$$

::: {.Solution}
Since the two columns of $M$ are linearly independent,
${\rm rank}(M)=2$. Thus, by the Rank Theorem, ${\rm nullity}(M) = 2 - {\rm rank}(M) = 2 - 2 = 0$.

To find the nullity of ${\rm N}$ we first determine the dimension of the row space and the use Theorem \ref{rowcolumnspace} for find the rank. To find the dimension of the row space we can apply row operations to reduce ${\rm N}$ to a row echelon form, as row operations don't change the span of the row vectors:
$$\left[\begin{array}{cccc} 2 & 1 & -2 & -1 \\ 4 & 4 & -3 & 1 \\ 2 & 7 & 1 & 8 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_2 - 2R_1 \\ R_3 - R_1 \end{subarray}} \left[\begin{array}{cccc} 2 & 1 & -2 & -1 \\ 0 & 2 & 1 & 3 \\ 0 & 6 & 3 & 9 \end{array}\right] \xrightarrow{R_3 - 2R_2} \left[\begin{array}{cccc} 2 & 1 & -2 & -1 \\ 0 & 2 & 1 & 3 \\ 0 & 0 & 0 & 0 \end{array}\right]$$
We see that there are only two nonzero rows, so ${\rm dim(row\ space)} = {\rm rank}(N)=2$. Hence,
${\rm nullity}(N) = 4 - {\rm rank}(N) = 4 - 2 = 2$.
:::
::::


:::: {#exm-    }
Let
$$A = \left[\begin{array}{cccc}
            1 & 0 & 1 & 2 \\
            2 & 1 & 2 & 5 \\
            1 & 1 & 0 & 2 \\
            0 & 1 & 0 & 1
        \end{array}\right]$$
Verify the Rank Theorem.

::: {.Solution}
A row echelon form is given by

$$\left[\begin{array}{cccc} 1 & 0 & 1 & 2 \\ 2 & 1 & 2 & 5 \\ 1 & 1 & 0 & 2 \\ 0 & 1 & 0 & 1 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_2 - 2R_1 \\ R_3 - R_1 \end{subarray}} \left[\begin{array}{cccc} 1 & 0 & 1 & 2 \\ 0 & 1 & 0 & 1 \\ 0 & 1 & -1 & 0 \\ 0 & 1 & 0 & 1 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_3 - R_2 \\ R_4 - R_2 \end{subarray}} \left[\begin{array}{cccc} 1 & 0 & 1 & 2 \\ 0 & 1 & 0 & 1 \\ 0 & 0 & -1 & -1 \\ 0 & 0 & 0 & 0 \end{array}\right]$$

Notice that columns $1, 2$ and $3$ have pivots, but not column $4$, so
columns $1, 2$ and $3$ are linearly independent and the ${\rm rank}(A) = 3$.\
Solving the equation $A\mathbf{x} = \mathbf{0}$, we see that all
solutions are of the form
$\mathbf{x} = c \left[1, 1, 1, -1 \right]^{T}$, so that the dimension of
the nullspace is $1$. Thus, ${\rm rank}(A) + {\rm nullity}(A) = 3 + 1 = 4$,
verifying the Rank Theorem.
:::

:::::



:::: {#exm-    }
Find the rank and nullity of
$$A = \left[\begin{array}{ccccc}
            1 & 1 & 0 & 1 & 1 \\
            0 & 1 & 1 & 2 & 0 \\
            1 & 2 & 1 & 3 & 1 \\
            2 & 3 & 1 & 4 & 2 
        \end{array}\right]$$

Identify bases for both the range and nullspace of $A$.

::: {.Solution}
 A row echelon form is given by
$$\left[\begin{array}{ccccc} 1 & 1 & 0 & 1 & 1 \\ 0 & 1 & 1 & 2 & 0 \\ 1 & 2 & 1 & 3 & 1 \\ 2 & 3 & 1 & 4 & 2 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_{3} - R_{1} \\ R_{4} - 2R_{1} \end{subarray}} \left[\begin{array}{ccccc} 1 & 1 & 0 & 1 & 1 \\ 0 & 1 & 1 & 2 & 0 \\ 0 & 1 & 1 & 2 & 0 \\ 0 & 1 & 1 & 2 & 0 \end{array}\right] \xrightarrow{\begin{subarray}{c} R_{3} - R_{2} \\ R_{4} - R_{2} \end{subarray}} \left[\begin{array}{ccccc} 1 & 1 & 0 & 1 & 1 \\ 0 & 1 & 1 & 2 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \end{array}\right]$$

Hence ${\rm rank}(A) = 2$ and ${\rm nullity}(A) = 5 - {\rm rank}(A) = 5 - 2 = 3$. Columns $1$ and $2$ have pivots, therefore a basis for ${\rm range}(A)$ is
$\left\{\left[1,0,1,2\right]^T, \left[1,1,2,3\right]^T\right\}$.

Let $x_3 = r, x_4 = s$ and $x_5 = t$, then $x_1 = r + s - t, x_2 = -r - 2s$, so that
$$\mathbf{x} = r \left[\begin{array}{c} 1 \\ -1 \\ 1 \\ 0 \\ 0 \end{array}\right] + s \left[\begin{array}{c} 1 \\ -2 \\ 0 \\ 1 \\ 0 \end{array}\right] + t \left[\begin{array}{c} -1 \\ 0 \\ 0 \\ 0 \\ 1 \end{array}\right]$$
So a basis for the nullspace is
$$\left\{\left[1,-1,1,0,0\right]^T, \left[1,-2,0,1,0\right]^T, \left[-1,0,0,0,1\right]^T\right\}$$

**Summary of the procedure to find a basis for the nullspace of $A$.**

1. Find a row echelon form $R$ of $A$.\
2. Solve for the leading variables ($x_1$ and $x_2$ above) of
$R\mathbf{x} = \mathbf{0}$ in terms of the free variables ($x_3, x_4$
and $x_5$ in the example).
3. Set the free variables equal to parameters, substitute back
into $\mathbf{x}$, and write the results as a linear combination of $f$
vectors (where $f$ is the number of free variables). These $f$ vectors
form a basis for ${\rm Null}(A)$.
:::
::::

::: {.callout-note #nte-nonsquare} 
## Remark: Nullspace of non-square matrices
A non-square matrix, $m \times n$, with $n>m$ must always have a non-trivial nullspace, i.e. a nullity > 0. The reason is that the rank can be at most $m$ as the dimension of column space and row space are the same (Theorem \ref{rowcolumnspace}). Then the rank theorem implies that the nullity is at least 1. This is the reason why a non-square matrix can't have an inverse. 
:::
